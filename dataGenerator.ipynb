{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9685a5f-1689-455b-80c6-c2147070f961",
   "metadata": {},
   "source": [
    "#### This notebook generates and stores a synthetic dataset for either a finetuned GAN model or regular model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba749d-71aa-436a-a348-d25c58c425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install --upgrade transformers\n",
    "!pip install einops\n",
    "!pip install torch\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e9e825-3a40-4d7a-ae26-eacea69891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset, DatasetDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64ad5e9-3929-4ef3-a8bc-f252e9f24dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "285475d3-ab56-4e9c-bc52-9b293705c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetPrompts(needsExamples):\n",
    "    if(needsExamples):\n",
    "        positivePrompt = '''Generate an enthusiastic and positive social media tweet. Your tweet should express praise or excitement. Use the following examples as a guide for formatting and tone:\\n'''\n",
    "        negativePrompt = '''Generate a disapproving and negative social media tweet. The tweet should convey criticism or disappointment. Use the following examples as a guide for formatting and tone:\\n'''\n",
    "\n",
    "    else:\n",
    "        positivePrompt = '''Generate an enthusiastic and positive social media tweet. Your tweet should express praise or excitement.\\n'''\n",
    "        negativePrompt = '''Generate a disapproving and negative social media tweet. The tweet should convey criticism or disappointment.\\n'''\n",
    "\n",
    "    return positivePrompt, negativePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1417f418-5659-4264-9bff-e68e466a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a sampled version of the dataset and a prompt with k examples. \n",
    "def sampleDataset(samples, k):\n",
    "    positivePrompt, negativePrompt = resetPrompts(k != 0)\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    dataset_sentiment_0 = train_dataset.filter(lambda x: x['label'] == 0)\n",
    "    dataset_sentiment_1 = train_dataset.filter(lambda x: x['label'] == 1)\n",
    "    \n",
    "    dataset_sampled_0 = dataset_sentiment_0.shuffle().select(range(samples//2))\n",
    "    dataset_sampled_1 = dataset_sentiment_1.shuffle().select(range(samples//2))\n",
    "    sampled_dataset = concatenate_datasets([dataset_sampled_0, dataset_sampled_1]).shuffle()\n",
    "\n",
    "    print(\"Positive: \", sum(1 for example in sampled_dataset if example['label'] == 1))\n",
    "    print(\"Negative: \", sum(1 for example in sampled_dataset if example['label'] == 0))\n",
    "    print(sampled_dataset)\n",
    "    \n",
    "    examples_0 = dataset_sentiment_0.select(range(k))[\"text\"]\n",
    "    examples_1 = dataset_sentiment_1.select(range(k))[\"text\"]\n",
    "\n",
    "    #Add examples only if needed\n",
    "    if (k !=0): \n",
    "        for entry in examples_1:\n",
    "            positivePrompt += f'Positive: \"{entry}\"\\n'\n",
    "        for entry in examples_0:\n",
    "            negativePrompt += f'Negative: \"{entry}\"\\n'\n",
    "        \n",
    "    plainPositive = positivePrompt\n",
    "    plainNegative = negativePrompt\n",
    "    \n",
    "    positivePrompt += 'Positive: \"'\n",
    "    negativePrompt += 'Negative: \"'\n",
    "    \n",
    "    return sampled_dataset, positivePrompt, negativePrompt, plainPositive, plainNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40653fa-56ff-4757-aae7-cea7f554fc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f2831d9e5548079692df16d1cb1027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28ebd93669c457a88337c543200e788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  500\n",
      "Negative:  500\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "size = 1000\n",
    "sampled_dataset, positivePrompt, negativePrompt, plainPositive, plainNegative = sampleDataset(1000, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccde460f-50bf-47c2-91ed-d18a902b715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6068cdb8f8ed436e874f7d6f9d484fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "login(YOUR_KEY)\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "  model = model.cuda()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f5a4ed-c54d-4fc3-a112-67680e5f21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptModel(prompt, length):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        do_sample=True,\n",
    "        attention_mask=attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        max_length=length,\n",
    "        temperature = 0.9, \n",
    "        top_p = 0.9, \n",
    "        repetition_penalty=1.2,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get logits\n",
    "        logits = model(outputs.sequences).logits\n",
    "\n",
    "\n",
    "    return text, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ef6f959-c434-4cc7-81fb-4ba936addbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTweets(generated_text, isPositive, plainPrompt):\n",
    "    output = generated_text.replace(plainPrompt, \"\").replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "    if(isPositive):\n",
    "        positive_match = re.search(r'Positive:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        positive_tweet = positive_match.group(1).strip() if positive_match else \"-1\"\n",
    "        return positive_tweet\n",
    "\n",
    "    else:\n",
    "        negative_match = re.search(r'Negative:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        negative_tweet = negative_match.group(1).strip() if negative_match else \"-1\"\n",
    "        return negative_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "195bb976-9646-4a11-b090-2abd358f879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAndExtractTweets(prompt, label, length, plainPrompt):\n",
    "            \n",
    "    while True:\n",
    "        text, logits = promptModel(prompt, length)\n",
    "        tweet = findTweets(text, label == 1, plainPrompt)\n",
    "        \n",
    "        if tweet != \"-1\" and len(tweet) > 0:\n",
    "            length = len(tokenizer.encode(tweet, add_special_tokens=True))\n",
    "            text, logits = promptModel(tweet, length + 1)  \n",
    "            return text, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b252068f-bda9-404e-8bea-731b1d1d745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a dataset given its size and tokens per tweet. It prompts the model, the neccessary amount of times and arranges everything into a Dataset object. \n",
    "def generateDataset (size, tokensPerTweet):\n",
    "\n",
    "    syntheticDataset = {\n",
    "        'text': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    for i in range (size//2):\n",
    "        text, logits = generateAndExtractTweets(positivePrompt, 1, tokensPerTweet, plainPositive)\n",
    "        syntheticDataset[\"text\"].append(text)\n",
    "        syntheticDataset[\"label\"].append(1)\n",
    "        if(i % 20 == 0):\n",
    "            print(i)\n",
    "\n",
    "    print(\"FINISHED POSITIVE\")\n",
    "\n",
    "    for i in range (size//2):\n",
    "        text, logits = generateAndExtractTweets(negativePrompt, 0, tokensPerTweet, plainNegative)\n",
    "        syntheticDataset[\"text\"].append(text)\n",
    "        syntheticDataset[\"label\"].append(0)\n",
    "        if(i % 20 == 0):\n",
    "            print(i)\n",
    "    print(\"FINISHED NEGATIVE\")\n",
    "\n",
    "    syntheticDataset = Dataset.from_dict(syntheticDataset).shuffle()\n",
    "\n",
    "    return syntheticDataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "626c0f31-e836-4518-809b-c7fff5457b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2340\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "2740\n",
      "2760\n",
      "2780\n",
      "2800\n",
      "2820\n",
      "2840\n",
      "2860\n",
      "2880\n",
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "2980\n",
      "3000\n",
      "3020\n",
      "3040\n",
      "3060\n",
      "3080\n",
      "3100\n",
      "3120\n",
      "3140\n",
      "3160\n",
      "3180\n",
      "3200\n",
      "3220\n",
      "3240\n",
      "3260\n",
      "3280\n",
      "3300\n",
      "3320\n",
      "3340\n",
      "3360\n",
      "3380\n",
      "3400\n",
      "3420\n",
      "3440\n",
      "3460\n",
      "3480\n",
      "3500\n",
      "3520\n",
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "3620\n",
      "3640\n",
      "3660\n",
      "3680\n",
      "3700\n",
      "3720\n",
      "3740\n",
      "FINISHED POSITIVE\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2340\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "2740\n",
      "2760\n",
      "2780\n",
      "2800\n",
      "2820\n",
      "2840\n",
      "2860\n",
      "2880\n",
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "2980\n",
      "3000\n",
      "3020\n",
      "3040\n",
      "3060\n",
      "3080\n",
      "3100\n",
      "3120\n",
      "3140\n",
      "3160\n",
      "3180\n",
      "3200\n",
      "3220\n",
      "3240\n",
      "3260\n",
      "3280\n",
      "3300\n",
      "3320\n",
      "3340\n",
      "3360\n",
      "3380\n",
      "3400\n",
      "3420\n",
      "3440\n",
      "3460\n",
      "3480\n",
      "3500\n",
      "3520\n",
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "3620\n",
      "3640\n",
      "3660\n",
      "3680\n",
      "3700\n",
      "3720\n",
      "3740\n",
      "FINISHED NEGATIVE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FINISHED TRAIN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "FINISHED POSITIVE\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "FINISHED NEGATIVE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FINISHED TEST\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7a276cd19346049572abde68e15f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5199ea2b0b6b42bcb743093661cd474d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate a full dataset with 15k training datapoints and 2k testing datapoints. \n",
    "train = generateDataset(15000, 100)\n",
    "print(\"\\n\\n\\n\\nFINISHED TRAIN\\n\\n\\n\\n\")\n",
    "test = generateDataset(2000, 100)\n",
    "print(\"\\n\\n\\n\\nFINISHED TEST\\n\\n\\n\\n\")\n",
    "\n",
    "syntheticDataset = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': test\n",
    "})\n",
    "\n",
    "syntheticDataset.save_to_disk(YOUR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05657a22-dde9-4b03-bdbc-1e95b8abccde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
