{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb00a03-3405-40f0-9c9b-6ac9907fb45e",
   "metadata": {},
   "source": [
    "#### This file will finetune a model using a GAN setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb29b19-f7ab-4cf8-81bc-3cb1880878ec",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba749d-71aa-436a-a348-d25c58c425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install --upgrade transformers\n",
    "!pip install einops\n",
    "!pip install torch\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25e9e825-3a40-4d7a-ae26-eacea69891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset, DatasetDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64ad5e9-3929-4ef3-a8bc-f252e9f24dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1417f418-5659-4264-9bff-e68e466a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns an equal number of positive and negative samples. Ex: samples = 1000 means 500 positive and 500 negative datapoints\n",
    "\n",
    "def sampleDataset(samples):\n",
    "    train_dataset = dataset['train']\n",
    "\n",
    "    dataset_sentiment_0 = train_dataset.filter(lambda x: x['label'] == 0)\n",
    "    dataset_sentiment_1 = train_dataset.filter(lambda x: x['label'] == 1)\n",
    "\n",
    "    dataset_sampled_0 = dataset_sentiment_0.shuffle().select(range(samples//2))\n",
    "    dataset_sampled_1 = dataset_sentiment_1.shuffle().select(range(samples//2))\n",
    "\n",
    "    dataset_combined = concatenate_datasets([dataset_sampled_0, dataset_sampled_1])\n",
    "\n",
    "    sampled_dataset = dataset_combined.shuffle()\n",
    "\n",
    "    print(\"Positive: \", sum(1 for example in sampled_dataset if example['label'] == 1))\n",
    "    print(\"Negative: \", sum(1 for example in sampled_dataset if example['label'] == 0))\n",
    "    print(sampled_dataset)\n",
    "    \n",
    "    return sampled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40653fa-56ff-4757-aae7-cea7f554fc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f2b9834daa4330aacdf4f49f75b3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7feee2f1944471cb95cfffd5928a4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  500\n",
      "Negative:  500\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#For the purposes of this GAN we only trained with 1000 datapoints\n",
    "sampled_dataset = sampleDataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "285475d3-ab56-4e9c-bc52-9b293705c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivePrompt = '''\n",
    "Generate a positive social media tweet on a specific topic. Ensure your tweet is enclosed in straight double quotation marks. Provide ONLY one tweet. The positive tweet should express enthusiasm or praise. \n",
    "\n",
    "Positive: \"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b9404a-3e6f-498c-8c70-175a4e545e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativePrompt = '''\n",
    "Generate a negative social media tweet on a specific topic. Ensure your tweet is enclosed in straight double quotation marks and separated by a colon. Provide ONLY one tweet. The negative tweet should express convey criticism or disappointment. \n",
    "\n",
    "Negative: \"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccde460f-50bf-47c2-91ed-d18a902b715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583585125934492f85c292f1f228febe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a383bebb024bbf97f09bfcbf73b214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f09b0ad3c41dfb8bdce4e97947b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae12b25e67024c54af8a6e26b148cbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5b21ac0c694e0fa02c0dbf27efa8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3ff843c7aa4f44a2a542bb453080b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e31a43d13e48afa093cb30389fb05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cc7b50144749ed93ee7616b0fe41bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9107ac3d2804bbca9436da842e1e42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7323e685e12d465196a4b48e245ad31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b77a4724ad4e46bfcc065301f30c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0af999e3c8941e0b4ce8134ed247aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "login(YOUR_KEY_HERE)\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "  model = model.cuda()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287043b0-c29b-43a2-95aa-9ff60f768561",
   "metadata": {},
   "source": [
    "### Model Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc5da9-5cad-4446-8c3e-72d1bd69a5dd",
   "metadata": {},
   "source": [
    "A major issue we ran into here was the model repeating the instructions back in it's output. Also, the model tends to include addtional unwanted information after the requested tweet. Obviously this isn't ideal because it shouldn't be part of what the generator and discriminator learn on. \n",
    "\n",
    "The problem with simply modifying the string output is that it doesn't show the logits (this is what the GAN setup actually uses). To solve this, we created a two step approach: \n",
    "\n",
    "1. Call promptModel() with an intial length of 100 tokens.\n",
    "2. Filter out the tweet from the unwanted content using findTweets()\n",
    "3. Feed the tweet back into the model using promptModel() but set a length of the tweet.\n",
    "\n",
    "Now, the model is forced to just return the tweet back (no space for additonal content), but also gives the correct logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f5a4ed-c54d-4fc3-a112-67680e5f21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the text and logits of a generated tweet. \n",
    "def promptModel(prompt, length):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        do_sample=True,\n",
    "        attention_mask=attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        max_length=length,\n",
    "        temperature = 0.9, \n",
    "        top_p = 0.9, \n",
    "        repetition_penalty=1.2,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get logits\n",
    "        logits = model(outputs.sequences).logits\n",
    "\n",
    "\n",
    "    return text, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6f959-c434-4cc7-81fb-4ba936addbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses regex to find the actual tweet content of a tweet. \n",
    "def findTweets(generated_text, isPositive):\n",
    "    output = generated_text.replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "    if(isPositive):\n",
    "        positive_match = re.search(r'Positive:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        positive_tweet = positive_match.group(1).strip() if positive_match else \"-1\"\n",
    "        return positive_tweet\n",
    "\n",
    "    else:\n",
    "        negative_match = re.search(r'Negative:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        negative_tweet = negative_match.group(1).strip() if negative_match else \"-1\"\n",
    "        return negative_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195bb976-9646-4a11-b090-2abd358f879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a tweet and it's logits using the two step filter. \n",
    "def generateAndExtractTweets(prompt, label, length):\n",
    "            \n",
    "    while True:\n",
    "        text, logits = promptModel(prompt, length)\n",
    "        tweet = findTweets(text, label == 1)\n",
    "        \n",
    "        if tweet != \"-1\" and len(tweet) > 0:\n",
    "            length = len(tokenizer.encode(tweet, add_special_tokens=True))\n",
    "            text, logits = promptModel(tweet, length + 1)  \n",
    "            return text, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f2b4e-8397-489c-ae75-de61fcdd5ccd",
   "metadata": {},
   "source": [
    "### GAN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b23e6-7254-4419-837d-9e504bb05d3b",
   "metadata": {},
   "source": [
    "Our GAN uses 1000 datapoints. You can change this amount by updating the sampled_dataset size in the Setup section. \n",
    "\n",
    "We train for 5 epochs. You can change this amount by changing the epochs variable on the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecfa368b-4199-4c05-9084-875e9d8ee03e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Our Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, 128, batch_first=True)\n",
    "    self.fc = nn.Linear(128, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    embeds = self.embedding(input_ids)\n",
    "    _, (hidden, _) = self.lstm(embeds)\n",
    "    output = self.fc(hidden[-1])\n",
    "    return self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ad849-ca1c-4faf-8677-7ce2e59f0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the generator\n",
    "generator = model\n",
    "\n",
    "#Instantiate the Discriminator\n",
    "if(torch.cuda.is_available()):\n",
    "  discriminator = Discriminator(len(tokenizer), 768).cuda()\n",
    "else:\n",
    "    discriminator = Discriminator(len(tokenizer), 768)\n",
    "\n",
    "\n",
    "#Optimizers\n",
    "optimizerG = AdamW(generator.parameters(), lr=5e-5)\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=5e-5)\n",
    "\n",
    "#Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Dataloader\n",
    "data_loader = torch.utils.data.DataLoader(sampled_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295be7f6-d0cd-48f9-b8fb-169f743ca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more accurate error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "batchCount = 1\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch in data_loader:\n",
    "        \n",
    "        # Convert the input batch into a PyTorch tensor and move to GPU\n",
    "        padded_inputs = tokenizer(batch['text'], padding=True, return_tensors=\"pt\", truncation=True)\n",
    "        real_text = padded_inputs['input_ids'].cuda()        \n",
    "\n",
    "        #Generate FAKE outputs\n",
    "        promptUsed = positivePrompt if batch[\"label\"] == 1 else negativePrompt # Use the positive prompt if the batch is positive. \n",
    "        labelUsed = batch[\"label\"] \n",
    "        fake_text, fake_logits_raw = generateAndExtractTweets(promptUsed, labelUsed, 100) # Get the logits from the tweet\n",
    "        fake_logits_raw = fake_logits_raw.cuda()\n",
    "        fake_logits = fake_logits_raw.argmax(dim=-1).detach().cuda()\n",
    "        \n",
    "        # Reset gradients for discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Create real labels tensor and move to GPU\n",
    "        real_labels = torch.ones((real_text.size(0), 1), dtype=torch.float).cuda()\n",
    "\n",
    "        # Get discriminator's prediction on real text and calculate loss\n",
    "        real_output = discriminator(real_text)\n",
    "        lossD_real = criterion(real_output.view(-1, 1), real_labels)\n",
    "        \n",
    "        # Reshape fake logits to ensure correct shape\n",
    "        fake_logits = fake_logits.view(-1, 1).cuda()\n",
    "\n",
    "        # Get discriminator's prediction on fake text and calculate loss\n",
    "        fake_output = discriminator(fake_logits)\n",
    "        \n",
    "        # Create fake labels tensor and move to GPU\n",
    "        fake_labels = torch.zeros((fake_output.size(0), 1), dtype=torch.float).cuda()\n",
    "        \n",
    "        lossD_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Combine real and fake losses for the discriminator\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        lossD.backward()\n",
    "\n",
    "        #Update the Discriminator\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Reset gradients for generator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Generate new fake text logits\n",
    "        with torch.no_grad():\n",
    "            fake_text, fake_logits_raw = generateAndExtractTweets(promptUsed, labelUsed, 100)\n",
    "            fake_logits_raw = fake_logits_raw.cuda()\n",
    "            \n",
    "        fake_logits = fake_logits_raw.argmax(dim=-1).detach().cuda()\n",
    "\n",
    "        # Get discriminator's assessment of the newly generated fake data\n",
    "        fake_output = discriminator(fake_logits.view(-1, 1).cuda())\n",
    "        real_labels = torch.ones((fake_output.size(0), 1), dtype=torch.float).cuda()\n",
    "\n",
    "        # Calculate the generator's loss\n",
    "        lossG = criterion(fake_output.view(-1, 1), real_labels)\n",
    "        lossG.backward()\n",
    "\n",
    "        # Update the Generator \n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Print status and clear memory\n",
    "        print(f'Epoch [{epoch}/{epochs}] Loss_D: {lossD.item():.4f} Loss_G: {lossG.item():.4f}')\n",
    "        print(\"BATCH NUMBER \" + str(batchCount))\n",
    "        batchCount+=1\n",
    "        del padded_inputs, real_text, fake_logits, fake_logits_raw, real_labels, real_output, fake_labels, fake_output\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4c279-ea1c-479e-9ddf-ab79c265820c",
   "metadata": {},
   "source": [
    "Save the GAN. \n",
    "\n",
    "Provide YOUR_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d64a940-f31f-4a6c-9e6a-6a63f6966996",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = YOUR_PATH\n",
    "epoch = YOUR_EPOCHS\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "model_paths = {\n",
    "    'generator': 'generator.pth',\n",
    "    'discriminator': 'discriminator.pth',\n",
    "    'optimizerG': 'optimizerG.pth',\n",
    "    'optimizerD': 'optimizerD.pth',\n",
    "    'checkpoint': 'checkpoint.pth'\n",
    "}\n",
    "\n",
    "for key in model_paths:\n",
    "    model_paths[key] = os.path.join(save_dir, model_paths[key])\n",
    "\n",
    "# Save the generator and discriminator state\n",
    "torch.save(generator.state_dict(), model_paths['generator'])\n",
    "torch.save(discriminator.state_dict(), model_paths['discriminator'])\n",
    "\n",
    "# # Save the optimizer states\n",
    "torch.save(optimizerG.state_dict(), model_paths['optimizerG'])\n",
    "torch.save(optimizerD.state_dict(), model_paths['optimizerD'])\n",
    "\n",
    "# Optionally, save the epoch number or other states\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'lossG': lossG,  # Current generator loss\n",
    "    'lossD': lossD   # Current discriminator loss\n",
    "}\n",
    "torch.save(checkpoint, model_paths['checkpoint'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
