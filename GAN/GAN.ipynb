{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ba749d-71aa-436a-a348-d25c58c425c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.3.5-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, requests, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed aiohappyeyeballs-2.3.5 aiohttp-3.10.2 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 huggingface-hub-0.24.5 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-17.0.0 pyarrow-hotfix-0.6 pytz-2024.1 requests-2.32.3 tqdm-4.66.5 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers[sentencepiece])\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers[sentencepiece])\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers[sentencepiece])\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.5)\n",
      "Collecting protobuf (from transformers[sentencepiece])\n",
      "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n",
      "Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, safetensors, regex, protobuf, tokenizers, transformers\n",
      "Successfully installed protobuf-5.27.3 regex-2024.7.24 safetensors-0.4.4 sentencepiece-0.2.0 tokenizers-0.19.1 transformers-4.44.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting openai\n",
      "  Downloading openai-1.40.2-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.40.2-py3-none-any.whl (360 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.7/360.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: typing-extensions, jiter, annotated-types, pydantic-core, pydantic, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed annotated-types-0.7.0 jiter-0.5.0 openai-1.40.2 pydantic-2.8.2 pydantic-core-2.20.1 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install --upgrade transformers\n",
    "!pip install einops\n",
    "!pip install openai\n",
    "!pip install evaluate\n",
    "!pip install torch\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25e9e825-3a40-4d7a-ae26-eacea69891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6619b7ca-aef2-42c5-a35c-0dd02d9bb003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_dataset(zip_path):\n",
    "    # Create a temporary directory\n",
    "    temp_dir = '/tmp/dataset_extracted'\n",
    "    \n",
    "    # Ensure the directory is clean\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    os.makedirs(temp_dir)\n",
    "    \n",
    "    # Unzip the file to the temporary directory\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "\n",
    "    # Load the dataset from the extracted directory\n",
    "    dataset = load_from_disk(temp_dir)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64ad5e9-3929-4ef3-a8bc-f252e9f24dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_and_extract_dataset(\"dataset (1).zip\")\n",
    "# tokenized_datasets = load_and_extract_dataset(\"tokenized_datasets2.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1417f418-5659-4264-9bff-e68e466a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataset(samples):\n",
    "    train_dataset = dataset['train']\n",
    "\n",
    "    dataset_sentiment_0 = train_dataset.filter(lambda x: x['label'] == 0)\n",
    "    dataset_sentiment_1 = train_dataset.filter(lambda x: x['label'] == 1)\n",
    "\n",
    "    dataset_sampled_0 = dataset_sentiment_0.shuffle().select(range(samples//2))\n",
    "    dataset_sampled_1 = dataset_sentiment_1.shuffle().select(range(samples//2))\n",
    "\n",
    "    dataset_combined = concatenate_datasets([dataset_sampled_0, dataset_sampled_1])\n",
    "\n",
    "    sampled_dataset = dataset_combined.shuffle()\n",
    "\n",
    "    print(\"Positive: \", sum(1 for example in sampled_dataset if example['label'] == 1))\n",
    "    print(\"Negative: \", sum(1 for example in sampled_dataset if example['label'] == 0))\n",
    "    print(sampled_dataset)\n",
    "\n",
    "    return sampled_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e40653fa-56ff-4757-aae7-cea7f554fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  500\n",
      "Negative:  500\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sampled_dataset = sampleDataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ccde460f-50bf-47c2-91ed-d18a902b715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773ad89880cc4f1c9157eb7517a877cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3624b074f1d3455689cd872a2400ad0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc01fd8d9914244ac51ea5e2ad4d24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3e6c8160bd44d0b292ff174d2bc485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5bc4d7e3f8489da03b8d0f2ef3277e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcc8e5da9b945d1988ebdce40969c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b9d0b361e647c3a519d610120af848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9d36d93d764dd0b66169357613c69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9c0700a0184773b6bc7e8e711ae479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4d744ee73f45f0b091433391b1eb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7710ebb3324fc59cdfd999b4694e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229f28fc2b024ed6a3aed3425888e365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "  model = model.cuda()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32f5a4ed-c54d-4fc3-a112-67680e5f21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptModel(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = inputs['attention_mask'].to(\"cuda\")\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        attention_mask=attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True, \n",
    "        temperature = 0.9, \n",
    "        top_p = 0.9, \n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "    generated_tokens = output_sequences.sequences\n",
    "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get logits\n",
    "        logits = model(generated_tokens).logits\n",
    "\n",
    "    return generated_text, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ef6f959-c434-4cc7-81fb-4ba936addbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTweets(generated_text, isPositive):\n",
    "    output = generated_text.replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "    if(isPositive):\n",
    "        positive_match = re.search(r'Positive:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        positive_tweet = positive_match.group(1).strip() if positive_match else \"-1\"\n",
    "        return positive_tweet\n",
    "\n",
    "    else:\n",
    "        negative_match = re.search(r'Negative:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        negative_tweet = negative_match.group(1).strip() if negative_match else \"-1\"\n",
    "        return negative_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "195bb976-9646-4a11-b090-2abd358f879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAndExtractTweets(positivePrompt, negativePrompt,  batchSize):\n",
    "\n",
    "    outputs = []\n",
    "    promptUsed = \"\"\n",
    "    isPositive = True\n",
    "    \n",
    "    for i in range (batchSize):\n",
    "        \n",
    "        if(i <= 2):\n",
    "            promptUsed = positivePrompt\n",
    "            isPositive = True\n",
    "        else:\n",
    "            promptUsed = negativePrompt\n",
    "            isPositive = False\n",
    "            \n",
    "        while True:\n",
    "            generated_text, logits = promptModel(promptUsed)\n",
    "            tweet = findTweets(generated_text, isPositive)\n",
    "        \n",
    "            if tweet != \"-1\" and len(tweet) > 0:\n",
    "                newOutput = [tweet, logits]\n",
    "                outputs.append(newOutput)\n",
    "                break\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecfa368b-4199-4c05-9084-875e9d8ee03e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, 128, batch_first=True)\n",
    "    self.fc = nn.Linear(128, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    embeds = self.embedding(input_ids)\n",
    "    _, (hidden, _) = self.lstm(embeds)\n",
    "    output = self.fc(hidden[-1])\n",
    "    return self.sigmoid(output)\n",
    "#generator and discriminator instantiation\n",
    "generator = model\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "  discriminator = Discriminator(len(tokenizer), 768).cuda()\n",
    "else:\n",
    "    discriminator = Discriminator(len(tokenizer), 768)\n",
    "\n",
    "#optimizers\n",
    "optimizerG = AdamW(generator.parameters(), lr=5e-5)\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=5e-5)\n",
    "#loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "data_loader = torch.utils.data.DataLoader(sampled_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f7c5782-e67d-4fd0-9899-e9883267e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label'])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef90cb1-e9ea-456d-87d3-013cb5cc1e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Not feeling to good, I think I hurt my hand pretty bad ... typing is not fun right now, not a good thing for a writer  ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataset[370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "285475d3-ab56-4e9c-bc52-9b293705c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivePrompt = '''\n",
    "Generate a positive social media tweet on a specific topic. Ensure your tweet is enclosed in straight double quotation marks. Provide ONLY one tweet. The positive tweet should express enthusiasm or praise. \n",
    "\n",
    "Positive: \"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23b9404a-3e6f-498c-8c70-175a4e545e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativePrompt = '''\n",
    "Generate a negative social media tweet on a specific topic. Ensure your tweet is enclosed in straight double quotation marks and separated by a colon. Provide ONLY one tweet. The negative tweet should express convey criticism or disappointment. \n",
    "\n",
    "Negative: \"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "597f31a4-6647-4b92-b23e-cd3ed317f8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGenerate a positive social media tweet on a specific topic. Ensure your tweet is enclosed in straight double quotation marks. Provide ONLY one tweet. The positive tweet should express enthusiasm or praise. \\n\\nPositive: \"Thrilled to see our team\\'s dedication and hard work lead us towards success! Keep shining, everyone!\" #TeamSpirit @CompanyName'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = promptModel(positivePrompt)\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "40bc67e5-fd21-4c2f-8366-d50de5a84df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"We're celebrating National Teacher Appreciation Week by recognizing the hard work of all educators who dedicate their lives to shaping young minds! Their selflessness, patience & love inspire us every day\",\n",
       "  tensor([[[-2.9512,  1.5942,  7.4129,  ...,  1.6402,  1.6403,  1.6403],\n",
       "           [ 7.8343,  6.6701, 13.2224,  ..., -5.3764, -5.3765, -5.3765],\n",
       "           [ 0.9604, -0.9753,  0.2828,  ..., -6.5249, -6.5248, -6.5249],\n",
       "           ...,\n",
       "           [ 4.2491,  4.9086,  1.4445,  ..., -5.6226, -5.6225, -5.6227],\n",
       "           [ 4.6136,  4.7835,  1.6697,  ..., -4.5970, -4.5970, -4.5973],\n",
       "           [ 2.2296,  4.5771,  2.0076,  ..., -3.9358, -3.9359, -3.9360]]],\n",
       "         device='cuda:0')],\n",
       " [\"We are all born with the power to choose our own path! Let's support each other along the way, every step of this incredible journey we call life.\",\n",
       "  tensor([[[-2.9512,  1.5942,  7.4129,  ...,  1.6402,  1.6403,  1.6403],\n",
       "           [ 7.8343,  6.6701, 13.2224,  ..., -5.3764, -5.3765, -5.3765],\n",
       "           [ 0.9604, -0.9753,  0.2828,  ..., -6.5249, -6.5248, -6.5249],\n",
       "           ...,\n",
       "           [ 6.3047,  8.0920, 12.8738,  ..., -0.8806, -0.8805, -0.8803],\n",
       "           [ 3.3633,  5.5476,  8.4849,  ..., -0.8971, -0.8971, -0.8971],\n",
       "           [ 3.7376,  6.8831,  8.2947,  ..., -0.4929, -0.4930, -0.4929]]],\n",
       "         device='cuda:0')]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generateAndExtractTweets(prompt, prompt, 2)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f51942f-94dd-4dab-8691-f8f1426ec4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeData(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3c40ae00-dd80-4fff-81e0-d7e7e110580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Me and Momo just saw a crazy ass bitch at Walmart! All bad; but we got most of the stuff on our list '], 'label': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    print(batch) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "784811d9-db77-43c4-b4c9-c3af078125aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Loss_D: 1.4040 Loss_G: 0.4741\n",
      "BATCH NUMBER 1\n",
      "Epoch [0/5] Loss_D: 1.4569 Loss_G: 0.4639\n",
      "BATCH NUMBER 2\n",
      "Epoch [0/5] Loss_D: 1.4063 Loss_G: 0.4734\n",
      "BATCH NUMBER 3\n",
      "Epoch [0/5] Loss_D: 1.4101 Loss_G: 0.4698\n",
      "BATCH NUMBER 4\n",
      "Epoch [0/5] Loss_D: 1.4122 Loss_G: 0.4699\n",
      "BATCH NUMBER 5\n",
      "Epoch [0/5] Loss_D: 1.3919 Loss_G: 0.4779\n",
      "BATCH NUMBER 6\n",
      "Epoch [0/5] Loss_D: 1.3755 Loss_G: 0.4774\n",
      "BATCH NUMBER 7\n",
      "Epoch [0/5] Loss_D: 1.3977 Loss_G: 0.4715\n",
      "BATCH NUMBER 8\n",
      "Epoch [0/5] Loss_D: 1.4558 Loss_G: 0.4775\n",
      "BATCH NUMBER 9\n",
      "Epoch [0/5] Loss_D: 1.4432 Loss_G: 0.4721\n",
      "BATCH NUMBER 10\n",
      "Epoch [0/5] Loss_D: 1.3837 Loss_G: 0.4801\n",
      "BATCH NUMBER 11\n",
      "Epoch [0/5] Loss_D: 1.4556 Loss_G: 0.4444\n",
      "BATCH NUMBER 12\n",
      "Epoch [0/5] Loss_D: 1.3989 Loss_G: 0.4685\n",
      "BATCH NUMBER 13\n",
      "Epoch [0/5] Loss_D: 1.3945 Loss_G: 0.4773\n",
      "BATCH NUMBER 14\n",
      "Epoch [0/5] Loss_D: 1.3883 Loss_G: 0.4724\n",
      "BATCH NUMBER 15\n",
      "Epoch [0/5] Loss_D: 1.4492 Loss_G: 0.4718\n",
      "BATCH NUMBER 16\n",
      "Epoch [0/5] Loss_D: 1.3905 Loss_G: 0.4719\n",
      "BATCH NUMBER 17\n",
      "Epoch [0/5] Loss_D: 1.3562 Loss_G: 0.4926\n",
      "BATCH NUMBER 18\n",
      "Epoch [0/5] Loss_D: 1.3784 Loss_G: 0.4769\n",
      "BATCH NUMBER 19\n",
      "Epoch [0/5] Loss_D: 1.3711 Loss_G: 0.4858\n",
      "BATCH NUMBER 20\n",
      "Epoch [0/5] Loss_D: 1.3515 Loss_G: 0.4940\n",
      "BATCH NUMBER 21\n",
      "Epoch [0/5] Loss_D: 1.4347 Loss_G: 0.4884\n",
      "BATCH NUMBER 22\n",
      "Epoch [0/5] Loss_D: 1.3525 Loss_G: 0.4932\n",
      "BATCH NUMBER 23\n",
      "Epoch [0/5] Loss_D: 1.4725 Loss_G: 0.4722\n",
      "BATCH NUMBER 24\n",
      "Epoch [0/5] Loss_D: 1.4808 Loss_G: 0.4731\n",
      "BATCH NUMBER 25\n",
      "Epoch [0/5] Loss_D: 1.3286 Loss_G: 0.5021\n",
      "BATCH NUMBER 26\n",
      "Epoch [0/5] Loss_D: 1.3455 Loss_G: 0.4953\n",
      "BATCH NUMBER 27\n",
      "Epoch [0/5] Loss_D: 1.3460 Loss_G: 0.4898\n",
      "BATCH NUMBER 28\n",
      "Epoch [0/5] Loss_D: 1.4556 Loss_G: 0.4760\n",
      "BATCH NUMBER 29\n",
      "Epoch [0/5] Loss_D: 1.4061 Loss_G: 0.4617\n",
      "BATCH NUMBER 30\n",
      "Epoch [0/5] Loss_D: 1.3899 Loss_G: 0.4690\n",
      "BATCH NUMBER 31\n",
      "Epoch [0/5] Loss_D: 1.3742 Loss_G: 0.4781\n",
      "BATCH NUMBER 32\n",
      "Epoch [0/5] Loss_D: 1.3387 Loss_G: 0.5026\n",
      "BATCH NUMBER 33\n",
      "Epoch [0/5] Loss_D: 1.3784 Loss_G: 0.4686\n",
      "BATCH NUMBER 34\n",
      "Epoch [0/5] Loss_D: 1.3544 Loss_G: 0.4911\n",
      "BATCH NUMBER 35\n",
      "Epoch [0/5] Loss_D: 1.4200 Loss_G: 0.4472\n",
      "BATCH NUMBER 36\n",
      "Epoch [0/5] Loss_D: 1.3762 Loss_G: 0.4664\n",
      "BATCH NUMBER 37\n",
      "Epoch [0/5] Loss_D: 1.3877 Loss_G: 0.5029\n",
      "BATCH NUMBER 38\n",
      "Epoch [0/5] Loss_D: 1.3532 Loss_G: 0.4773\n",
      "BATCH NUMBER 39\n",
      "Epoch [0/5] Loss_D: 1.4748 Loss_G: 0.4739\n",
      "BATCH NUMBER 40\n",
      "Epoch [0/5] Loss_D: 1.4161 Loss_G: 0.4818\n",
      "BATCH NUMBER 41\n",
      "Epoch [0/5] Loss_D: 1.4148 Loss_G: 0.4494\n",
      "BATCH NUMBER 42\n",
      "Epoch [0/5] Loss_D: 1.2993 Loss_G: 0.5180\n",
      "BATCH NUMBER 43\n",
      "Epoch [0/5] Loss_D: 1.4056 Loss_G: 0.4562\n",
      "BATCH NUMBER 44\n",
      "Epoch [0/5] Loss_D: 1.3656 Loss_G: 0.4706\n",
      "BATCH NUMBER 45\n",
      "Epoch [0/5] Loss_D: 1.4522 Loss_G: 0.4738\n",
      "BATCH NUMBER 46\n",
      "Epoch [0/5] Loss_D: 1.3585 Loss_G: 0.4767\n",
      "BATCH NUMBER 47\n",
      "Epoch [0/5] Loss_D: 1.3503 Loss_G: 0.4774\n",
      "BATCH NUMBER 48\n",
      "Epoch [0/5] Loss_D: 1.3613 Loss_G: 0.4724\n",
      "BATCH NUMBER 49\n",
      "Epoch [0/5] Loss_D: 1.3915 Loss_G: 0.4633\n",
      "BATCH NUMBER 50\n",
      "Epoch [0/5] Loss_D: 1.3174 Loss_G: 0.4998\n",
      "BATCH NUMBER 51\n",
      "Epoch [0/5] Loss_D: 1.3185 Loss_G: 0.5013\n",
      "BATCH NUMBER 52\n",
      "Epoch [0/5] Loss_D: 1.3169 Loss_G: 0.4994\n",
      "BATCH NUMBER 53\n",
      "Epoch [0/5] Loss_D: 1.3881 Loss_G: 0.4566\n",
      "BATCH NUMBER 54\n",
      "Epoch [0/5] Loss_D: 1.3116 Loss_G: 0.5070\n",
      "BATCH NUMBER 55\n",
      "Epoch [0/5] Loss_D: 1.3703 Loss_G: 0.4696\n",
      "BATCH NUMBER 56\n",
      "Epoch [0/5] Loss_D: 1.5076 Loss_G: 0.4547\n",
      "BATCH NUMBER 57\n",
      "Epoch [0/5] Loss_D: 1.3865 Loss_G: 0.4591\n",
      "BATCH NUMBER 58\n",
      "Epoch [0/5] Loss_D: 1.3344 Loss_G: 0.4887\n",
      "BATCH NUMBER 59\n",
      "Epoch [0/5] Loss_D: 1.3112 Loss_G: 0.4980\n",
      "BATCH NUMBER 60\n",
      "Epoch [0/5] Loss_D: 1.3474 Loss_G: 0.4778\n",
      "BATCH NUMBER 61\n",
      "Epoch [0/5] Loss_D: 1.4087 Loss_G: 0.4858\n",
      "BATCH NUMBER 62\n",
      "Epoch [0/5] Loss_D: 1.3476 Loss_G: 0.4767\n",
      "BATCH NUMBER 63\n",
      "Epoch [0/5] Loss_D: 1.3279 Loss_G: 0.4925\n",
      "BATCH NUMBER 64\n",
      "Epoch [0/5] Loss_D: 1.4083 Loss_G: 0.4433\n",
      "BATCH NUMBER 65\n",
      "Epoch [0/5] Loss_D: 1.3565 Loss_G: 0.4721\n",
      "BATCH NUMBER 66\n",
      "Epoch [0/5] Loss_D: 1.4877 Loss_G: 0.4529\n",
      "BATCH NUMBER 67\n",
      "Epoch [0/5] Loss_D: 1.3116 Loss_G: 0.4993\n",
      "BATCH NUMBER 68\n",
      "Epoch [0/5] Loss_D: 1.3790 Loss_G: 0.4583\n",
      "BATCH NUMBER 69\n",
      "Epoch [0/5] Loss_D: 1.4164 Loss_G: 0.4420\n",
      "BATCH NUMBER 70\n",
      "Epoch [0/5] Loss_D: 1.3632 Loss_G: 0.4688\n",
      "BATCH NUMBER 71\n",
      "Epoch [0/5] Loss_D: 1.3620 Loss_G: 0.4686\n",
      "BATCH NUMBER 72\n",
      "Epoch [0/5] Loss_D: 1.3917 Loss_G: 0.4820\n",
      "BATCH NUMBER 73\n",
      "Epoch [0/5] Loss_D: 1.3382 Loss_G: 0.4774\n",
      "BATCH NUMBER 74\n",
      "Epoch [0/5] Loss_D: 1.3388 Loss_G: 0.5238\n",
      "BATCH NUMBER 75\n",
      "Epoch [0/5] Loss_D: 1.3488 Loss_G: 0.4760\n",
      "BATCH NUMBER 76\n",
      "Epoch [0/5] Loss_D: 1.3638 Loss_G: 0.4671\n",
      "BATCH NUMBER 77\n",
      "Epoch [0/5] Loss_D: 1.3174 Loss_G: 0.4954\n",
      "BATCH NUMBER 78\n",
      "Epoch [0/5] Loss_D: 1.3193 Loss_G: 0.4880\n",
      "BATCH NUMBER 79\n",
      "Epoch [0/5] Loss_D: 1.3707 Loss_G: 0.4624\n",
      "BATCH NUMBER 80\n",
      "Epoch [0/5] Loss_D: 1.3010 Loss_G: 0.5007\n",
      "BATCH NUMBER 81\n",
      "Epoch [0/5] Loss_D: 1.3135 Loss_G: 0.4950\n",
      "BATCH NUMBER 82\n",
      "Epoch [0/5] Loss_D: 1.3579 Loss_G: 0.4678\n",
      "BATCH NUMBER 83\n",
      "Epoch [0/5] Loss_D: 1.3794 Loss_G: 0.5117\n",
      "BATCH NUMBER 84\n",
      "Epoch [0/5] Loss_D: 1.4128 Loss_G: 0.4374\n",
      "BATCH NUMBER 85\n",
      "Epoch [0/5] Loss_D: 1.2911 Loss_G: 0.5114\n",
      "BATCH NUMBER 86\n",
      "Epoch [0/5] Loss_D: 1.4702 Loss_G: 0.4704\n",
      "BATCH NUMBER 87\n",
      "Epoch [0/5] Loss_D: 1.3032 Loss_G: 0.4995\n",
      "BATCH NUMBER 88\n",
      "Epoch [0/5] Loss_D: 1.4808 Loss_G: 0.4600\n",
      "BATCH NUMBER 89\n",
      "Epoch [0/5] Loss_D: 1.2822 Loss_G: 0.5147\n",
      "BATCH NUMBER 90\n",
      "Epoch [0/5] Loss_D: 1.2969 Loss_G: 0.5049\n",
      "BATCH NUMBER 91\n",
      "Epoch [0/5] Loss_D: 1.2739 Loss_G: 0.5214\n",
      "BATCH NUMBER 92\n",
      "Epoch [0/5] Loss_D: 1.3226 Loss_G: 0.5265\n",
      "BATCH NUMBER 93\n",
      "Epoch [0/5] Loss_D: 1.3775 Loss_G: 0.4541\n",
      "BATCH NUMBER 94\n",
      "Epoch [0/5] Loss_D: 1.3197 Loss_G: 0.4862\n",
      "BATCH NUMBER 95\n",
      "Epoch [0/5] Loss_D: 1.3817 Loss_G: 0.4537\n",
      "BATCH NUMBER 96\n",
      "Epoch [0/5] Loss_D: 1.3235 Loss_G: 0.4842\n",
      "BATCH NUMBER 97\n",
      "Epoch [0/5] Loss_D: 1.2894 Loss_G: 0.5038\n",
      "BATCH NUMBER 98\n",
      "Epoch [0/5] Loss_D: 1.3199 Loss_G: 0.4922\n",
      "BATCH NUMBER 99\n",
      "Epoch [0/5] Loss_D: 1.3244 Loss_G: 0.4809\n",
      "BATCH NUMBER 100\n",
      "Epoch [0/5] Loss_D: 1.4296 Loss_G: 0.5056\n",
      "BATCH NUMBER 101\n",
      "Epoch [0/5] Loss_D: 1.3103 Loss_G: 0.4913\n",
      "BATCH NUMBER 102\n",
      "Epoch [0/5] Loss_D: 1.3343 Loss_G: 0.4779\n",
      "BATCH NUMBER 103\n",
      "Epoch [0/5] Loss_D: 1.3547 Loss_G: 0.5312\n",
      "BATCH NUMBER 104\n",
      "Epoch [0/5] Loss_D: 1.3245 Loss_G: 0.4823\n",
      "BATCH NUMBER 105\n",
      "Epoch [0/5] Loss_D: 1.3519 Loss_G: 0.4653\n",
      "BATCH NUMBER 106\n",
      "Epoch [0/5] Loss_D: 1.3637 Loss_G: 0.5308\n",
      "BATCH NUMBER 107\n",
      "Epoch [0/5] Loss_D: 1.2762 Loss_G: 0.5143\n",
      "BATCH NUMBER 108\n",
      "Epoch [0/5] Loss_D: 1.2973 Loss_G: 0.4985\n",
      "BATCH NUMBER 109\n",
      "Epoch [0/5] Loss_D: 1.2975 Loss_G: 0.4998\n",
      "BATCH NUMBER 110\n",
      "Epoch [0/5] Loss_D: 1.4165 Loss_G: 0.4887\n",
      "BATCH NUMBER 111\n",
      "Epoch [0/5] Loss_D: 1.2764 Loss_G: 0.5108\n",
      "BATCH NUMBER 112\n",
      "Epoch [0/5] Loss_D: 1.4094 Loss_G: 0.4962\n",
      "BATCH NUMBER 113\n",
      "Epoch [0/5] Loss_D: 1.2831 Loss_G: 0.5115\n",
      "BATCH NUMBER 114\n",
      "Epoch [0/5] Loss_D: 1.2190 Loss_G: 0.5497\n",
      "BATCH NUMBER 115\n",
      "Epoch [0/5] Loss_D: 1.4063 Loss_G: 0.4824\n",
      "BATCH NUMBER 116\n",
      "Epoch [0/5] Loss_D: 1.3942 Loss_G: 0.4422\n",
      "BATCH NUMBER 117\n",
      "Epoch [0/5] Loss_D: 1.3154 Loss_G: 0.4833\n",
      "BATCH NUMBER 118\n",
      "Epoch [0/5] Loss_D: 1.4066 Loss_G: 0.4388\n",
      "BATCH NUMBER 119\n",
      "Epoch [0/5] Loss_D: 1.3360 Loss_G: 0.5369\n",
      "BATCH NUMBER 120\n",
      "Epoch [0/5] Loss_D: 1.4030 Loss_G: 0.4932\n",
      "BATCH NUMBER 121\n",
      "Epoch [0/5] Loss_D: 1.3809 Loss_G: 0.5006\n",
      "BATCH NUMBER 122\n",
      "Epoch [0/5] Loss_D: 1.4450 Loss_G: 0.4786\n",
      "BATCH NUMBER 123\n",
      "Epoch [0/5] Loss_D: 1.3245 Loss_G: 0.4787\n",
      "BATCH NUMBER 124\n",
      "Epoch [0/5] Loss_D: 1.4758 Loss_G: 0.4869\n",
      "BATCH NUMBER 125\n",
      "Epoch [0/5] Loss_D: 1.2787 Loss_G: 0.5101\n",
      "BATCH NUMBER 126\n",
      "Epoch [0/5] Loss_D: 1.2943 Loss_G: 0.5015\n",
      "BATCH NUMBER 127\n",
      "Epoch [0/5] Loss_D: 1.4429 Loss_G: 0.4891\n",
      "BATCH NUMBER 128\n",
      "Epoch [0/5] Loss_D: 1.3641 Loss_G: 0.4604\n",
      "BATCH NUMBER 129\n",
      "Epoch [0/5] Loss_D: 1.3298 Loss_G: 0.4770\n",
      "BATCH NUMBER 130\n",
      "Epoch [0/5] Loss_D: 1.3856 Loss_G: 0.5034\n",
      "BATCH NUMBER 131\n",
      "Epoch [0/5] Loss_D: 1.2473 Loss_G: 0.5326\n",
      "BATCH NUMBER 132\n",
      "Epoch [0/5] Loss_D: 1.3975 Loss_G: 0.4997\n",
      "BATCH NUMBER 133\n",
      "Epoch [0/5] Loss_D: 1.2801 Loss_G: 0.5029\n",
      "BATCH NUMBER 134\n",
      "Epoch [0/5] Loss_D: 1.2960 Loss_G: 0.4992\n",
      "BATCH NUMBER 135\n",
      "Epoch [0/5] Loss_D: 1.2450 Loss_G: 0.5335\n",
      "BATCH NUMBER 136\n",
      "Epoch [0/5] Loss_D: 1.2787 Loss_G: 0.5094\n",
      "BATCH NUMBER 137\n",
      "Epoch [0/5] Loss_D: 1.3067 Loss_G: 0.4911\n",
      "BATCH NUMBER 138\n",
      "Epoch [0/5] Loss_D: 1.3535 Loss_G: 0.5090\n",
      "BATCH NUMBER 139\n",
      "Epoch [0/5] Loss_D: 1.2481 Loss_G: 0.5294\n",
      "BATCH NUMBER 140\n",
      "Epoch [0/5] Loss_D: 1.4478 Loss_G: 0.4753\n",
      "BATCH NUMBER 141\n",
      "Epoch [0/5] Loss_D: 1.4038 Loss_G: 0.4690\n",
      "BATCH NUMBER 142\n",
      "Epoch [0/5] Loss_D: 1.3678 Loss_G: 0.5122\n",
      "BATCH NUMBER 143\n",
      "Epoch [0/5] Loss_D: 1.2446 Loss_G: 0.5311\n",
      "BATCH NUMBER 144\n",
      "Epoch [0/5] Loss_D: 1.3402 Loss_G: 0.4756\n",
      "BATCH NUMBER 145\n",
      "Epoch [0/5] Loss_D: 1.2702 Loss_G: 0.5183\n",
      "BATCH NUMBER 146\n",
      "Epoch [0/5] Loss_D: 1.4394 Loss_G: 0.5010\n",
      "BATCH NUMBER 147\n",
      "Epoch [0/5] Loss_D: 1.3257 Loss_G: 0.4803\n",
      "BATCH NUMBER 148\n",
      "Epoch [0/5] Loss_D: 1.3422 Loss_G: 0.4653\n",
      "BATCH NUMBER 149\n",
      "Epoch [0/5] Loss_D: 1.3383 Loss_G: 0.4826\n",
      "BATCH NUMBER 150\n",
      "Epoch [0/5] Loss_D: 1.3734 Loss_G: 0.5099\n",
      "BATCH NUMBER 151\n",
      "Epoch [0/5] Loss_D: 1.3259 Loss_G: 0.4841\n",
      "BATCH NUMBER 152\n",
      "Epoch [0/5] Loss_D: 1.3837 Loss_G: 0.4449\n",
      "BATCH NUMBER 153\n",
      "Epoch [0/5] Loss_D: 1.3609 Loss_G: 0.4660\n",
      "BATCH NUMBER 154\n",
      "Epoch [0/5] Loss_D: 1.3844 Loss_G: 0.4447\n",
      "BATCH NUMBER 155\n",
      "Epoch [0/5] Loss_D: 1.2844 Loss_G: 0.5050\n",
      "BATCH NUMBER 156\n",
      "Epoch [0/5] Loss_D: 1.3912 Loss_G: 0.5208\n",
      "BATCH NUMBER 157\n",
      "Epoch [0/5] Loss_D: 1.2739 Loss_G: 0.5104\n",
      "BATCH NUMBER 158\n",
      "Epoch [0/5] Loss_D: 1.2625 Loss_G: 0.5169\n",
      "BATCH NUMBER 159\n",
      "Epoch [0/5] Loss_D: 1.4027 Loss_G: 0.4995\n",
      "BATCH NUMBER 160\n",
      "Epoch [0/5] Loss_D: 1.4611 Loss_G: 0.4782\n",
      "BATCH NUMBER 161\n",
      "Epoch [0/5] Loss_D: 1.3132 Loss_G: 0.4828\n",
      "BATCH NUMBER 162\n",
      "Epoch [0/5] Loss_D: 1.3962 Loss_G: 0.4732\n",
      "BATCH NUMBER 163\n",
      "Epoch [0/5] Loss_D: 1.2910 Loss_G: 0.4954\n",
      "BATCH NUMBER 164\n",
      "Epoch [0/5] Loss_D: 1.3134 Loss_G: 0.4878\n",
      "BATCH NUMBER 165\n",
      "Epoch [0/5] Loss_D: 1.4127 Loss_G: 0.4825\n",
      "BATCH NUMBER 166\n",
      "Epoch [0/5] Loss_D: 1.2702 Loss_G: 0.5222\n",
      "BATCH NUMBER 167\n",
      "Epoch [0/5] Loss_D: 1.3870 Loss_G: 0.4403\n",
      "BATCH NUMBER 168\n",
      "Epoch [0/5] Loss_D: 1.4314 Loss_G: 0.4869\n",
      "BATCH NUMBER 169\n",
      "Epoch [0/5] Loss_D: 1.4988 Loss_G: 0.4455\n",
      "BATCH NUMBER 170\n",
      "Epoch [0/5] Loss_D: 1.2813 Loss_G: 0.5030\n",
      "BATCH NUMBER 171\n",
      "Epoch [0/5] Loss_D: 1.2936 Loss_G: 0.4959\n",
      "BATCH NUMBER 172\n",
      "Epoch [0/5] Loss_D: 1.5559 Loss_G: 0.4138\n",
      "BATCH NUMBER 173\n",
      "Epoch [0/5] Loss_D: 1.3354 Loss_G: 0.4680\n",
      "BATCH NUMBER 174\n",
      "Epoch [0/5] Loss_D: 1.2476 Loss_G: 0.5221\n",
      "BATCH NUMBER 175\n",
      "Epoch [0/5] Loss_D: 1.2504 Loss_G: 0.5303\n",
      "BATCH NUMBER 176\n",
      "Epoch [0/5] Loss_D: 1.2851 Loss_G: 0.5030\n",
      "BATCH NUMBER 177\n",
      "Epoch [0/5] Loss_D: 1.3285 Loss_G: 0.4728\n",
      "BATCH NUMBER 178\n",
      "Epoch [0/5] Loss_D: 1.3502 Loss_G: 0.4613\n",
      "BATCH NUMBER 179\n",
      "Epoch [0/5] Loss_D: 1.2626 Loss_G: 0.5175\n",
      "BATCH NUMBER 180\n",
      "Epoch [0/5] Loss_D: 1.3994 Loss_G: 0.4894\n",
      "BATCH NUMBER 181\n",
      "Epoch [0/5] Loss_D: 1.2997 Loss_G: 0.4881\n",
      "BATCH NUMBER 182\n",
      "Epoch [0/5] Loss_D: 1.2703 Loss_G: 0.5159\n",
      "BATCH NUMBER 183\n",
      "Epoch [0/5] Loss_D: 1.2730 Loss_G: 0.5035\n",
      "BATCH NUMBER 184\n",
      "Epoch [0/5] Loss_D: 1.3638 Loss_G: 0.4568\n",
      "BATCH NUMBER 185\n",
      "Epoch [0/5] Loss_D: 1.2987 Loss_G: 0.5001\n",
      "BATCH NUMBER 186\n",
      "Epoch [0/5] Loss_D: 1.2350 Loss_G: 0.5379\n",
      "BATCH NUMBER 187\n",
      "Epoch [0/5] Loss_D: 1.2987 Loss_G: 0.4912\n",
      "BATCH NUMBER 188\n",
      "Epoch [0/5] Loss_D: 1.2483 Loss_G: 0.5238\n",
      "BATCH NUMBER 189\n",
      "Epoch [0/5] Loss_D: 1.2252 Loss_G: 0.5441\n",
      "BATCH NUMBER 190\n",
      "Epoch [0/5] Loss_D: 1.3777 Loss_G: 0.5056\n",
      "BATCH NUMBER 191\n",
      "Epoch [0/5] Loss_D: 1.2454 Loss_G: 0.5268\n",
      "BATCH NUMBER 192\n",
      "Epoch [0/5] Loss_D: 1.2349 Loss_G: 0.5289\n",
      "BATCH NUMBER 193\n",
      "Epoch [0/5] Loss_D: 1.2862 Loss_G: 0.5008\n",
      "BATCH NUMBER 194\n",
      "Epoch [0/5] Loss_D: 1.3753 Loss_G: 0.4878\n",
      "BATCH NUMBER 195\n",
      "Epoch [0/5] Loss_D: 1.3938 Loss_G: 0.5099\n",
      "BATCH NUMBER 196\n",
      "Epoch [0/5] Loss_D: 1.3209 Loss_G: 0.4788\n",
      "BATCH NUMBER 197\n",
      "Epoch [0/5] Loss_D: 1.2925 Loss_G: 0.4946\n",
      "BATCH NUMBER 198\n",
      "Epoch [0/5] Loss_D: 1.2698 Loss_G: 0.5067\n",
      "BATCH NUMBER 199\n",
      "Epoch [0/5] Loss_D: 1.1752 Loss_G: 0.5787\n",
      "BATCH NUMBER 200\n",
      "Epoch [0/5] Loss_D: 1.2851 Loss_G: 0.4938\n",
      "BATCH NUMBER 201\n",
      "Epoch [0/5] Loss_D: 1.4800 Loss_G: 0.4491\n",
      "BATCH NUMBER 202\n",
      "Epoch [0/5] Loss_D: 1.2611 Loss_G: 0.5101\n",
      "BATCH NUMBER 203\n",
      "Epoch [0/5] Loss_D: 1.2641 Loss_G: 0.5094\n",
      "BATCH NUMBER 204\n",
      "Epoch [0/5] Loss_D: 1.2353 Loss_G: 0.5293\n",
      "BATCH NUMBER 205\n",
      "Epoch [0/5] Loss_D: 1.2521 Loss_G: 0.5213\n",
      "BATCH NUMBER 206\n",
      "Epoch [0/5] Loss_D: 1.2645 Loss_G: 0.5171\n",
      "BATCH NUMBER 207\n",
      "Epoch [0/5] Loss_D: 1.2776 Loss_G: 0.5010\n",
      "BATCH NUMBER 208\n",
      "Epoch [0/5] Loss_D: 1.3035 Loss_G: 0.4835\n",
      "BATCH NUMBER 209\n",
      "Epoch [0/5] Loss_D: 1.2755 Loss_G: 0.5023\n",
      "BATCH NUMBER 210\n",
      "Epoch [0/5] Loss_D: 1.2326 Loss_G: 0.5437\n",
      "BATCH NUMBER 211\n",
      "Epoch [0/5] Loss_D: 1.3018 Loss_G: 0.4878\n",
      "BATCH NUMBER 212\n",
      "Epoch [0/5] Loss_D: 1.1514 Loss_G: 0.5911\n",
      "BATCH NUMBER 213\n",
      "Epoch [0/5] Loss_D: 1.3852 Loss_G: 0.5146\n",
      "BATCH NUMBER 214\n",
      "Epoch [0/5] Loss_D: 1.2253 Loss_G: 0.5379\n",
      "BATCH NUMBER 215\n",
      "Epoch [0/5] Loss_D: 1.4471 Loss_G: 0.4920\n",
      "BATCH NUMBER 216\n",
      "Epoch [0/5] Loss_D: 1.3442 Loss_G: 0.4616\n",
      "BATCH NUMBER 217\n",
      "Epoch [0/5] Loss_D: 1.2841 Loss_G: 0.4978\n",
      "BATCH NUMBER 218\n",
      "Epoch [0/5] Loss_D: 1.2006 Loss_G: 0.5530\n",
      "BATCH NUMBER 219\n",
      "Epoch [0/5] Loss_D: 1.2349 Loss_G: 0.5299\n",
      "BATCH NUMBER 220\n",
      "Epoch [0/5] Loss_D: 1.2937 Loss_G: 0.4932\n",
      "BATCH NUMBER 221\n",
      "Epoch [0/5] Loss_D: 1.4179 Loss_G: 0.5114\n",
      "BATCH NUMBER 222\n",
      "Epoch [0/5] Loss_D: 1.3628 Loss_G: 0.4531\n",
      "BATCH NUMBER 223\n",
      "Epoch [0/5] Loss_D: 1.2778 Loss_G: 0.5036\n",
      "BATCH NUMBER 224\n",
      "Epoch [0/5] Loss_D: 1.2711 Loss_G: 0.5070\n",
      "BATCH NUMBER 225\n",
      "Epoch [0/5] Loss_D: 1.2824 Loss_G: 0.5014\n",
      "BATCH NUMBER 226\n",
      "Epoch [0/5] Loss_D: 1.2842 Loss_G: 0.4961\n",
      "BATCH NUMBER 227\n",
      "Epoch [0/5] Loss_D: 1.1524 Loss_G: 0.5891\n",
      "BATCH NUMBER 228\n",
      "Epoch [0/5] Loss_D: 1.2103 Loss_G: 0.5499\n",
      "BATCH NUMBER 229\n",
      "Epoch [0/5] Loss_D: 1.3732 Loss_G: 0.4513\n",
      "BATCH NUMBER 230\n",
      "Epoch [0/5] Loss_D: 1.2198 Loss_G: 0.5401\n",
      "BATCH NUMBER 231\n",
      "Epoch [0/5] Loss_D: 1.3391 Loss_G: 0.4693\n",
      "BATCH NUMBER 232\n",
      "Epoch [0/5] Loss_D: 1.3246 Loss_G: 0.4739\n",
      "BATCH NUMBER 233\n",
      "Epoch [0/5] Loss_D: 1.2837 Loss_G: 0.4925\n",
      "BATCH NUMBER 234\n",
      "Epoch [0/5] Loss_D: 1.3184 Loss_G: 0.4800\n",
      "BATCH NUMBER 235\n",
      "Epoch [0/5] Loss_D: 1.2506 Loss_G: 0.5170\n",
      "BATCH NUMBER 236\n",
      "Epoch [0/5] Loss_D: 1.4409 Loss_G: 0.4835\n",
      "BATCH NUMBER 237\n",
      "Epoch [0/5] Loss_D: 1.2488 Loss_G: 0.5138\n",
      "BATCH NUMBER 238\n",
      "Epoch [0/5] Loss_D: 1.2060 Loss_G: 0.5553\n",
      "BATCH NUMBER 239\n",
      "Epoch [0/5] Loss_D: 1.4681 Loss_G: 0.4509\n",
      "BATCH NUMBER 240\n",
      "Epoch [0/5] Loss_D: 1.3482 Loss_G: 0.4594\n",
      "BATCH NUMBER 241\n",
      "Epoch [0/5] Loss_D: 1.4351 Loss_G: 0.4926\n",
      "BATCH NUMBER 242\n",
      "Epoch [0/5] Loss_D: 1.2928 Loss_G: 0.4913\n",
      "BATCH NUMBER 243\n",
      "Epoch [0/5] Loss_D: 1.2325 Loss_G: 0.5296\n",
      "BATCH NUMBER 244\n",
      "Epoch [0/5] Loss_D: 1.2489 Loss_G: 0.5136\n",
      "BATCH NUMBER 245\n",
      "Epoch [0/5] Loss_D: 1.2641 Loss_G: 0.5131\n",
      "BATCH NUMBER 246\n",
      "Epoch [0/5] Loss_D: 1.1817 Loss_G: 0.5681\n",
      "BATCH NUMBER 247\n",
      "Epoch [0/5] Loss_D: 1.2900 Loss_G: 0.4890\n",
      "BATCH NUMBER 248\n",
      "Epoch [0/5] Loss_D: 1.2522 Loss_G: 0.5150\n",
      "BATCH NUMBER 249\n",
      "Epoch [0/5] Loss_D: 1.2457 Loss_G: 0.5203\n",
      "BATCH NUMBER 250\n",
      "Epoch [0/5] Loss_D: 1.4930 Loss_G: 0.4630\n",
      "BATCH NUMBER 251\n",
      "Epoch [0/5] Loss_D: 1.1562 Loss_G: 0.5855\n",
      "BATCH NUMBER 252\n",
      "Epoch [0/5] Loss_D: 1.3969 Loss_G: 0.5188\n",
      "BATCH NUMBER 253\n",
      "Epoch [0/5] Loss_D: 1.2419 Loss_G: 0.5205\n",
      "BATCH NUMBER 254\n",
      "Epoch [0/5] Loss_D: 1.2170 Loss_G: 0.5356\n",
      "BATCH NUMBER 255\n",
      "Epoch [0/5] Loss_D: 1.2547 Loss_G: 0.5190\n",
      "BATCH NUMBER 256\n",
      "Epoch [0/5] Loss_D: 1.3344 Loss_G: 0.5603\n",
      "BATCH NUMBER 257\n",
      "Epoch [0/5] Loss_D: 1.3905 Loss_G: 0.5071\n",
      "BATCH NUMBER 258\n",
      "Epoch [0/5] Loss_D: 1.2333 Loss_G: 0.5270\n",
      "BATCH NUMBER 259\n",
      "Epoch [0/5] Loss_D: 1.2011 Loss_G: 0.5528\n",
      "BATCH NUMBER 260\n",
      "Epoch [0/5] Loss_D: 1.2803 Loss_G: 0.5404\n",
      "BATCH NUMBER 261\n",
      "Epoch [0/5] Loss_D: 1.2851 Loss_G: 0.4980\n",
      "BATCH NUMBER 262\n",
      "Epoch [0/5] Loss_D: 1.2918 Loss_G: 0.4858\n",
      "BATCH NUMBER 263\n",
      "Epoch [0/5] Loss_D: 1.4734 Loss_G: 0.4664\n",
      "BATCH NUMBER 264\n",
      "Epoch [0/5] Loss_D: 1.2502 Loss_G: 0.5141\n",
      "BATCH NUMBER 265\n",
      "Epoch [0/5] Loss_D: 1.2208 Loss_G: 0.5419\n",
      "BATCH NUMBER 266\n",
      "Epoch [0/5] Loss_D: 1.2019 Loss_G: 0.5512\n",
      "BATCH NUMBER 267\n",
      "Epoch [0/5] Loss_D: 1.2564 Loss_G: 0.5796\n",
      "BATCH NUMBER 268\n",
      "Epoch [0/5] Loss_D: 1.2764 Loss_G: 0.5051\n",
      "BATCH NUMBER 269\n",
      "Epoch [0/5] Loss_D: 1.3401 Loss_G: 0.5550\n",
      "BATCH NUMBER 270\n",
      "Epoch [0/5] Loss_D: 1.3148 Loss_G: 0.5282\n",
      "BATCH NUMBER 271\n",
      "Epoch [0/5] Loss_D: 1.3814 Loss_G: 0.5196\n",
      "BATCH NUMBER 272\n",
      "Epoch [0/5] Loss_D: 1.3011 Loss_G: 0.4853\n",
      "BATCH NUMBER 273\n",
      "Epoch [0/5] Loss_D: 1.1953 Loss_G: 0.5583\n",
      "BATCH NUMBER 274\n",
      "Epoch [0/5] Loss_D: 1.2493 Loss_G: 0.5281\n",
      "BATCH NUMBER 275\n",
      "Epoch [0/5] Loss_D: 1.2932 Loss_G: 0.4905\n",
      "BATCH NUMBER 276\n",
      "Epoch [0/5] Loss_D: 1.1691 Loss_G: 0.5758\n",
      "BATCH NUMBER 277\n",
      "Epoch [0/5] Loss_D: 1.2867 Loss_G: 0.5006\n",
      "BATCH NUMBER 278\n",
      "Epoch [0/5] Loss_D: 1.3123 Loss_G: 0.5562\n",
      "BATCH NUMBER 279\n",
      "Epoch [0/5] Loss_D: 1.1934 Loss_G: 0.5623\n",
      "BATCH NUMBER 280\n",
      "Epoch [0/5] Loss_D: 1.2790 Loss_G: 0.4977\n",
      "BATCH NUMBER 281\n",
      "Epoch [0/5] Loss_D: 1.4079 Loss_G: 0.4898\n",
      "BATCH NUMBER 282\n",
      "Epoch [0/5] Loss_D: 1.2660 Loss_G: 0.5140\n",
      "BATCH NUMBER 283\n",
      "Epoch [0/5] Loss_D: 1.3664 Loss_G: 0.5375\n",
      "BATCH NUMBER 284\n",
      "Epoch [0/5] Loss_D: 1.4206 Loss_G: 0.4864\n",
      "BATCH NUMBER 285\n",
      "Epoch [0/5] Loss_D: 1.1329 Loss_G: 0.6025\n",
      "BATCH NUMBER 286\n",
      "Epoch [0/5] Loss_D: 1.4175 Loss_G: 0.4770\n",
      "BATCH NUMBER 287\n",
      "Epoch [0/5] Loss_D: 1.3521 Loss_G: 0.4553\n",
      "BATCH NUMBER 288\n",
      "Epoch [0/5] Loss_D: 1.2590 Loss_G: 0.5151\n",
      "BATCH NUMBER 289\n",
      "Epoch [0/5] Loss_D: 1.1878 Loss_G: 0.5595\n",
      "BATCH NUMBER 290\n",
      "Epoch [0/5] Loss_D: 1.3817 Loss_G: 0.5306\n",
      "BATCH NUMBER 291\n",
      "Epoch [0/5] Loss_D: 1.2354 Loss_G: 0.5230\n",
      "BATCH NUMBER 292\n",
      "Epoch [0/5] Loss_D: 1.2802 Loss_G: 0.4961\n",
      "BATCH NUMBER 293\n",
      "Epoch [0/5] Loss_D: 1.3790 Loss_G: 0.4945\n",
      "BATCH NUMBER 294\n",
      "Epoch [0/5] Loss_D: 1.1470 Loss_G: 0.5937\n",
      "BATCH NUMBER 295\n",
      "Epoch [0/5] Loss_D: 1.2276 Loss_G: 0.5277\n",
      "BATCH NUMBER 296\n",
      "Epoch [0/5] Loss_D: 1.2223 Loss_G: 0.5352\n",
      "BATCH NUMBER 297\n",
      "Epoch [0/5] Loss_D: 1.2092 Loss_G: 0.5414\n",
      "BATCH NUMBER 298\n",
      "Epoch [0/5] Loss_D: 1.4464 Loss_G: 0.4765\n",
      "BATCH NUMBER 299\n",
      "Epoch [0/5] Loss_D: 1.2803 Loss_G: 0.5555\n",
      "BATCH NUMBER 300\n",
      "Epoch [0/5] Loss_D: 1.1941 Loss_G: 0.5577\n",
      "BATCH NUMBER 301\n",
      "Epoch [0/5] Loss_D: 1.2204 Loss_G: 0.5365\n",
      "BATCH NUMBER 302\n",
      "Epoch [0/5] Loss_D: 1.2968 Loss_G: 0.4870\n",
      "BATCH NUMBER 303\n",
      "Epoch [0/5] Loss_D: 1.2467 Loss_G: 0.5185\n",
      "BATCH NUMBER 304\n",
      "Epoch [0/5] Loss_D: 1.3594 Loss_G: 0.4560\n",
      "BATCH NUMBER 305\n",
      "Epoch [0/5] Loss_D: 1.3225 Loss_G: 0.5295\n",
      "BATCH NUMBER 306\n",
      "Epoch [0/5] Loss_D: 1.3467 Loss_G: 0.5426\n",
      "BATCH NUMBER 307\n",
      "Epoch [0/5] Loss_D: 1.2928 Loss_G: 0.5438\n",
      "BATCH NUMBER 308\n",
      "Epoch [0/5] Loss_D: 1.2669 Loss_G: 0.5159\n",
      "BATCH NUMBER 309\n",
      "Epoch [0/5] Loss_D: 1.2285 Loss_G: 0.5330\n",
      "BATCH NUMBER 310\n",
      "Epoch [0/5] Loss_D: 1.2989 Loss_G: 0.5252\n",
      "BATCH NUMBER 311\n",
      "Epoch [0/5] Loss_D: 1.1792 Loss_G: 0.5684\n",
      "BATCH NUMBER 312\n",
      "Epoch [0/5] Loss_D: 1.1796 Loss_G: 0.5639\n",
      "BATCH NUMBER 313\n",
      "Epoch [0/5] Loss_D: 1.4114 Loss_G: 0.5096\n",
      "BATCH NUMBER 314\n",
      "Epoch [0/5] Loss_D: 1.2943 Loss_G: 0.5648\n",
      "BATCH NUMBER 315\n",
      "Epoch [0/5] Loss_D: 1.2600 Loss_G: 0.5039\n",
      "BATCH NUMBER 316\n",
      "Epoch [0/5] Loss_D: 1.3351 Loss_G: 0.5155\n",
      "BATCH NUMBER 317\n",
      "Epoch [0/5] Loss_D: 1.3339 Loss_G: 0.5382\n",
      "BATCH NUMBER 318\n",
      "Epoch [0/5] Loss_D: 1.3241 Loss_G: 0.5571\n",
      "BATCH NUMBER 319\n",
      "Epoch [0/5] Loss_D: 1.2180 Loss_G: 0.5355\n",
      "BATCH NUMBER 320\n",
      "Epoch [0/5] Loss_D: 1.4077 Loss_G: 0.4730\n",
      "BATCH NUMBER 321\n",
      "Epoch [0/5] Loss_D: 1.2450 Loss_G: 0.5155\n",
      "BATCH NUMBER 322\n",
      "Epoch [0/5] Loss_D: 1.2834 Loss_G: 0.5436\n",
      "BATCH NUMBER 323\n",
      "Epoch [0/5] Loss_D: 1.2981 Loss_G: 0.4903\n",
      "BATCH NUMBER 324\n",
      "Epoch [0/5] Loss_D: 1.3629 Loss_G: 0.4887\n",
      "BATCH NUMBER 325\n",
      "Epoch [0/5] Loss_D: 1.3139 Loss_G: 0.4756\n",
      "BATCH NUMBER 326\n",
      "Epoch [0/5] Loss_D: 1.2019 Loss_G: 0.5499\n",
      "BATCH NUMBER 327\n",
      "Epoch [0/5] Loss_D: 1.1697 Loss_G: 0.5746\n",
      "BATCH NUMBER 328\n",
      "Epoch [0/5] Loss_D: 1.2521 Loss_G: 0.5111\n",
      "BATCH NUMBER 329\n",
      "Epoch [0/5] Loss_D: 1.2483 Loss_G: 0.5155\n",
      "BATCH NUMBER 330\n",
      "Epoch [0/5] Loss_D: 1.2627 Loss_G: 0.4999\n",
      "BATCH NUMBER 331\n",
      "Epoch [0/5] Loss_D: 1.1389 Loss_G: 0.5937\n",
      "BATCH NUMBER 332\n",
      "Epoch [0/5] Loss_D: 1.3377 Loss_G: 0.4971\n",
      "BATCH NUMBER 333\n",
      "Epoch [0/5] Loss_D: 1.2889 Loss_G: 0.4881\n",
      "BATCH NUMBER 334\n",
      "Epoch [0/5] Loss_D: 1.3296 Loss_G: 0.4686\n",
      "BATCH NUMBER 335\n",
      "Epoch [0/5] Loss_D: 1.2357 Loss_G: 0.5212\n",
      "BATCH NUMBER 336\n",
      "Epoch [0/5] Loss_D: 1.2662 Loss_G: 0.5053\n",
      "BATCH NUMBER 337\n",
      "Epoch [0/5] Loss_D: 1.1562 Loss_G: 0.5832\n",
      "BATCH NUMBER 338\n",
      "Epoch [0/5] Loss_D: 1.2834 Loss_G: 0.4889\n",
      "BATCH NUMBER 339\n",
      "Epoch [0/5] Loss_D: 1.3486 Loss_G: 0.5525\n",
      "BATCH NUMBER 340\n",
      "Epoch [0/5] Loss_D: 1.2436 Loss_G: 0.5211\n",
      "BATCH NUMBER 341\n",
      "Epoch [0/5] Loss_D: 1.1961 Loss_G: 0.5539\n",
      "BATCH NUMBER 342\n",
      "Epoch [0/5] Loss_D: 1.2661 Loss_G: 0.5079\n",
      "BATCH NUMBER 343\n",
      "Epoch [0/5] Loss_D: 1.2656 Loss_G: 0.5144\n",
      "BATCH NUMBER 344\n",
      "Epoch [0/5] Loss_D: 1.1918 Loss_G: 0.5543\n",
      "BATCH NUMBER 345\n",
      "Epoch [0/5] Loss_D: 1.3401 Loss_G: 0.5047\n",
      "BATCH NUMBER 346\n",
      "Epoch [0/5] Loss_D: 1.2927 Loss_G: 0.5324\n",
      "BATCH NUMBER 347\n",
      "Epoch [0/5] Loss_D: 1.3035 Loss_G: 0.4941\n",
      "BATCH NUMBER 348\n",
      "Epoch [0/5] Loss_D: 1.4431 Loss_G: 0.4865\n",
      "BATCH NUMBER 349\n",
      "Epoch [0/5] Loss_D: 1.1999 Loss_G: 0.5508\n",
      "BATCH NUMBER 350\n",
      "Epoch [0/5] Loss_D: 1.2353 Loss_G: 0.5237\n",
      "BATCH NUMBER 351\n",
      "Epoch [0/5] Loss_D: 1.3527 Loss_G: 0.5575\n",
      "BATCH NUMBER 352\n",
      "Epoch [0/5] Loss_D: 1.2599 Loss_G: 0.5040\n",
      "BATCH NUMBER 353\n",
      "Epoch [0/5] Loss_D: 1.2232 Loss_G: 0.5812\n",
      "BATCH NUMBER 354\n",
      "Epoch [0/5] Loss_D: 1.3671 Loss_G: 0.4500\n",
      "BATCH NUMBER 355\n",
      "Epoch [0/5] Loss_D: 1.3363 Loss_G: 0.5571\n",
      "BATCH NUMBER 356\n",
      "Epoch [0/5] Loss_D: 1.3185 Loss_G: 0.4704\n",
      "BATCH NUMBER 357\n",
      "Epoch [0/5] Loss_D: 1.2575 Loss_G: 0.5088\n",
      "BATCH NUMBER 358\n",
      "Epoch [0/5] Loss_D: 1.5038 Loss_G: 0.4429\n",
      "BATCH NUMBER 359\n",
      "Epoch [0/5] Loss_D: 1.4360 Loss_G: 0.5027\n",
      "BATCH NUMBER 360\n",
      "Epoch [0/5] Loss_D: 1.4045 Loss_G: 0.5022\n",
      "BATCH NUMBER 361\n",
      "Epoch [0/5] Loss_D: 1.3598 Loss_G: 0.5409\n",
      "BATCH NUMBER 362\n",
      "Epoch [0/5] Loss_D: 1.2627 Loss_G: 0.5089\n",
      "BATCH NUMBER 363\n",
      "Epoch [0/5] Loss_D: 1.4487 Loss_G: 0.4833\n",
      "BATCH NUMBER 364\n",
      "Epoch [0/5] Loss_D: 1.3384 Loss_G: 0.4872\n",
      "BATCH NUMBER 365\n",
      "Epoch [0/5] Loss_D: 1.1899 Loss_G: 0.5582\n",
      "BATCH NUMBER 366\n",
      "Epoch [0/5] Loss_D: 1.3536 Loss_G: 0.5294\n",
      "BATCH NUMBER 367\n",
      "Epoch [0/5] Loss_D: 1.4971 Loss_G: 0.4592\n",
      "BATCH NUMBER 368\n",
      "Epoch [0/5] Loss_D: 1.2877 Loss_G: 0.5747\n",
      "BATCH NUMBER 369\n",
      "Epoch [0/5] Loss_D: 1.2825 Loss_G: 0.4945\n",
      "BATCH NUMBER 370\n",
      "Epoch [0/5] Loss_D: 1.2438 Loss_G: 0.5167\n",
      "BATCH NUMBER 371\n",
      "Epoch [0/5] Loss_D: 1.2782 Loss_G: 0.4932\n",
      "BATCH NUMBER 372\n",
      "Epoch [0/5] Loss_D: 1.4177 Loss_G: 0.5113\n",
      "BATCH NUMBER 373\n",
      "Epoch [0/5] Loss_D: 1.1843 Loss_G: 0.5603\n",
      "BATCH NUMBER 374\n",
      "Epoch [0/5] Loss_D: 1.3865 Loss_G: 0.4332\n",
      "BATCH NUMBER 375\n",
      "Epoch [0/5] Loss_D: 1.3856 Loss_G: 0.5198\n",
      "BATCH NUMBER 376\n",
      "Epoch [0/5] Loss_D: 1.2481 Loss_G: 0.5154\n",
      "BATCH NUMBER 377\n",
      "Epoch [0/5] Loss_D: 1.2734 Loss_G: 0.5067\n",
      "BATCH NUMBER 378\n",
      "Epoch [0/5] Loss_D: 1.2636 Loss_G: 0.4972\n",
      "BATCH NUMBER 379\n",
      "Epoch [0/5] Loss_D: 1.1689 Loss_G: 0.5715\n",
      "BATCH NUMBER 380\n",
      "Epoch [0/5] Loss_D: 1.3532 Loss_G: 0.4586\n",
      "BATCH NUMBER 381\n",
      "Epoch [0/5] Loss_D: 1.3552 Loss_G: 0.4578\n",
      "BATCH NUMBER 382\n",
      "Epoch [0/5] Loss_D: 1.2453 Loss_G: 0.5196\n",
      "BATCH NUMBER 383\n",
      "Epoch [0/5] Loss_D: 1.4013 Loss_G: 0.5120\n",
      "BATCH NUMBER 384\n",
      "Epoch [0/5] Loss_D: 1.3210 Loss_G: 0.5336\n",
      "BATCH NUMBER 385\n",
      "Epoch [0/5] Loss_D: 1.2927 Loss_G: 0.5419\n",
      "BATCH NUMBER 386\n",
      "Epoch [0/5] Loss_D: 1.3871 Loss_G: 0.5335\n",
      "BATCH NUMBER 387\n",
      "Epoch [0/5] Loss_D: 1.2535 Loss_G: 0.5169\n",
      "BATCH NUMBER 388\n",
      "Epoch [0/5] Loss_D: 1.3093 Loss_G: 0.4853\n",
      "BATCH NUMBER 389\n",
      "Epoch [0/5] Loss_D: 1.2831 Loss_G: 0.5410\n",
      "BATCH NUMBER 390\n",
      "Epoch [0/5] Loss_D: 1.1563 Loss_G: 0.5816\n",
      "BATCH NUMBER 391\n",
      "Epoch [0/5] Loss_D: 1.2733 Loss_G: 0.4908\n",
      "BATCH NUMBER 392\n",
      "Epoch [0/5] Loss_D: 1.2160 Loss_G: 0.5463\n",
      "BATCH NUMBER 393\n",
      "Epoch [0/5] Loss_D: 1.1859 Loss_G: 0.5577\n",
      "BATCH NUMBER 394\n",
      "Epoch [0/5] Loss_D: 1.2437 Loss_G: 0.5164\n",
      "BATCH NUMBER 395\n",
      "Epoch [0/5] Loss_D: 1.2172 Loss_G: 0.5482\n",
      "BATCH NUMBER 396\n",
      "Epoch [0/5] Loss_D: 1.2671 Loss_G: 0.5092\n",
      "BATCH NUMBER 397\n",
      "Epoch [0/5] Loss_D: 1.2984 Loss_G: 0.5398\n",
      "BATCH NUMBER 398\n",
      "Epoch [0/5] Loss_D: 1.2427 Loss_G: 0.5222\n",
      "BATCH NUMBER 399\n",
      "Epoch [0/5] Loss_D: 1.2479 Loss_G: 0.5138\n",
      "BATCH NUMBER 400\n",
      "Epoch [0/5] Loss_D: 1.2496 Loss_G: 0.5158\n",
      "BATCH NUMBER 401\n",
      "Epoch [0/5] Loss_D: 1.2537 Loss_G: 0.5052\n",
      "BATCH NUMBER 402\n",
      "Epoch [0/5] Loss_D: 1.2454 Loss_G: 0.5240\n",
      "BATCH NUMBER 403\n",
      "Epoch [0/5] Loss_D: 1.2371 Loss_G: 0.5234\n",
      "BATCH NUMBER 404\n",
      "Epoch [0/5] Loss_D: 1.1582 Loss_G: 0.5802\n",
      "BATCH NUMBER 405\n",
      "Epoch [0/5] Loss_D: 1.2268 Loss_G: 0.5246\n",
      "BATCH NUMBER 406\n",
      "Epoch [0/5] Loss_D: 1.3083 Loss_G: 0.4929\n",
      "BATCH NUMBER 407\n",
      "Epoch [0/5] Loss_D: 1.2371 Loss_G: 0.5247\n",
      "BATCH NUMBER 408\n",
      "Epoch [0/5] Loss_D: 1.2497 Loss_G: 0.5127\n",
      "BATCH NUMBER 409\n",
      "Epoch [0/5] Loss_D: 1.2543 Loss_G: 0.5142\n",
      "BATCH NUMBER 410\n",
      "Epoch [0/5] Loss_D: 1.3897 Loss_G: 0.4318\n",
      "BATCH NUMBER 411\n",
      "Epoch [0/5] Loss_D: 1.2271 Loss_G: 0.5267\n",
      "BATCH NUMBER 412\n",
      "Epoch [0/5] Loss_D: 1.2277 Loss_G: 0.5238\n",
      "BATCH NUMBER 413\n",
      "Epoch [0/5] Loss_D: 1.2983 Loss_G: 0.4796\n",
      "BATCH NUMBER 414\n",
      "Epoch [0/5] Loss_D: 1.1846 Loss_G: 0.5612\n",
      "BATCH NUMBER 415\n",
      "Epoch [0/5] Loss_D: 1.3268 Loss_G: 0.5633\n",
      "BATCH NUMBER 416\n",
      "Epoch [0/5] Loss_D: 1.1903 Loss_G: 0.5556\n",
      "BATCH NUMBER 417\n",
      "Epoch [0/5] Loss_D: 1.1532 Loss_G: 0.5831\n",
      "BATCH NUMBER 418\n",
      "Epoch [0/5] Loss_D: 1.3281 Loss_G: 0.4663\n",
      "BATCH NUMBER 419\n",
      "Epoch [0/5] Loss_D: 1.3562 Loss_G: 0.4558\n",
      "BATCH NUMBER 420\n",
      "Epoch [0/5] Loss_D: 1.4682 Loss_G: 0.4780\n",
      "BATCH NUMBER 421\n",
      "Epoch [0/5] Loss_D: 1.2526 Loss_G: 0.5155\n",
      "BATCH NUMBER 422\n",
      "Epoch [0/5] Loss_D: 1.3198 Loss_G: 0.4770\n",
      "BATCH NUMBER 423\n",
      "Epoch [0/5] Loss_D: 1.2538 Loss_G: 0.5229\n",
      "BATCH NUMBER 424\n",
      "Epoch [0/5] Loss_D: 1.2228 Loss_G: 0.5314\n",
      "BATCH NUMBER 425\n",
      "Epoch [0/5] Loss_D: 1.2995 Loss_G: 0.5848\n",
      "BATCH NUMBER 426\n",
      "Epoch [0/5] Loss_D: 1.2707 Loss_G: 0.4941\n",
      "BATCH NUMBER 427\n",
      "Epoch [0/5] Loss_D: 1.3752 Loss_G: 0.4434\n",
      "BATCH NUMBER 428\n",
      "Epoch [0/5] Loss_D: 1.2008 Loss_G: 0.5479\n",
      "BATCH NUMBER 429\n",
      "Epoch [0/5] Loss_D: 1.2768 Loss_G: 0.5407\n",
      "BATCH NUMBER 430\n",
      "Epoch [0/5] Loss_D: 1.4332 Loss_G: 0.4947\n",
      "BATCH NUMBER 431\n",
      "Epoch [0/5] Loss_D: 1.4563 Loss_G: 0.4297\n",
      "BATCH NUMBER 432\n",
      "Epoch [0/5] Loss_D: 1.3553 Loss_G: 0.4614\n",
      "BATCH NUMBER 433\n",
      "Epoch [0/5] Loss_D: 1.2792 Loss_G: 0.5515\n",
      "BATCH NUMBER 434\n",
      "Epoch [0/5] Loss_D: 1.1817 Loss_G: 0.5606\n",
      "BATCH NUMBER 435\n",
      "Epoch [0/5] Loss_D: 1.2600 Loss_G: 0.5061\n",
      "BATCH NUMBER 436\n",
      "Epoch [0/5] Loss_D: 1.3492 Loss_G: 0.5490\n",
      "BATCH NUMBER 437\n",
      "Epoch [0/5] Loss_D: 1.1429 Loss_G: 0.5875\n",
      "BATCH NUMBER 438\n",
      "Epoch [0/5] Loss_D: 1.3673 Loss_G: 0.5172\n",
      "BATCH NUMBER 439\n",
      "Epoch [0/5] Loss_D: 1.1992 Loss_G: 0.5517\n",
      "BATCH NUMBER 440\n",
      "Epoch [0/5] Loss_D: 1.2030 Loss_G: 0.5493\n",
      "BATCH NUMBER 441\n",
      "Epoch [0/5] Loss_D: 1.2219 Loss_G: 0.5300\n",
      "BATCH NUMBER 442\n",
      "Epoch [0/5] Loss_D: 1.2279 Loss_G: 0.5288\n",
      "BATCH NUMBER 443\n",
      "Epoch [0/5] Loss_D: 1.2131 Loss_G: 0.5480\n",
      "BATCH NUMBER 444\n",
      "Epoch [0/5] Loss_D: 1.2148 Loss_G: 0.5313\n",
      "BATCH NUMBER 445\n",
      "Epoch [0/5] Loss_D: 1.1463 Loss_G: 0.5869\n",
      "BATCH NUMBER 446\n",
      "Epoch [0/5] Loss_D: 1.1642 Loss_G: 0.5720\n",
      "BATCH NUMBER 447\n",
      "Epoch [0/5] Loss_D: 1.1694 Loss_G: 0.5698\n",
      "BATCH NUMBER 448\n",
      "Epoch [0/5] Loss_D: 1.2108 Loss_G: 0.5387\n",
      "BATCH NUMBER 449\n",
      "Epoch [0/5] Loss_D: 1.2240 Loss_G: 0.5306\n",
      "BATCH NUMBER 450\n",
      "Epoch [0/5] Loss_D: 1.2729 Loss_G: 0.4985\n",
      "BATCH NUMBER 451\n",
      "Epoch [0/5] Loss_D: 1.4247 Loss_G: 0.5064\n",
      "BATCH NUMBER 452\n",
      "Epoch [0/5] Loss_D: 1.1302 Loss_G: 0.6015\n",
      "BATCH NUMBER 453\n",
      "Epoch [0/5] Loss_D: 1.4313 Loss_G: 0.5065\n",
      "BATCH NUMBER 454\n",
      "Epoch [0/5] Loss_D: 1.3279 Loss_G: 0.4708\n",
      "BATCH NUMBER 455\n",
      "Epoch [0/5] Loss_D: 1.1581 Loss_G: 0.5773\n",
      "BATCH NUMBER 456\n",
      "Epoch [0/5] Loss_D: 1.3464 Loss_G: 0.5364\n",
      "BATCH NUMBER 457\n",
      "Epoch [0/5] Loss_D: 1.3018 Loss_G: 0.4901\n",
      "BATCH NUMBER 458\n",
      "Epoch [0/5] Loss_D: 1.2719 Loss_G: 0.5093\n",
      "BATCH NUMBER 459\n",
      "Epoch [0/5] Loss_D: 1.2432 Loss_G: 0.5192\n",
      "BATCH NUMBER 460\n",
      "Epoch [0/5] Loss_D: 1.2704 Loss_G: 0.5077\n",
      "BATCH NUMBER 461\n",
      "Epoch [0/5] Loss_D: 1.5570 Loss_G: 0.4212\n",
      "BATCH NUMBER 462\n",
      "Epoch [0/5] Loss_D: 1.2164 Loss_G: 0.5400\n",
      "BATCH NUMBER 463\n",
      "Epoch [0/5] Loss_D: 1.2265 Loss_G: 0.5370\n",
      "BATCH NUMBER 464\n",
      "Epoch [0/5] Loss_D: 1.2142 Loss_G: 0.5360\n",
      "BATCH NUMBER 465\n",
      "Epoch [0/5] Loss_D: 1.2414 Loss_G: 0.5247\n",
      "BATCH NUMBER 466\n",
      "Epoch [0/5] Loss_D: 1.2443 Loss_G: 0.5175\n",
      "BATCH NUMBER 467\n",
      "Epoch [0/5] Loss_D: 1.3783 Loss_G: 0.4885\n",
      "BATCH NUMBER 468\n",
      "Epoch [0/5] Loss_D: 1.2697 Loss_G: 0.5028\n",
      "BATCH NUMBER 469\n",
      "Epoch [0/5] Loss_D: 1.2850 Loss_G: 0.4847\n",
      "BATCH NUMBER 470\n",
      "Epoch [0/5] Loss_D: 1.2463 Loss_G: 0.5524\n",
      "BATCH NUMBER 471\n",
      "Epoch [0/5] Loss_D: 1.3235 Loss_G: 0.5287\n",
      "BATCH NUMBER 472\n",
      "Epoch [0/5] Loss_D: 1.2437 Loss_G: 0.5302\n",
      "BATCH NUMBER 473\n",
      "Epoch [0/5] Loss_D: 1.3192 Loss_G: 0.4676\n",
      "BATCH NUMBER 474\n",
      "Epoch [0/5] Loss_D: 1.2443 Loss_G: 0.5241\n",
      "BATCH NUMBER 475\n",
      "Epoch [0/5] Loss_D: 1.2533 Loss_G: 0.5098\n",
      "BATCH NUMBER 476\n",
      "Epoch [0/5] Loss_D: 1.2688 Loss_G: 0.5007\n",
      "BATCH NUMBER 477\n",
      "Epoch [0/5] Loss_D: 1.3337 Loss_G: 0.4620\n",
      "BATCH NUMBER 478\n",
      "Epoch [0/5] Loss_D: 1.3220 Loss_G: 0.4736\n",
      "BATCH NUMBER 479\n",
      "Epoch [0/5] Loss_D: 1.2750 Loss_G: 0.5779\n",
      "BATCH NUMBER 480\n",
      "Epoch [0/5] Loss_D: 1.2598 Loss_G: 0.5107\n",
      "BATCH NUMBER 481\n",
      "Epoch [0/5] Loss_D: 1.5505 Loss_G: 0.4334\n",
      "BATCH NUMBER 482\n",
      "Epoch [0/5] Loss_D: 1.3873 Loss_G: 0.5087\n",
      "BATCH NUMBER 483\n",
      "Epoch [0/5] Loss_D: 1.3163 Loss_G: 0.5156\n",
      "BATCH NUMBER 484\n",
      "Epoch [0/5] Loss_D: 1.2286 Loss_G: 0.5274\n",
      "BATCH NUMBER 485\n",
      "Epoch [0/5] Loss_D: 1.3714 Loss_G: 0.5456\n",
      "BATCH NUMBER 486\n",
      "Epoch [0/5] Loss_D: 1.3355 Loss_G: 0.4864\n",
      "BATCH NUMBER 487\n",
      "Epoch [0/5] Loss_D: 1.2863 Loss_G: 0.4932\n",
      "BATCH NUMBER 488\n",
      "Epoch [0/5] Loss_D: 1.4108 Loss_G: 0.4953\n",
      "BATCH NUMBER 489\n",
      "Epoch [0/5] Loss_D: 1.2251 Loss_G: 0.5351\n",
      "BATCH NUMBER 490\n",
      "Epoch [0/5] Loss_D: 1.2316 Loss_G: 0.5215\n",
      "BATCH NUMBER 491\n",
      "Epoch [0/5] Loss_D: 1.2548 Loss_G: 0.5935\n",
      "BATCH NUMBER 492\n",
      "Epoch [0/5] Loss_D: 1.3824 Loss_G: 0.4855\n",
      "BATCH NUMBER 493\n",
      "Epoch [0/5] Loss_D: 1.3264 Loss_G: 0.5420\n",
      "BATCH NUMBER 494\n",
      "Epoch [0/5] Loss_D: 1.4646 Loss_G: 0.4715\n",
      "BATCH NUMBER 495\n",
      "Epoch [0/5] Loss_D: 1.3114 Loss_G: 0.5234\n",
      "BATCH NUMBER 496\n",
      "Epoch [0/5] Loss_D: 1.1818 Loss_G: 0.5631\n",
      "BATCH NUMBER 497\n",
      "Epoch [0/5] Loss_D: 1.2888 Loss_G: 0.4864\n",
      "BATCH NUMBER 498\n",
      "Epoch [0/5] Loss_D: 1.2616 Loss_G: 0.5153\n",
      "BATCH NUMBER 499\n",
      "Epoch [0/5] Loss_D: 1.2761 Loss_G: 0.4961\n",
      "BATCH NUMBER 500\n",
      "Epoch [0/5] Loss_D: 1.3609 Loss_G: 0.5144\n",
      "BATCH NUMBER 501\n",
      "Epoch [0/5] Loss_D: 1.2168 Loss_G: 0.5318\n",
      "BATCH NUMBER 502\n",
      "Epoch [0/5] Loss_D: 1.1847 Loss_G: 0.5625\n",
      "BATCH NUMBER 503\n",
      "Epoch [0/5] Loss_D: 1.3959 Loss_G: 0.5036\n",
      "BATCH NUMBER 504\n",
      "Epoch [0/5] Loss_D: 1.2702 Loss_G: 0.5018\n",
      "BATCH NUMBER 505\n",
      "Epoch [0/5] Loss_D: 1.3223 Loss_G: 0.5522\n",
      "BATCH NUMBER 506\n",
      "Epoch [0/5] Loss_D: 1.1700 Loss_G: 0.5751\n",
      "BATCH NUMBER 507\n",
      "Epoch [0/5] Loss_D: 1.1957 Loss_G: 0.5550\n",
      "BATCH NUMBER 508\n",
      "Epoch [0/5] Loss_D: 1.3174 Loss_G: 0.5098\n",
      "BATCH NUMBER 509\n",
      "Epoch [0/5] Loss_D: 1.3268 Loss_G: 0.5026\n",
      "BATCH NUMBER 510\n",
      "Epoch [0/5] Loss_D: 1.3983 Loss_G: 0.4859\n",
      "BATCH NUMBER 511\n",
      "Epoch [0/5] Loss_D: 1.1882 Loss_G: 0.5514\n",
      "BATCH NUMBER 512\n",
      "Epoch [0/5] Loss_D: 1.3990 Loss_G: 0.4326\n",
      "BATCH NUMBER 513\n",
      "Epoch [0/5] Loss_D: 1.1968 Loss_G: 0.5442\n",
      "BATCH NUMBER 514\n",
      "Epoch [0/5] Loss_D: 1.2916 Loss_G: 0.4865\n",
      "BATCH NUMBER 515\n",
      "Epoch [0/5] Loss_D: 1.1589 Loss_G: 0.5755\n",
      "BATCH NUMBER 516\n",
      "Epoch [0/5] Loss_D: 1.2251 Loss_G: 0.5427\n",
      "BATCH NUMBER 517\n",
      "Epoch [0/5] Loss_D: 1.3476 Loss_G: 0.4969\n",
      "BATCH NUMBER 518\n",
      "Epoch [0/5] Loss_D: 1.2539 Loss_G: 0.5037\n",
      "BATCH NUMBER 519\n",
      "Epoch [0/5] Loss_D: 1.3079 Loss_G: 0.5798\n",
      "BATCH NUMBER 520\n",
      "Epoch [0/5] Loss_D: 1.1867 Loss_G: 0.5557\n",
      "BATCH NUMBER 521\n",
      "Epoch [0/5] Loss_D: 1.2995 Loss_G: 0.5396\n",
      "BATCH NUMBER 522\n",
      "Epoch [0/5] Loss_D: 1.2132 Loss_G: 0.5407\n",
      "BATCH NUMBER 523\n",
      "Epoch [0/5] Loss_D: 1.1713 Loss_G: 0.5732\n",
      "BATCH NUMBER 524\n",
      "Epoch [0/5] Loss_D: 1.2044 Loss_G: 0.5369\n",
      "BATCH NUMBER 525\n",
      "Epoch [0/5] Loss_D: 1.3058 Loss_G: 0.5548\n",
      "BATCH NUMBER 526\n",
      "Epoch [0/5] Loss_D: 1.3631 Loss_G: 0.4509\n",
      "BATCH NUMBER 527\n",
      "Epoch [0/5] Loss_D: 1.1808 Loss_G: 0.5652\n",
      "BATCH NUMBER 528\n",
      "Epoch [0/5] Loss_D: 1.2648 Loss_G: 0.5039\n",
      "BATCH NUMBER 529\n",
      "Epoch [0/5] Loss_D: 1.2660 Loss_G: 0.5049\n",
      "BATCH NUMBER 530\n",
      "Epoch [0/5] Loss_D: 1.2741 Loss_G: 0.4928\n",
      "BATCH NUMBER 531\n",
      "Epoch [0/5] Loss_D: 1.3151 Loss_G: 0.5250\n",
      "BATCH NUMBER 532\n",
      "Epoch [0/5] Loss_D: 1.2652 Loss_G: 0.5050\n",
      "BATCH NUMBER 533\n",
      "Epoch [0/5] Loss_D: 1.2637 Loss_G: 0.5714\n",
      "BATCH NUMBER 534\n",
      "Epoch [0/5] Loss_D: 1.2065 Loss_G: 0.5396\n",
      "BATCH NUMBER 535\n",
      "Epoch [0/5] Loss_D: 1.3215 Loss_G: 0.4654\n",
      "BATCH NUMBER 536\n",
      "Epoch [0/5] Loss_D: 1.4245 Loss_G: 0.4741\n",
      "BATCH NUMBER 537\n",
      "Epoch [0/5] Loss_D: 1.2343 Loss_G: 0.5179\n",
      "BATCH NUMBER 538\n",
      "Epoch [0/5] Loss_D: 1.2780 Loss_G: 0.4963\n",
      "BATCH NUMBER 539\n",
      "Epoch [0/5] Loss_D: 1.2441 Loss_G: 0.5170\n",
      "BATCH NUMBER 540\n",
      "Epoch [0/5] Loss_D: 1.2974 Loss_G: 0.4755\n",
      "BATCH NUMBER 541\n",
      "Epoch [0/5] Loss_D: 1.2127 Loss_G: 0.5344\n",
      "BATCH NUMBER 542\n",
      "Epoch [0/5] Loss_D: 1.2222 Loss_G: 0.5287\n",
      "BATCH NUMBER 543\n",
      "Epoch [0/5] Loss_D: 1.2164 Loss_G: 0.5356\n",
      "BATCH NUMBER 544\n",
      "Epoch [0/5] Loss_D: 1.2445 Loss_G: 0.5067\n",
      "BATCH NUMBER 545\n",
      "Epoch [0/5] Loss_D: 1.2849 Loss_G: 0.4960\n",
      "BATCH NUMBER 546\n",
      "Epoch [0/5] Loss_D: 1.3919 Loss_G: 0.5082\n",
      "BATCH NUMBER 547\n",
      "Epoch [0/5] Loss_D: 1.2317 Loss_G: 0.5207\n",
      "BATCH NUMBER 548\n",
      "Epoch [0/5] Loss_D: 1.1598 Loss_G: 0.5720\n",
      "BATCH NUMBER 549\n",
      "Epoch [0/5] Loss_D: 1.2673 Loss_G: 0.5116\n",
      "BATCH NUMBER 550\n",
      "Epoch [0/5] Loss_D: 1.1805 Loss_G: 0.5570\n",
      "BATCH NUMBER 551\n",
      "Epoch [0/5] Loss_D: 1.3315 Loss_G: 0.5371\n",
      "BATCH NUMBER 552\n",
      "Epoch [0/5] Loss_D: 1.2786 Loss_G: 0.4954\n",
      "BATCH NUMBER 553\n",
      "Epoch [0/5] Loss_D: 1.2909 Loss_G: 0.4919\n",
      "BATCH NUMBER 554\n",
      "Epoch [0/5] Loss_D: 1.1845 Loss_G: 0.5565\n",
      "BATCH NUMBER 555\n",
      "Epoch [0/5] Loss_D: 1.4039 Loss_G: 0.4834\n",
      "BATCH NUMBER 556\n",
      "Epoch [0/5] Loss_D: 1.2443 Loss_G: 0.5158\n",
      "BATCH NUMBER 557\n",
      "Epoch [0/5] Loss_D: 1.1563 Loss_G: 0.5811\n",
      "BATCH NUMBER 558\n",
      "Epoch [0/5] Loss_D: 1.2989 Loss_G: 0.4764\n",
      "BATCH NUMBER 559\n",
      "Epoch [0/5] Loss_D: 1.3416 Loss_G: 0.5223\n",
      "BATCH NUMBER 560\n",
      "Epoch [0/5] Loss_D: 1.2951 Loss_G: 0.4836\n",
      "BATCH NUMBER 561\n",
      "Epoch [0/5] Loss_D: 1.2803 Loss_G: 0.5728\n",
      "BATCH NUMBER 562\n",
      "Epoch [0/5] Loss_D: 1.2728 Loss_G: 0.5955\n",
      "BATCH NUMBER 563\n",
      "Epoch [0/5] Loss_D: 1.2323 Loss_G: 0.5277\n",
      "BATCH NUMBER 564\n",
      "Epoch [0/5] Loss_D: 1.2174 Loss_G: 0.5327\n",
      "BATCH NUMBER 565\n",
      "Epoch [0/5] Loss_D: 1.1797 Loss_G: 0.5636\n",
      "BATCH NUMBER 566\n",
      "Epoch [0/5] Loss_D: 1.3119 Loss_G: 0.5335\n",
      "BATCH NUMBER 567\n",
      "Epoch [0/5] Loss_D: 1.2646 Loss_G: 0.5459\n",
      "BATCH NUMBER 568\n",
      "Epoch [0/5] Loss_D: 1.2591 Loss_G: 0.5010\n",
      "BATCH NUMBER 569\n",
      "Epoch [0/5] Loss_D: 1.1957 Loss_G: 0.5449\n",
      "BATCH NUMBER 570\n",
      "Epoch [0/5] Loss_D: 1.2757 Loss_G: 0.6061\n",
      "BATCH NUMBER 571\n",
      "Epoch [0/5] Loss_D: 1.2953 Loss_G: 0.4913\n",
      "BATCH NUMBER 572\n",
      "Epoch [0/5] Loss_D: 1.3350 Loss_G: 0.5328\n",
      "BATCH NUMBER 573\n",
      "Epoch [0/5] Loss_D: 1.3157 Loss_G: 0.5683\n",
      "BATCH NUMBER 574\n",
      "Epoch [0/5] Loss_D: 1.2166 Loss_G: 0.5282\n",
      "BATCH NUMBER 575\n",
      "Epoch [0/5] Loss_D: 1.2292 Loss_G: 0.5353\n",
      "BATCH NUMBER 576\n",
      "Epoch [0/5] Loss_D: 1.2604 Loss_G: 0.5139\n",
      "BATCH NUMBER 577\n",
      "Epoch [0/5] Loss_D: 1.2005 Loss_G: 0.5425\n",
      "BATCH NUMBER 578\n",
      "Epoch [0/5] Loss_D: 1.3528 Loss_G: 0.4775\n",
      "BATCH NUMBER 579\n",
      "Epoch [0/5] Loss_D: 1.1977 Loss_G: 0.5511\n",
      "BATCH NUMBER 580\n",
      "Epoch [0/5] Loss_D: 1.1549 Loss_G: 0.5748\n",
      "BATCH NUMBER 581\n",
      "Epoch [0/5] Loss_D: 1.1318 Loss_G: 0.5949\n",
      "BATCH NUMBER 582\n",
      "Epoch [0/5] Loss_D: 1.2423 Loss_G: 0.5144\n",
      "BATCH NUMBER 583\n",
      "Epoch [0/5] Loss_D: 1.3176 Loss_G: 0.4769\n",
      "BATCH NUMBER 584\n",
      "Epoch [0/5] Loss_D: 1.2378 Loss_G: 0.5257\n",
      "BATCH NUMBER 585\n",
      "Epoch [0/5] Loss_D: 1.2418 Loss_G: 0.5192\n",
      "BATCH NUMBER 586\n",
      "Epoch [0/5] Loss_D: 1.2524 Loss_G: 0.5131\n",
      "BATCH NUMBER 587\n",
      "Epoch [0/5] Loss_D: 1.2173 Loss_G: 0.5390\n",
      "BATCH NUMBER 588\n",
      "Epoch [0/5] Loss_D: 1.2287 Loss_G: 0.5537\n",
      "BATCH NUMBER 589\n",
      "Epoch [0/5] Loss_D: 1.3198 Loss_G: 0.4862\n",
      "BATCH NUMBER 590\n",
      "Epoch [0/5] Loss_D: 1.3075 Loss_G: 0.4790\n",
      "BATCH NUMBER 591\n",
      "Epoch [0/5] Loss_D: 1.2448 Loss_G: 0.5469\n",
      "BATCH NUMBER 592\n",
      "Epoch [0/5] Loss_D: 1.1996 Loss_G: 0.5498\n",
      "BATCH NUMBER 593\n",
      "Epoch [0/5] Loss_D: 1.3207 Loss_G: 0.5961\n",
      "BATCH NUMBER 594\n",
      "Epoch [0/5] Loss_D: 1.4508 Loss_G: 0.5130\n",
      "BATCH NUMBER 595\n",
      "Epoch [0/5] Loss_D: 1.2101 Loss_G: 0.5425\n",
      "BATCH NUMBER 596\n",
      "Epoch [0/5] Loss_D: 1.3683 Loss_G: 0.5075\n",
      "BATCH NUMBER 597\n",
      "Epoch [0/5] Loss_D: 1.1695 Loss_G: 0.5686\n",
      "BATCH NUMBER 598\n",
      "Epoch [0/5] Loss_D: 1.3815 Loss_G: 0.4714\n",
      "BATCH NUMBER 599\n",
      "Epoch [0/5] Loss_D: 1.4491 Loss_G: 0.4121\n",
      "BATCH NUMBER 600\n",
      "Epoch [0/5] Loss_D: 1.1465 Loss_G: 0.5884\n",
      "BATCH NUMBER 601\n",
      "Epoch [0/5] Loss_D: 1.2408 Loss_G: 0.5107\n",
      "BATCH NUMBER 602\n",
      "Epoch [0/5] Loss_D: 1.1732 Loss_G: 0.5649\n",
      "BATCH NUMBER 603\n",
      "Epoch [0/5] Loss_D: 1.2206 Loss_G: 0.5328\n",
      "BATCH NUMBER 604\n",
      "Epoch [0/5] Loss_D: 1.1884 Loss_G: 0.5545\n",
      "BATCH NUMBER 605\n",
      "Epoch [0/5] Loss_D: 1.3226 Loss_G: 0.4787\n",
      "BATCH NUMBER 606\n",
      "Epoch [0/5] Loss_D: 1.1789 Loss_G: 0.5560\n",
      "BATCH NUMBER 607\n",
      "Epoch [0/5] Loss_D: 1.3298 Loss_G: 0.4609\n",
      "BATCH NUMBER 608\n",
      "Epoch [0/5] Loss_D: 1.2703 Loss_G: 0.5090\n",
      "BATCH NUMBER 609\n",
      "Epoch [0/5] Loss_D: 1.3556 Loss_G: 0.4796\n",
      "BATCH NUMBER 610\n",
      "Epoch [0/5] Loss_D: 1.1329 Loss_G: 0.5941\n",
      "BATCH NUMBER 611\n",
      "Epoch [0/5] Loss_D: 1.2408 Loss_G: 0.5166\n",
      "BATCH NUMBER 612\n",
      "Epoch [0/5] Loss_D: 1.2868 Loss_G: 0.4934\n",
      "BATCH NUMBER 613\n",
      "Epoch [0/5] Loss_D: 1.2794 Loss_G: 0.4947\n",
      "BATCH NUMBER 614\n",
      "Epoch [0/5] Loss_D: 1.1808 Loss_G: 0.5562\n",
      "BATCH NUMBER 615\n",
      "Epoch [0/5] Loss_D: 1.1464 Loss_G: 0.5873\n",
      "BATCH NUMBER 616\n",
      "Epoch [0/5] Loss_D: 1.2397 Loss_G: 0.5200\n",
      "BATCH NUMBER 617\n",
      "Epoch [0/5] Loss_D: 1.1633 Loss_G: 0.5713\n",
      "BATCH NUMBER 618\n",
      "Epoch [0/5] Loss_D: 1.1663 Loss_G: 0.5747\n",
      "BATCH NUMBER 619\n",
      "Epoch [0/5] Loss_D: 1.1672 Loss_G: 0.5709\n",
      "BATCH NUMBER 620\n",
      "Epoch [0/5] Loss_D: 1.3580 Loss_G: 0.4462\n",
      "BATCH NUMBER 621\n",
      "Epoch [0/5] Loss_D: 1.2234 Loss_G: 0.5832\n",
      "BATCH NUMBER 622\n",
      "Epoch [0/5] Loss_D: 1.2018 Loss_G: 0.5488\n",
      "BATCH NUMBER 623\n",
      "Epoch [0/5] Loss_D: 1.3053 Loss_G: 0.4742\n",
      "BATCH NUMBER 624\n",
      "Epoch [0/5] Loss_D: 1.3992 Loss_G: 0.5087\n",
      "BATCH NUMBER 625\n",
      "Epoch [0/5] Loss_D: 1.2178 Loss_G: 0.5314\n",
      "BATCH NUMBER 626\n",
      "Epoch [0/5] Loss_D: 1.2633 Loss_G: 0.4999\n",
      "BATCH NUMBER 627\n",
      "Epoch [0/5] Loss_D: 1.3114 Loss_G: 0.5065\n",
      "BATCH NUMBER 628\n",
      "Epoch [0/5] Loss_D: 1.2680 Loss_G: 0.5053\n",
      "BATCH NUMBER 629\n",
      "Epoch [0/5] Loss_D: 1.2610 Loss_G: 0.5867\n",
      "BATCH NUMBER 630\n",
      "Epoch [0/5] Loss_D: 1.2264 Loss_G: 0.5241\n",
      "BATCH NUMBER 631\n",
      "Epoch [0/5] Loss_D: 1.1286 Loss_G: 0.6014\n",
      "BATCH NUMBER 632\n",
      "Epoch [0/5] Loss_D: 1.2442 Loss_G: 0.5770\n",
      "BATCH NUMBER 633\n",
      "Epoch [0/5] Loss_D: 1.2467 Loss_G: 0.5172\n",
      "BATCH NUMBER 634\n",
      "Epoch [0/5] Loss_D: 1.1498 Loss_G: 0.5800\n",
      "BATCH NUMBER 635\n",
      "Epoch [0/5] Loss_D: 1.2162 Loss_G: 0.5341\n",
      "BATCH NUMBER 636\n",
      "Epoch [0/5] Loss_D: 1.1960 Loss_G: 0.5489\n",
      "BATCH NUMBER 637\n",
      "Epoch [0/5] Loss_D: 1.2520 Loss_G: 0.5806\n",
      "BATCH NUMBER 638\n",
      "Epoch [0/5] Loss_D: 1.1938 Loss_G: 0.5460\n",
      "BATCH NUMBER 639\n",
      "Epoch [0/5] Loss_D: 1.3157 Loss_G: 0.5095\n",
      "BATCH NUMBER 640\n",
      "Epoch [0/5] Loss_D: 1.2348 Loss_G: 0.5336\n",
      "BATCH NUMBER 641\n",
      "Epoch [0/5] Loss_D: 1.4382 Loss_G: 0.4926\n",
      "BATCH NUMBER 642\n",
      "Epoch [0/5] Loss_D: 1.2249 Loss_G: 0.5286\n",
      "BATCH NUMBER 643\n",
      "Epoch [0/5] Loss_D: 1.2621 Loss_G: 0.5136\n",
      "BATCH NUMBER 644\n",
      "Epoch [0/5] Loss_D: 1.1409 Loss_G: 0.5934\n",
      "BATCH NUMBER 645\n",
      "Epoch [0/5] Loss_D: 1.1374 Loss_G: 0.5894\n",
      "BATCH NUMBER 646\n",
      "Epoch [0/5] Loss_D: 1.2193 Loss_G: 0.5282\n",
      "BATCH NUMBER 647\n",
      "Epoch [0/5] Loss_D: 1.3660 Loss_G: 0.4518\n",
      "BATCH NUMBER 648\n",
      "Epoch [0/5] Loss_D: 1.3744 Loss_G: 0.4432\n",
      "BATCH NUMBER 649\n",
      "Epoch [0/5] Loss_D: 1.2580 Loss_G: 0.5600\n",
      "BATCH NUMBER 650\n",
      "Epoch [0/5] Loss_D: 1.1836 Loss_G: 0.5635\n",
      "BATCH NUMBER 651\n",
      "Epoch [0/5] Loss_D: 1.3154 Loss_G: 0.4841\n",
      "BATCH NUMBER 652\n",
      "Epoch [0/5] Loss_D: 1.1642 Loss_G: 0.5685\n",
      "BATCH NUMBER 653\n",
      "Epoch [0/5] Loss_D: 1.1718 Loss_G: 0.5645\n",
      "BATCH NUMBER 654\n",
      "Epoch [0/5] Loss_D: 1.2215 Loss_G: 0.5338\n",
      "BATCH NUMBER 655\n",
      "Epoch [0/5] Loss_D: 1.2479 Loss_G: 0.5505\n",
      "BATCH NUMBER 656\n",
      "Epoch [0/5] Loss_D: 1.1789 Loss_G: 0.5586\n",
      "BATCH NUMBER 657\n",
      "Epoch [0/5] Loss_D: 1.3658 Loss_G: 0.5430\n",
      "BATCH NUMBER 658\n",
      "Epoch [0/5] Loss_D: 1.2380 Loss_G: 0.5160\n",
      "BATCH NUMBER 659\n",
      "Epoch [0/5] Loss_D: 1.2271 Loss_G: 0.5299\n",
      "BATCH NUMBER 660\n",
      "Epoch [0/5] Loss_D: 1.2327 Loss_G: 0.5891\n",
      "BATCH NUMBER 661\n",
      "Epoch [0/5] Loss_D: 1.1473 Loss_G: 0.5820\n",
      "BATCH NUMBER 662\n",
      "Epoch [0/5] Loss_D: 1.1773 Loss_G: 0.5606\n",
      "BATCH NUMBER 663\n",
      "Epoch [0/5] Loss_D: 1.2587 Loss_G: 0.5079\n",
      "BATCH NUMBER 664\n",
      "Epoch [0/5] Loss_D: 1.2595 Loss_G: 0.5124\n",
      "BATCH NUMBER 665\n",
      "Epoch [0/5] Loss_D: 1.2135 Loss_G: 0.5307\n",
      "BATCH NUMBER 666\n",
      "Epoch [0/5] Loss_D: 1.3469 Loss_G: 0.5481\n",
      "BATCH NUMBER 667\n",
      "Epoch [0/5] Loss_D: 1.2089 Loss_G: 0.5383\n",
      "BATCH NUMBER 668\n",
      "Epoch [0/5] Loss_D: 1.3402 Loss_G: 0.5229\n",
      "BATCH NUMBER 669\n",
      "Epoch [0/5] Loss_D: 1.2623 Loss_G: 0.5080\n",
      "BATCH NUMBER 670\n",
      "Epoch [0/5] Loss_D: 1.4124 Loss_G: 0.5009\n",
      "BATCH NUMBER 671\n",
      "Epoch [0/5] Loss_D: 1.3568 Loss_G: 0.4587\n",
      "BATCH NUMBER 672\n",
      "Epoch [0/5] Loss_D: 1.2670 Loss_G: 0.5102\n",
      "BATCH NUMBER 673\n",
      "Epoch [0/5] Loss_D: 1.1675 Loss_G: 0.5658\n",
      "BATCH NUMBER 674\n",
      "Epoch [0/5] Loss_D: 1.1532 Loss_G: 0.5765\n",
      "BATCH NUMBER 675\n",
      "Epoch [0/5] Loss_D: 1.3078 Loss_G: 0.4743\n",
      "BATCH NUMBER 676\n",
      "Epoch [0/5] Loss_D: 1.1261 Loss_G: 0.5997\n",
      "BATCH NUMBER 677\n",
      "Epoch [0/5] Loss_D: 1.3238 Loss_G: 0.4687\n",
      "BATCH NUMBER 678\n",
      "Epoch [0/5] Loss_D: 1.2648 Loss_G: 0.5145\n",
      "BATCH NUMBER 679\n",
      "Epoch [0/5] Loss_D: 1.5204 Loss_G: 0.5480\n",
      "BATCH NUMBER 680\n",
      "Epoch [0/5] Loss_D: 1.2062 Loss_G: 0.5491\n",
      "BATCH NUMBER 681\n",
      "Epoch [0/5] Loss_D: 1.1916 Loss_G: 0.5471\n",
      "BATCH NUMBER 682\n",
      "Epoch [0/5] Loss_D: 1.3339 Loss_G: 0.4967\n",
      "BATCH NUMBER 683\n",
      "Epoch [0/5] Loss_D: 1.3777 Loss_G: 0.4376\n",
      "BATCH NUMBER 684\n",
      "Epoch [0/5] Loss_D: 1.2857 Loss_G: 0.5362\n",
      "BATCH NUMBER 685\n",
      "Epoch [0/5] Loss_D: 1.4572 Loss_G: 0.5006\n",
      "BATCH NUMBER 686\n",
      "Epoch [0/5] Loss_D: 1.2824 Loss_G: 0.5434\n",
      "BATCH NUMBER 687\n",
      "Epoch [0/5] Loss_D: 1.3689 Loss_G: 0.5155\n",
      "BATCH NUMBER 688\n",
      "Epoch [0/5] Loss_D: 1.2598 Loss_G: 0.5196\n",
      "BATCH NUMBER 689\n",
      "Epoch [0/5] Loss_D: 1.2569 Loss_G: 0.5696\n",
      "BATCH NUMBER 690\n",
      "Epoch [0/5] Loss_D: 1.1767 Loss_G: 0.5676\n",
      "BATCH NUMBER 691\n",
      "Epoch [0/5] Loss_D: 1.2155 Loss_G: 0.5367\n",
      "BATCH NUMBER 692\n",
      "Epoch [0/5] Loss_D: 1.2342 Loss_G: 0.5270\n",
      "BATCH NUMBER 693\n",
      "Epoch [0/5] Loss_D: 1.2195 Loss_G: 0.5309\n",
      "BATCH NUMBER 694\n",
      "Epoch [0/5] Loss_D: 1.2363 Loss_G: 0.5255\n",
      "BATCH NUMBER 695\n",
      "Epoch [0/5] Loss_D: 1.2859 Loss_G: 0.4941\n",
      "BATCH NUMBER 696\n",
      "Epoch [0/5] Loss_D: 1.1030 Loss_G: 0.6159\n",
      "BATCH NUMBER 697\n",
      "Epoch [0/5] Loss_D: 1.3669 Loss_G: 0.5386\n",
      "BATCH NUMBER 698\n",
      "Epoch [0/5] Loss_D: 1.3688 Loss_G: 0.5305\n",
      "BATCH NUMBER 699\n",
      "Epoch [0/5] Loss_D: 1.2747 Loss_G: 0.4900\n",
      "BATCH NUMBER 700\n",
      "Epoch [0/5] Loss_D: 1.2382 Loss_G: 0.5163\n",
      "BATCH NUMBER 701\n",
      "Epoch [0/5] Loss_D: 1.2642 Loss_G: 0.5092\n",
      "BATCH NUMBER 702\n",
      "Epoch [0/5] Loss_D: 1.3156 Loss_G: 0.5230\n",
      "BATCH NUMBER 703\n",
      "Epoch [0/5] Loss_D: 1.1658 Loss_G: 0.5669\n",
      "BATCH NUMBER 704\n",
      "Epoch [0/5] Loss_D: 1.3468 Loss_G: 0.5420\n",
      "BATCH NUMBER 705\n",
      "Epoch [0/5] Loss_D: 1.1664 Loss_G: 0.5828\n",
      "BATCH NUMBER 706\n",
      "Epoch [0/5] Loss_D: 1.2093 Loss_G: 0.5470\n",
      "BATCH NUMBER 707\n",
      "Epoch [0/5] Loss_D: 1.1362 Loss_G: 0.5899\n",
      "BATCH NUMBER 708\n",
      "Epoch [0/5] Loss_D: 1.4026 Loss_G: 0.5132\n",
      "BATCH NUMBER 709\n",
      "Epoch [0/5] Loss_D: 1.4489 Loss_G: 0.4752\n",
      "BATCH NUMBER 710\n",
      "Epoch [0/5] Loss_D: 1.2162 Loss_G: 0.5645\n",
      "BATCH NUMBER 711\n",
      "Epoch [0/5] Loss_D: 1.3625 Loss_G: 0.5398\n",
      "BATCH NUMBER 712\n",
      "Epoch [0/5] Loss_D: 1.1128 Loss_G: 0.6076\n",
      "BATCH NUMBER 713\n",
      "Epoch [0/5] Loss_D: 1.3414 Loss_G: 0.5534\n",
      "BATCH NUMBER 714\n",
      "Epoch [0/5] Loss_D: 1.2315 Loss_G: 0.5250\n",
      "BATCH NUMBER 715\n",
      "Epoch [0/5] Loss_D: 1.2004 Loss_G: 0.5488\n",
      "BATCH NUMBER 716\n",
      "Epoch [0/5] Loss_D: 1.1101 Loss_G: 0.6095\n",
      "BATCH NUMBER 717\n",
      "Epoch [0/5] Loss_D: 1.0790 Loss_G: 0.6338\n",
      "BATCH NUMBER 718\n",
      "Epoch [0/5] Loss_D: 1.1918 Loss_G: 0.5485\n",
      "BATCH NUMBER 719\n",
      "Epoch [0/5] Loss_D: 1.2151 Loss_G: 0.5889\n",
      "BATCH NUMBER 720\n",
      "Epoch [0/5] Loss_D: 1.1203 Loss_G: 0.6008\n",
      "BATCH NUMBER 721\n",
      "Epoch [0/5] Loss_D: 1.3261 Loss_G: 0.5083\n",
      "BATCH NUMBER 722\n",
      "Epoch [0/5] Loss_D: 1.3836 Loss_G: 0.4650\n",
      "BATCH NUMBER 723\n",
      "Epoch [0/5] Loss_D: 1.4245 Loss_G: 0.5401\n",
      "BATCH NUMBER 724\n",
      "Epoch [0/5] Loss_D: 1.2244 Loss_G: 0.5367\n",
      "BATCH NUMBER 725\n",
      "Epoch [0/5] Loss_D: 1.2290 Loss_G: 0.5269\n",
      "BATCH NUMBER 726\n",
      "Epoch [0/5] Loss_D: 1.2150 Loss_G: 0.5444\n",
      "BATCH NUMBER 727\n",
      "Epoch [0/5] Loss_D: 1.3043 Loss_G: 0.4746\n",
      "BATCH NUMBER 728\n",
      "Epoch [0/5] Loss_D: 1.3455 Loss_G: 0.4618\n",
      "BATCH NUMBER 729\n",
      "Epoch [0/5] Loss_D: 1.2199 Loss_G: 0.5326\n",
      "BATCH NUMBER 730\n",
      "Epoch [0/5] Loss_D: 1.2326 Loss_G: 0.5293\n",
      "BATCH NUMBER 731\n",
      "Epoch [0/5] Loss_D: 1.4758 Loss_G: 0.4543\n",
      "BATCH NUMBER 732\n",
      "Epoch [0/5] Loss_D: 1.3392 Loss_G: 0.4651\n",
      "BATCH NUMBER 733\n",
      "Epoch [0/5] Loss_D: 1.3651 Loss_G: 0.4471\n",
      "BATCH NUMBER 734\n",
      "Epoch [0/5] Loss_D: 1.1940 Loss_G: 0.5452\n",
      "BATCH NUMBER 735\n",
      "Epoch [0/5] Loss_D: 1.4222 Loss_G: 0.5027\n",
      "BATCH NUMBER 736\n",
      "Epoch [0/5] Loss_D: 1.2909 Loss_G: 0.5027\n",
      "BATCH NUMBER 737\n",
      "Epoch [0/5] Loss_D: 1.2788 Loss_G: 0.4888\n",
      "BATCH NUMBER 738\n",
      "Epoch [0/5] Loss_D: 1.2687 Loss_G: 0.5047\n",
      "BATCH NUMBER 739\n",
      "Epoch [0/5] Loss_D: 1.1502 Loss_G: 0.5811\n",
      "BATCH NUMBER 740\n",
      "Epoch [0/5] Loss_D: 1.3085 Loss_G: 0.4909\n",
      "BATCH NUMBER 741\n",
      "Epoch [0/5] Loss_D: 1.1015 Loss_G: 0.6136\n",
      "BATCH NUMBER 742\n",
      "Epoch [0/5] Loss_D: 1.2502 Loss_G: 0.5373\n",
      "BATCH NUMBER 743\n",
      "Epoch [0/5] Loss_D: 1.0719 Loss_G: 0.6375\n",
      "BATCH NUMBER 744\n",
      "Epoch [0/5] Loss_D: 1.2745 Loss_G: 0.4936\n",
      "BATCH NUMBER 745\n",
      "Epoch [0/5] Loss_D: 1.2730 Loss_G: 0.5747\n",
      "BATCH NUMBER 746\n",
      "Epoch [0/5] Loss_D: 1.1891 Loss_G: 0.5800\n",
      "BATCH NUMBER 747\n",
      "Epoch [0/5] Loss_D: 1.1866 Loss_G: 0.5675\n",
      "BATCH NUMBER 748\n",
      "Epoch [0/5] Loss_D: 1.1567 Loss_G: 0.5786\n",
      "BATCH NUMBER 749\n",
      "Epoch [0/5] Loss_D: 1.2674 Loss_G: 0.4893\n",
      "BATCH NUMBER 750\n",
      "Epoch [0/5] Loss_D: 1.1288 Loss_G: 0.5951\n",
      "BATCH NUMBER 751\n",
      "Epoch [0/5] Loss_D: 1.2003 Loss_G: 0.5487\n",
      "BATCH NUMBER 752\n",
      "Epoch [0/5] Loss_D: 1.2127 Loss_G: 0.5444\n",
      "BATCH NUMBER 753\n",
      "Epoch [0/5] Loss_D: 1.4518 Loss_G: 0.4244\n",
      "BATCH NUMBER 754\n",
      "Epoch [0/5] Loss_D: 1.2778 Loss_G: 0.5498\n",
      "BATCH NUMBER 755\n",
      "Epoch [0/5] Loss_D: 1.2523 Loss_G: 0.5180\n",
      "BATCH NUMBER 756\n",
      "Epoch [0/5] Loss_D: 1.1293 Loss_G: 0.5964\n",
      "BATCH NUMBER 757\n",
      "Epoch [0/5] Loss_D: 1.1918 Loss_G: 0.5535\n",
      "BATCH NUMBER 758\n",
      "Epoch [0/5] Loss_D: 1.3599 Loss_G: 0.4577\n",
      "BATCH NUMBER 759\n",
      "Epoch [0/5] Loss_D: 1.1322 Loss_G: 0.6029\n",
      "BATCH NUMBER 760\n",
      "Epoch [0/5] Loss_D: 1.3005 Loss_G: 0.4945\n",
      "BATCH NUMBER 761\n",
      "Epoch [0/5] Loss_D: 1.4300 Loss_G: 0.4135\n",
      "BATCH NUMBER 762\n",
      "Epoch [0/5] Loss_D: 1.1788 Loss_G: 0.5605\n",
      "BATCH NUMBER 763\n",
      "Epoch [0/5] Loss_D: 1.2267 Loss_G: 0.5244\n",
      "BATCH NUMBER 764\n",
      "Epoch [0/5] Loss_D: 1.2476 Loss_G: 0.5321\n",
      "BATCH NUMBER 765\n",
      "Epoch [0/5] Loss_D: 1.1953 Loss_G: 0.5507\n",
      "BATCH NUMBER 766\n",
      "Epoch [0/5] Loss_D: 1.1534 Loss_G: 0.6024\n",
      "BATCH NUMBER 767\n",
      "Epoch [0/5] Loss_D: 1.2172 Loss_G: 0.5391\n",
      "BATCH NUMBER 768\n",
      "Epoch [0/5] Loss_D: 1.1347 Loss_G: 0.5911\n",
      "BATCH NUMBER 769\n",
      "Epoch [0/5] Loss_D: 1.1947 Loss_G: 0.5708\n",
      "BATCH NUMBER 770\n",
      "Epoch [0/5] Loss_D: 1.3506 Loss_G: 0.5763\n",
      "BATCH NUMBER 771\n",
      "Epoch [0/5] Loss_D: 1.2231 Loss_G: 0.5809\n",
      "BATCH NUMBER 772\n",
      "Epoch [0/5] Loss_D: 1.2701 Loss_G: 0.5261\n",
      "BATCH NUMBER 773\n",
      "Epoch [0/5] Loss_D: 1.1568 Loss_G: 0.5804\n",
      "BATCH NUMBER 774\n",
      "Epoch [0/5] Loss_D: 1.1951 Loss_G: 0.5516\n",
      "BATCH NUMBER 775\n",
      "Epoch [0/5] Loss_D: 1.1836 Loss_G: 0.5595\n",
      "BATCH NUMBER 776\n",
      "Epoch [0/5] Loss_D: 1.3300 Loss_G: 0.5391\n",
      "BATCH NUMBER 777\n",
      "Epoch [0/5] Loss_D: 1.1995 Loss_G: 0.5474\n",
      "BATCH NUMBER 778\n",
      "Epoch [0/5] Loss_D: 1.2362 Loss_G: 0.5229\n",
      "BATCH NUMBER 779\n",
      "Epoch [0/5] Loss_D: 1.2554 Loss_G: 0.5190\n",
      "BATCH NUMBER 780\n",
      "Epoch [0/5] Loss_D: 1.1896 Loss_G: 0.5559\n",
      "BATCH NUMBER 781\n",
      "Epoch [0/5] Loss_D: 1.2367 Loss_G: 0.5268\n",
      "BATCH NUMBER 782\n",
      "Epoch [0/5] Loss_D: 1.2443 Loss_G: 0.5084\n",
      "BATCH NUMBER 783\n",
      "Epoch [0/5] Loss_D: 1.1467 Loss_G: 0.5778\n",
      "BATCH NUMBER 784\n",
      "Epoch [0/5] Loss_D: 1.2195 Loss_G: 0.5413\n",
      "BATCH NUMBER 785\n",
      "Epoch [0/5] Loss_D: 1.1830 Loss_G: 0.5676\n",
      "BATCH NUMBER 786\n",
      "Epoch [0/5] Loss_D: 1.1679 Loss_G: 0.5676\n",
      "BATCH NUMBER 787\n",
      "Epoch [0/5] Loss_D: 1.1903 Loss_G: 0.5500\n",
      "BATCH NUMBER 788\n",
      "Epoch [0/5] Loss_D: 1.1232 Loss_G: 0.6022\n",
      "BATCH NUMBER 789\n",
      "Epoch [0/5] Loss_D: 1.2252 Loss_G: 0.5243\n",
      "BATCH NUMBER 790\n",
      "Epoch [0/5] Loss_D: 1.2416 Loss_G: 0.5276\n",
      "BATCH NUMBER 791\n",
      "Epoch [0/5] Loss_D: 1.2167 Loss_G: 0.5430\n",
      "BATCH NUMBER 792\n",
      "Epoch [0/5] Loss_D: 1.4321 Loss_G: 0.5001\n",
      "BATCH NUMBER 793\n",
      "Epoch [0/5] Loss_D: 1.3419 Loss_G: 0.4543\n",
      "BATCH NUMBER 794\n",
      "Epoch [0/5] Loss_D: 1.3571 Loss_G: 0.5013\n",
      "BATCH NUMBER 795\n",
      "Epoch [0/5] Loss_D: 1.2392 Loss_G: 0.5114\n",
      "BATCH NUMBER 796\n",
      "Epoch [0/5] Loss_D: 1.2704 Loss_G: 0.4955\n",
      "BATCH NUMBER 797\n",
      "Epoch [0/5] Loss_D: 1.3419 Loss_G: 0.4662\n",
      "BATCH NUMBER 798\n",
      "Epoch [0/5] Loss_D: 1.3459 Loss_G: 0.5124\n",
      "BATCH NUMBER 799\n",
      "Epoch [0/5] Loss_D: 1.1675 Loss_G: 0.5728\n",
      "BATCH NUMBER 800\n",
      "Epoch [0/5] Loss_D: 1.2655 Loss_G: 0.5219\n",
      "BATCH NUMBER 801\n",
      "Epoch [0/5] Loss_D: 1.1641 Loss_G: 0.5669\n",
      "BATCH NUMBER 802\n",
      "Epoch [0/5] Loss_D: 1.2519 Loss_G: 0.5266\n",
      "BATCH NUMBER 803\n",
      "Epoch [0/5] Loss_D: 1.3693 Loss_G: 0.5527\n",
      "BATCH NUMBER 804\n",
      "Epoch [0/5] Loss_D: 1.3298 Loss_G: 0.5486\n",
      "BATCH NUMBER 805\n",
      "Epoch [0/5] Loss_D: 1.2251 Loss_G: 0.5684\n",
      "BATCH NUMBER 806\n",
      "Epoch [0/5] Loss_D: 1.1827 Loss_G: 0.5654\n",
      "BATCH NUMBER 807\n",
      "Epoch [0/5] Loss_D: 1.1787 Loss_G: 0.5586\n",
      "BATCH NUMBER 808\n",
      "Epoch [0/5] Loss_D: 1.3302 Loss_G: 0.4709\n",
      "BATCH NUMBER 809\n",
      "Epoch [0/5] Loss_D: 1.2453 Loss_G: 0.5406\n",
      "BATCH NUMBER 810\n",
      "Epoch [0/5] Loss_D: 1.2178 Loss_G: 0.5427\n",
      "BATCH NUMBER 811\n",
      "Epoch [0/5] Loss_D: 1.2511 Loss_G: 0.5040\n",
      "BATCH NUMBER 812\n",
      "Epoch [0/5] Loss_D: 1.4246 Loss_G: 0.4940\n",
      "BATCH NUMBER 813\n",
      "Epoch [0/5] Loss_D: 1.1370 Loss_G: 0.5888\n",
      "BATCH NUMBER 814\n",
      "Epoch [0/5] Loss_D: 1.1205 Loss_G: 0.5998\n",
      "BATCH NUMBER 815\n",
      "Epoch [0/5] Loss_D: 1.2411 Loss_G: 0.5271\n",
      "BATCH NUMBER 816\n",
      "Epoch [0/5] Loss_D: 1.2145 Loss_G: 0.5313\n",
      "BATCH NUMBER 817\n",
      "Epoch [0/5] Loss_D: 1.2930 Loss_G: 0.4839\n",
      "BATCH NUMBER 818\n",
      "Epoch [0/5] Loss_D: 1.1894 Loss_G: 0.5638\n",
      "BATCH NUMBER 819\n",
      "Epoch [0/5] Loss_D: 1.1414 Loss_G: 0.5857\n",
      "BATCH NUMBER 820\n",
      "Epoch [0/5] Loss_D: 1.1444 Loss_G: 0.5968\n",
      "BATCH NUMBER 821\n",
      "Epoch [0/5] Loss_D: 1.2445 Loss_G: 0.5335\n",
      "BATCH NUMBER 822\n",
      "Epoch [0/5] Loss_D: 1.0998 Loss_G: 0.6178\n",
      "BATCH NUMBER 823\n",
      "Epoch [0/5] Loss_D: 1.2497 Loss_G: 0.5602\n",
      "BATCH NUMBER 824\n",
      "Epoch [0/5] Loss_D: 1.3097 Loss_G: 0.4862\n",
      "BATCH NUMBER 825\n",
      "Epoch [0/5] Loss_D: 1.2442 Loss_G: 0.5208\n",
      "BATCH NUMBER 826\n",
      "Epoch [0/5] Loss_D: 1.2888 Loss_G: 0.5307\n",
      "BATCH NUMBER 827\n",
      "Epoch [0/5] Loss_D: 1.2548 Loss_G: 0.5344\n",
      "BATCH NUMBER 828\n",
      "Epoch [0/5] Loss_D: 1.6534 Loss_G: 0.4679\n",
      "BATCH NUMBER 829\n",
      "Epoch [0/5] Loss_D: 1.4057 Loss_G: 0.4318\n",
      "BATCH NUMBER 830\n",
      "Epoch [0/5] Loss_D: 1.2112 Loss_G: 0.5391\n",
      "BATCH NUMBER 831\n",
      "Epoch [0/5] Loss_D: 1.1758 Loss_G: 0.5714\n",
      "BATCH NUMBER 832\n",
      "Epoch [0/5] Loss_D: 1.1607 Loss_G: 0.5738\n",
      "BATCH NUMBER 833\n",
      "Epoch [0/5] Loss_D: 1.1663 Loss_G: 0.5702\n",
      "BATCH NUMBER 834\n",
      "Epoch [0/5] Loss_D: 1.2039 Loss_G: 0.5640\n",
      "BATCH NUMBER 835\n",
      "Epoch [0/5] Loss_D: 1.1635 Loss_G: 0.5679\n",
      "BATCH NUMBER 836\n",
      "Epoch [0/5] Loss_D: 1.1530 Loss_G: 0.5786\n",
      "BATCH NUMBER 837\n",
      "Epoch [0/5] Loss_D: 1.3131 Loss_G: 0.4799\n",
      "BATCH NUMBER 838\n",
      "Epoch [0/5] Loss_D: 1.1362 Loss_G: 0.6381\n",
      "BATCH NUMBER 839\n",
      "Epoch [0/5] Loss_D: 1.2403 Loss_G: 0.5797\n",
      "BATCH NUMBER 840\n",
      "Epoch [0/5] Loss_D: 1.1466 Loss_G: 0.5848\n",
      "BATCH NUMBER 841\n",
      "Epoch [0/5] Loss_D: 1.2193 Loss_G: 0.5459\n",
      "BATCH NUMBER 842\n",
      "Epoch [0/5] Loss_D: 1.1665 Loss_G: 0.5723\n",
      "BATCH NUMBER 843\n",
      "Epoch [0/5] Loss_D: 1.4865 Loss_G: 0.4767\n",
      "BATCH NUMBER 844\n",
      "Epoch [0/5] Loss_D: 1.1709 Loss_G: 0.5580\n",
      "BATCH NUMBER 845\n",
      "Epoch [0/5] Loss_D: 1.2379 Loss_G: 0.5192\n",
      "BATCH NUMBER 846\n",
      "Epoch [0/5] Loss_D: 1.1539 Loss_G: 0.5742\n",
      "BATCH NUMBER 847\n",
      "Epoch [0/5] Loss_D: 1.2559 Loss_G: 0.5024\n",
      "BATCH NUMBER 848\n",
      "Epoch [0/5] Loss_D: 1.3664 Loss_G: 0.5029\n",
      "BATCH NUMBER 849\n",
      "Epoch [0/5] Loss_D: 1.1835 Loss_G: 0.5536\n",
      "BATCH NUMBER 850\n",
      "Epoch [0/5] Loss_D: 1.1233 Loss_G: 0.6020\n",
      "BATCH NUMBER 851\n",
      "Epoch [0/5] Loss_D: 1.2220 Loss_G: 0.5390\n",
      "BATCH NUMBER 852\n",
      "Epoch [0/5] Loss_D: 1.3070 Loss_G: 0.5320\n",
      "BATCH NUMBER 853\n",
      "Epoch [0/5] Loss_D: 1.2206 Loss_G: 0.5322\n",
      "BATCH NUMBER 854\n",
      "Epoch [0/5] Loss_D: 1.3015 Loss_G: 0.4846\n",
      "BATCH NUMBER 855\n",
      "Epoch [0/5] Loss_D: 1.1878 Loss_G: 0.6180\n",
      "BATCH NUMBER 856\n",
      "Epoch [0/5] Loss_D: 1.2868 Loss_G: 0.5955\n",
      "BATCH NUMBER 857\n",
      "Epoch [0/5] Loss_D: 1.1441 Loss_G: 0.5904\n",
      "BATCH NUMBER 858\n",
      "Epoch [0/5] Loss_D: 1.3127 Loss_G: 0.5853\n",
      "BATCH NUMBER 859\n",
      "Epoch [0/5] Loss_D: 1.1434 Loss_G: 0.5906\n",
      "BATCH NUMBER 860\n",
      "Epoch [0/5] Loss_D: 1.1441 Loss_G: 0.5821\n",
      "BATCH NUMBER 861\n",
      "Epoch [0/5] Loss_D: 1.2252 Loss_G: 0.5520\n",
      "BATCH NUMBER 862\n",
      "Epoch [0/5] Loss_D: 1.4012 Loss_G: 0.4988\n",
      "BATCH NUMBER 863\n",
      "Epoch [0/5] Loss_D: 1.1418 Loss_G: 0.5906\n",
      "BATCH NUMBER 864\n",
      "Epoch [0/5] Loss_D: 1.2084 Loss_G: 0.5490\n",
      "BATCH NUMBER 865\n",
      "Epoch [0/5] Loss_D: 1.2229 Loss_G: 0.5523\n",
      "BATCH NUMBER 866\n",
      "Epoch [0/5] Loss_D: 1.3329 Loss_G: 0.4687\n",
      "BATCH NUMBER 867\n",
      "Epoch [0/5] Loss_D: 1.4037 Loss_G: 0.4187\n",
      "BATCH NUMBER 868\n",
      "Epoch [0/5] Loss_D: 1.1517 Loss_G: 0.5766\n",
      "BATCH NUMBER 869\n",
      "Epoch [0/5] Loss_D: 1.1236 Loss_G: 0.5977\n",
      "BATCH NUMBER 870\n",
      "Epoch [0/5] Loss_D: 1.5114 Loss_G: 0.4106\n",
      "BATCH NUMBER 871\n",
      "Epoch [0/5] Loss_D: 1.2702 Loss_G: 0.5656\n",
      "BATCH NUMBER 872\n",
      "Epoch [0/5] Loss_D: 1.2340 Loss_G: 0.5335\n",
      "BATCH NUMBER 873\n",
      "Epoch [0/5] Loss_D: 1.1734 Loss_G: 0.5621\n",
      "BATCH NUMBER 874\n",
      "Epoch [0/5] Loss_D: 1.2271 Loss_G: 0.5266\n",
      "BATCH NUMBER 875\n",
      "Epoch [0/5] Loss_D: 1.2109 Loss_G: 0.5524\n",
      "BATCH NUMBER 876\n",
      "Epoch [0/5] Loss_D: 1.2703 Loss_G: 0.4978\n",
      "BATCH NUMBER 877\n",
      "Epoch [0/5] Loss_D: 1.2136 Loss_G: 0.5454\n",
      "BATCH NUMBER 878\n",
      "Epoch [0/5] Loss_D: 1.3663 Loss_G: 0.5649\n",
      "BATCH NUMBER 879\n",
      "Epoch [0/5] Loss_D: 1.1693 Loss_G: 0.5679\n",
      "BATCH NUMBER 880\n",
      "Epoch [0/5] Loss_D: 1.1548 Loss_G: 0.5757\n",
      "BATCH NUMBER 881\n",
      "Epoch [0/5] Loss_D: 1.2081 Loss_G: 0.5354\n",
      "BATCH NUMBER 882\n",
      "Epoch [0/5] Loss_D: 1.2479 Loss_G: 0.5549\n",
      "BATCH NUMBER 883\n",
      "Epoch [0/5] Loss_D: 1.3575 Loss_G: 0.4565\n",
      "BATCH NUMBER 884\n",
      "Epoch [0/5] Loss_D: 1.3031 Loss_G: 0.4887\n",
      "BATCH NUMBER 885\n",
      "Epoch [0/5] Loss_D: 1.1910 Loss_G: 0.5549\n",
      "BATCH NUMBER 886\n",
      "Epoch [0/5] Loss_D: 1.1820 Loss_G: 0.5874\n",
      "BATCH NUMBER 887\n",
      "Epoch [0/5] Loss_D: 1.2010 Loss_G: 0.5384\n",
      "BATCH NUMBER 888\n",
      "Epoch [0/5] Loss_D: 1.2081 Loss_G: 0.5359\n",
      "BATCH NUMBER 889\n",
      "Epoch [0/5] Loss_D: 1.1970 Loss_G: 0.5498\n",
      "BATCH NUMBER 890\n",
      "Epoch [0/5] Loss_D: 1.2239 Loss_G: 0.5231\n",
      "BATCH NUMBER 891\n",
      "Epoch [0/5] Loss_D: 1.2585 Loss_G: 0.5211\n",
      "BATCH NUMBER 892\n",
      "Epoch [0/5] Loss_D: 1.1824 Loss_G: 0.5563\n",
      "BATCH NUMBER 893\n",
      "Epoch [0/5] Loss_D: 1.2036 Loss_G: 0.5420\n",
      "BATCH NUMBER 894\n",
      "Epoch [0/5] Loss_D: 1.3088 Loss_G: 0.4865\n",
      "BATCH NUMBER 895\n",
      "Epoch [0/5] Loss_D: 1.3222 Loss_G: 0.4741\n",
      "BATCH NUMBER 896\n",
      "Epoch [0/5] Loss_D: 1.3514 Loss_G: 0.5371\n",
      "BATCH NUMBER 897\n",
      "Epoch [0/5] Loss_D: 1.1639 Loss_G: 0.5742\n",
      "BATCH NUMBER 898\n",
      "Epoch [0/5] Loss_D: 1.2068 Loss_G: 0.5354\n",
      "BATCH NUMBER 899\n",
      "Epoch [0/5] Loss_D: 1.1924 Loss_G: 0.5535\n",
      "BATCH NUMBER 900\n",
      "Epoch [0/5] Loss_D: 1.3486 Loss_G: 0.5588\n",
      "BATCH NUMBER 901\n",
      "Epoch [0/5] Loss_D: 1.3052 Loss_G: 0.4904\n",
      "BATCH NUMBER 902\n",
      "Epoch [0/5] Loss_D: 1.3223 Loss_G: 0.4821\n",
      "BATCH NUMBER 903\n",
      "Epoch [0/5] Loss_D: 1.5251 Loss_G: 0.4821\n",
      "BATCH NUMBER 904\n",
      "Epoch [0/5] Loss_D: 1.3586 Loss_G: 0.4647\n",
      "BATCH NUMBER 905\n",
      "Epoch [0/5] Loss_D: 1.1441 Loss_G: 0.5827\n",
      "BATCH NUMBER 906\n",
      "Epoch [0/5] Loss_D: 1.2666 Loss_G: 0.4981\n",
      "BATCH NUMBER 907\n",
      "Epoch [0/5] Loss_D: 1.1238 Loss_G: 0.5994\n",
      "BATCH NUMBER 908\n",
      "Epoch [0/5] Loss_D: 1.1898 Loss_G: 0.5545\n",
      "BATCH NUMBER 909\n",
      "Epoch [0/5] Loss_D: 1.2512 Loss_G: 0.5813\n",
      "BATCH NUMBER 910\n",
      "Epoch [0/5] Loss_D: 1.2040 Loss_G: 0.5468\n",
      "BATCH NUMBER 911\n",
      "Epoch [0/5] Loss_D: 1.2962 Loss_G: 0.4929\n",
      "BATCH NUMBER 912\n",
      "Epoch [0/5] Loss_D: 1.2387 Loss_G: 0.5317\n",
      "BATCH NUMBER 913\n",
      "Epoch [0/5] Loss_D: 1.1358 Loss_G: 0.5925\n",
      "BATCH NUMBER 914\n",
      "Epoch [0/5] Loss_D: 1.1592 Loss_G: 0.5696\n",
      "BATCH NUMBER 915\n",
      "Epoch [0/5] Loss_D: 1.2723 Loss_G: 0.6006\n",
      "BATCH NUMBER 916\n",
      "Epoch [0/5] Loss_D: 1.3992 Loss_G: 0.5043\n",
      "BATCH NUMBER 917\n",
      "Epoch [0/5] Loss_D: 1.1853 Loss_G: 0.5523\n",
      "BATCH NUMBER 918\n",
      "Epoch [0/5] Loss_D: 1.1447 Loss_G: 0.5845\n",
      "BATCH NUMBER 919\n",
      "Epoch [0/5] Loss_D: 1.1564 Loss_G: 0.5953\n",
      "BATCH NUMBER 920\n",
      "Epoch [0/5] Loss_D: 1.1680 Loss_G: 0.5635\n",
      "BATCH NUMBER 921\n",
      "Epoch [0/5] Loss_D: 1.3081 Loss_G: 0.4907\n",
      "BATCH NUMBER 922\n",
      "Epoch [0/5] Loss_D: 1.3135 Loss_G: 0.4848\n",
      "BATCH NUMBER 923\n",
      "Epoch [0/5] Loss_D: 1.2803 Loss_G: 0.5003\n",
      "BATCH NUMBER 924\n",
      "Epoch [0/5] Loss_D: 1.1819 Loss_G: 0.5520\n",
      "BATCH NUMBER 925\n",
      "Epoch [0/5] Loss_D: 1.1662 Loss_G: 0.5639\n",
      "BATCH NUMBER 926\n",
      "Epoch [0/5] Loss_D: 1.2046 Loss_G: 0.5365\n",
      "BATCH NUMBER 927\n",
      "Epoch [0/5] Loss_D: 1.1254 Loss_G: 0.5979\n",
      "BATCH NUMBER 928\n",
      "Epoch [0/5] Loss_D: 1.1949 Loss_G: 0.5486\n",
      "BATCH NUMBER 929\n",
      "Epoch [0/5] Loss_D: 1.2434 Loss_G: 0.5204\n",
      "BATCH NUMBER 930\n",
      "Epoch [0/5] Loss_D: 1.1378 Loss_G: 0.5885\n",
      "BATCH NUMBER 931\n",
      "Epoch [0/5] Loss_D: 1.1613 Loss_G: 0.5787\n",
      "BATCH NUMBER 932\n",
      "Epoch [0/5] Loss_D: 1.1360 Loss_G: 0.5922\n",
      "BATCH NUMBER 933\n",
      "Epoch [0/5] Loss_D: 1.1185 Loss_G: 0.6024\n",
      "BATCH NUMBER 934\n",
      "Epoch [0/5] Loss_D: 1.2500 Loss_G: 0.5199\n",
      "BATCH NUMBER 935\n",
      "Epoch [0/5] Loss_D: 1.1194 Loss_G: 0.6031\n",
      "BATCH NUMBER 936\n",
      "Epoch [0/5] Loss_D: 1.0825 Loss_G: 0.6329\n",
      "BATCH NUMBER 937\n",
      "Epoch [0/5] Loss_D: 1.4272 Loss_G: 0.4146\n",
      "BATCH NUMBER 938\n",
      "Epoch [0/5] Loss_D: 1.2550 Loss_G: 0.5336\n",
      "BATCH NUMBER 939\n",
      "Epoch [0/5] Loss_D: 1.2352 Loss_G: 0.5298\n",
      "BATCH NUMBER 940\n",
      "Epoch [0/5] Loss_D: 1.2120 Loss_G: 0.5290\n",
      "BATCH NUMBER 941\n",
      "Epoch [0/5] Loss_D: 1.1841 Loss_G: 0.5559\n",
      "BATCH NUMBER 942\n",
      "Epoch [0/5] Loss_D: 1.1834 Loss_G: 0.5668\n",
      "BATCH NUMBER 943\n",
      "Epoch [0/5] Loss_D: 1.1818 Loss_G: 0.5594\n",
      "BATCH NUMBER 944\n",
      "Epoch [0/5] Loss_D: 1.1147 Loss_G: 0.6137\n",
      "BATCH NUMBER 945\n",
      "Epoch [0/5] Loss_D: 1.4232 Loss_G: 0.5116\n",
      "BATCH NUMBER 946\n",
      "Epoch [0/5] Loss_D: 1.1023 Loss_G: 0.6172\n",
      "BATCH NUMBER 947\n",
      "Epoch [0/5] Loss_D: 1.1674 Loss_G: 0.5640\n",
      "BATCH NUMBER 948\n",
      "Epoch [0/5] Loss_D: 1.2790 Loss_G: 0.5611\n",
      "BATCH NUMBER 949\n",
      "Epoch [0/5] Loss_D: 1.1265 Loss_G: 0.5975\n",
      "BATCH NUMBER 950\n",
      "Epoch [0/5] Loss_D: 1.1937 Loss_G: 0.5467\n",
      "BATCH NUMBER 951\n",
      "Epoch [0/5] Loss_D: 1.2581 Loss_G: 0.5101\n",
      "BATCH NUMBER 952\n",
      "Epoch [0/5] Loss_D: 1.1812 Loss_G: 0.5552\n",
      "BATCH NUMBER 953\n",
      "Epoch [0/5] Loss_D: 1.4781 Loss_G: 0.4310\n",
      "BATCH NUMBER 954\n",
      "Epoch [0/5] Loss_D: 1.1178 Loss_G: 0.6043\n",
      "BATCH NUMBER 955\n",
      "Epoch [0/5] Loss_D: 1.2491 Loss_G: 0.5547\n",
      "BATCH NUMBER 956\n",
      "Epoch [0/5] Loss_D: 1.1983 Loss_G: 0.5625\n",
      "BATCH NUMBER 957\n",
      "Epoch [0/5] Loss_D: 1.1523 Loss_G: 0.5758\n",
      "BATCH NUMBER 958\n",
      "Epoch [0/5] Loss_D: 1.1973 Loss_G: 0.5421\n",
      "BATCH NUMBER 959\n",
      "Epoch [0/5] Loss_D: 1.1365 Loss_G: 0.5989\n",
      "BATCH NUMBER 960\n",
      "Epoch [0/5] Loss_D: 1.3245 Loss_G: 0.5325\n",
      "BATCH NUMBER 961\n",
      "Epoch [0/5] Loss_D: 1.1568 Loss_G: 0.5732\n",
      "BATCH NUMBER 962\n",
      "Epoch [0/5] Loss_D: 1.3249 Loss_G: 0.5067\n",
      "BATCH NUMBER 963\n",
      "Epoch [0/5] Loss_D: 1.2433 Loss_G: 0.5208\n",
      "BATCH NUMBER 964\n",
      "Epoch [0/5] Loss_D: 1.3496 Loss_G: 0.5251\n",
      "BATCH NUMBER 965\n",
      "Epoch [0/5] Loss_D: 1.1686 Loss_G: 0.5684\n",
      "BATCH NUMBER 966\n",
      "Epoch [0/5] Loss_D: 1.1887 Loss_G: 0.5553\n",
      "BATCH NUMBER 967\n",
      "Epoch [0/5] Loss_D: 1.2246 Loss_G: 0.5332\n",
      "BATCH NUMBER 968\n",
      "Epoch [0/5] Loss_D: 1.1190 Loss_G: 0.6031\n",
      "BATCH NUMBER 969\n",
      "Epoch [0/5] Loss_D: 1.2694 Loss_G: 0.5227\n",
      "BATCH NUMBER 970\n",
      "Epoch [0/5] Loss_D: 1.2009 Loss_G: 0.5672\n",
      "BATCH NUMBER 971\n",
      "Epoch [0/5] Loss_D: 1.1021 Loss_G: 0.6174\n",
      "BATCH NUMBER 972\n",
      "Epoch [0/5] Loss_D: 1.2059 Loss_G: 0.5543\n",
      "BATCH NUMBER 973\n",
      "Epoch [0/5] Loss_D: 1.2075 Loss_G: 0.5375\n",
      "BATCH NUMBER 974\n",
      "Epoch [0/5] Loss_D: 1.4683 Loss_G: 0.4712\n",
      "BATCH NUMBER 975\n",
      "Epoch [0/5] Loss_D: 1.2267 Loss_G: 0.5282\n",
      "BATCH NUMBER 976\n",
      "Epoch [0/5] Loss_D: 1.3373 Loss_G: 0.4547\n",
      "BATCH NUMBER 977\n",
      "Epoch [0/5] Loss_D: 1.3624 Loss_G: 0.4619\n",
      "BATCH NUMBER 978\n",
      "Epoch [0/5] Loss_D: 1.3096 Loss_G: 0.5502\n",
      "BATCH NUMBER 979\n",
      "Epoch [0/5] Loss_D: 1.0694 Loss_G: 0.6410\n",
      "BATCH NUMBER 980\n",
      "Epoch [0/5] Loss_D: 1.1632 Loss_G: 0.5808\n",
      "BATCH NUMBER 981\n",
      "Epoch [0/5] Loss_D: 1.1485 Loss_G: 0.6005\n",
      "BATCH NUMBER 982\n",
      "Epoch [0/5] Loss_D: 1.3348 Loss_G: 0.5629\n",
      "BATCH NUMBER 983\n",
      "Epoch [0/5] Loss_D: 1.2125 Loss_G: 0.5393\n",
      "BATCH NUMBER 984\n",
      "Epoch [0/5] Loss_D: 1.2744 Loss_G: 0.5075\n",
      "BATCH NUMBER 985\n",
      "Epoch [0/5] Loss_D: 1.1364 Loss_G: 0.5903\n",
      "BATCH NUMBER 986\n",
      "Epoch [0/5] Loss_D: 1.2002 Loss_G: 0.5425\n",
      "BATCH NUMBER 987\n",
      "Epoch [0/5] Loss_D: 1.1545 Loss_G: 0.5777\n",
      "BATCH NUMBER 988\n",
      "Epoch [0/5] Loss_D: 1.1489 Loss_G: 0.5753\n",
      "BATCH NUMBER 989\n",
      "Epoch [0/5] Loss_D: 1.2217 Loss_G: 0.5573\n",
      "BATCH NUMBER 990\n",
      "Epoch [0/5] Loss_D: 1.3336 Loss_G: 0.4688\n",
      "BATCH NUMBER 991\n",
      "Epoch [0/5] Loss_D: 1.2056 Loss_G: 0.5508\n",
      "BATCH NUMBER 992\n",
      "Epoch [0/5] Loss_D: 1.1834 Loss_G: 0.5695\n",
      "BATCH NUMBER 993\n",
      "Epoch [0/5] Loss_D: 1.3459 Loss_G: 0.5589\n",
      "BATCH NUMBER 994\n",
      "Epoch [0/5] Loss_D: 1.2594 Loss_G: 0.5055\n",
      "BATCH NUMBER 995\n",
      "Epoch [0/5] Loss_D: 1.3242 Loss_G: 0.4891\n",
      "BATCH NUMBER 996\n",
      "Epoch [0/5] Loss_D: 1.2623 Loss_G: 0.5128\n",
      "BATCH NUMBER 997\n",
      "Epoch [0/5] Loss_D: 1.1729 Loss_G: 0.5711\n",
      "BATCH NUMBER 998\n",
      "Epoch [0/5] Loss_D: 1.2837 Loss_G: 0.4982\n",
      "BATCH NUMBER 999\n",
      "Epoch [0/5] Loss_D: 1.1386 Loss_G: 0.5862\n",
      "BATCH NUMBER 1000\n",
      "Epoch [1/5] Loss_D: 1.2908 Loss_G: 0.5282\n",
      "BATCH NUMBER 1001\n",
      "Epoch [1/5] Loss_D: 1.1059 Loss_G: 0.6196\n",
      "BATCH NUMBER 1002\n",
      "Epoch [1/5] Loss_D: 1.1458 Loss_G: 0.5803\n",
      "BATCH NUMBER 1003\n",
      "Epoch [1/5] Loss_D: 1.3149 Loss_G: 0.4744\n",
      "BATCH NUMBER 1004\n",
      "Epoch [1/5] Loss_D: 1.1666 Loss_G: 0.5616\n",
      "BATCH NUMBER 1005\n",
      "Epoch [1/5] Loss_D: 1.2126 Loss_G: 0.5395\n",
      "BATCH NUMBER 1006\n",
      "Epoch [1/5] Loss_D: 1.2314 Loss_G: 0.5346\n",
      "BATCH NUMBER 1007\n",
      "Epoch [1/5] Loss_D: 1.1924 Loss_G: 0.5432\n",
      "BATCH NUMBER 1008\n",
      "Epoch [1/5] Loss_D: 1.1877 Loss_G: 0.5482\n",
      "BATCH NUMBER 1009\n",
      "Epoch [1/5] Loss_D: 1.2204 Loss_G: 0.6088\n",
      "BATCH NUMBER 1010\n",
      "Epoch [1/5] Loss_D: 1.1836 Loss_G: 0.5664\n",
      "BATCH NUMBER 1011\n",
      "Epoch [1/5] Loss_D: 1.2031 Loss_G: 0.5493\n",
      "BATCH NUMBER 1012\n",
      "Epoch [1/5] Loss_D: 1.3427 Loss_G: 0.5183\n",
      "BATCH NUMBER 1013\n",
      "Epoch [1/5] Loss_D: 1.2502 Loss_G: 0.5211\n",
      "BATCH NUMBER 1014\n",
      "Epoch [1/5] Loss_D: 1.2376 Loss_G: 0.5935\n",
      "BATCH NUMBER 1015\n",
      "Epoch [1/5] Loss_D: 1.2158 Loss_G: 0.5470\n",
      "BATCH NUMBER 1016\n",
      "Epoch [1/5] Loss_D: 1.2507 Loss_G: 0.5136\n",
      "BATCH NUMBER 1017\n",
      "Epoch [1/5] Loss_D: 1.2558 Loss_G: 0.5831\n",
      "BATCH NUMBER 1018\n",
      "Epoch [1/5] Loss_D: 1.2306 Loss_G: 0.5503\n",
      "BATCH NUMBER 1019\n",
      "Epoch [1/5] Loss_D: 1.3505 Loss_G: 0.4625\n",
      "BATCH NUMBER 1020\n",
      "Epoch [1/5] Loss_D: 1.2883 Loss_G: 0.5841\n",
      "BATCH NUMBER 1021\n",
      "Epoch [1/5] Loss_D: 1.1516 Loss_G: 0.5765\n",
      "BATCH NUMBER 1022\n",
      "Epoch [1/5] Loss_D: 1.2507 Loss_G: 0.5128\n",
      "BATCH NUMBER 1023\n",
      "Epoch [1/5] Loss_D: 1.2644 Loss_G: 0.5164\n",
      "BATCH NUMBER 1024\n",
      "Epoch [1/5] Loss_D: 1.2240 Loss_G: 0.5345\n",
      "BATCH NUMBER 1025\n",
      "Epoch [1/5] Loss_D: 1.1066 Loss_G: 0.6115\n",
      "BATCH NUMBER 1026\n",
      "Epoch [1/5] Loss_D: 1.3312 Loss_G: 0.5456\n",
      "BATCH NUMBER 1027\n",
      "Epoch [1/5] Loss_D: 1.1051 Loss_G: 0.6157\n",
      "BATCH NUMBER 1028\n",
      "Epoch [1/5] Loss_D: 1.1739 Loss_G: 0.5742\n",
      "BATCH NUMBER 1029\n",
      "Epoch [1/5] Loss_D: 1.2056 Loss_G: 0.5328\n",
      "BATCH NUMBER 1030\n",
      "Epoch [1/5] Loss_D: 1.3935 Loss_G: 0.4805\n",
      "BATCH NUMBER 1031\n",
      "Epoch [1/5] Loss_D: 1.2200 Loss_G: 0.5303\n",
      "BATCH NUMBER 1032\n",
      "Epoch [1/5] Loss_D: 1.2671 Loss_G: 0.5126\n",
      "BATCH NUMBER 1033\n",
      "Epoch [1/5] Loss_D: 1.2430 Loss_G: 0.5265\n",
      "BATCH NUMBER 1034\n",
      "Epoch [1/5] Loss_D: 1.2434 Loss_G: 0.5208\n",
      "BATCH NUMBER 1035\n",
      "Epoch [1/5] Loss_D: 1.2695 Loss_G: 0.5489\n",
      "BATCH NUMBER 1036\n",
      "Epoch [1/5] Loss_D: 1.1564 Loss_G: 0.5837\n",
      "BATCH NUMBER 1037\n",
      "Epoch [1/5] Loss_D: 1.3040 Loss_G: 0.5193\n",
      "BATCH NUMBER 1038\n",
      "Epoch [1/5] Loss_D: 1.3803 Loss_G: 0.5147\n",
      "BATCH NUMBER 1039\n",
      "Epoch [1/5] Loss_D: 1.2686 Loss_G: 0.5060\n",
      "BATCH NUMBER 1040\n",
      "Epoch [1/5] Loss_D: 1.3263 Loss_G: 0.4711\n",
      "BATCH NUMBER 1041\n",
      "Epoch [1/5] Loss_D: 1.2339 Loss_G: 0.5230\n",
      "BATCH NUMBER 1042\n",
      "Epoch [1/5] Loss_D: 1.1422 Loss_G: 0.5842\n",
      "BATCH NUMBER 1043\n",
      "Epoch [1/5] Loss_D: 1.1434 Loss_G: 0.5870\n",
      "BATCH NUMBER 1044\n",
      "Epoch [1/5] Loss_D: 1.2889 Loss_G: 0.4962\n",
      "BATCH NUMBER 1045\n",
      "Epoch [1/5] Loss_D: 1.2299 Loss_G: 0.5279\n",
      "BATCH NUMBER 1046\n",
      "Epoch [1/5] Loss_D: 1.2561 Loss_G: 0.5054\n",
      "BATCH NUMBER 1047\n",
      "Epoch [1/5] Loss_D: 1.0914 Loss_G: 0.6270\n",
      "BATCH NUMBER 1048\n",
      "Epoch [1/5] Loss_D: 1.3172 Loss_G: 0.4710\n",
      "BATCH NUMBER 1049\n",
      "Epoch [1/5] Loss_D: 1.1636 Loss_G: 0.5782\n",
      "BATCH NUMBER 1050\n",
      "Epoch [1/5] Loss_D: 1.3238 Loss_G: 0.4778\n",
      "BATCH NUMBER 1051\n",
      "Epoch [1/5] Loss_D: 1.3341 Loss_G: 0.4754\n",
      "BATCH NUMBER 1052\n",
      "Epoch [1/5] Loss_D: 1.1274 Loss_G: 0.5964\n",
      "BATCH NUMBER 1053\n",
      "Epoch [1/5] Loss_D: 1.1888 Loss_G: 0.5503\n",
      "BATCH NUMBER 1054\n",
      "Epoch [1/5] Loss_D: 1.2349 Loss_G: 0.5295\n",
      "BATCH NUMBER 1055\n",
      "Epoch [1/5] Loss_D: 1.1929 Loss_G: 0.5469\n",
      "BATCH NUMBER 1056\n",
      "Epoch [1/5] Loss_D: 1.2748 Loss_G: 0.5362\n",
      "BATCH NUMBER 1057\n",
      "Epoch [1/5] Loss_D: 1.3552 Loss_G: 0.4837\n",
      "BATCH NUMBER 1058\n",
      "Epoch [1/5] Loss_D: 1.1534 Loss_G: 0.5739\n",
      "BATCH NUMBER 1059\n",
      "Epoch [1/5] Loss_D: 1.2710 Loss_G: 0.4987\n",
      "BATCH NUMBER 1060\n",
      "Epoch [1/5] Loss_D: 1.1359 Loss_G: 0.5880\n",
      "BATCH NUMBER 1061\n",
      "Epoch [1/5] Loss_D: 1.2246 Loss_G: 0.5703\n",
      "BATCH NUMBER 1062\n",
      "Epoch [1/5] Loss_D: 1.1384 Loss_G: 0.5854\n",
      "BATCH NUMBER 1063\n",
      "Epoch [1/5] Loss_D: 1.3117 Loss_G: 0.5558\n",
      "BATCH NUMBER 1064\n",
      "Epoch [1/5] Loss_D: 1.3477 Loss_G: 0.5049\n",
      "BATCH NUMBER 1065\n",
      "Epoch [1/5] Loss_D: 1.3415 Loss_G: 0.4676\n",
      "BATCH NUMBER 1066\n",
      "Epoch [1/5] Loss_D: 1.2506 Loss_G: 0.5280\n",
      "BATCH NUMBER 1067\n",
      "Epoch [1/5] Loss_D: 1.1217 Loss_G: 0.6008\n",
      "BATCH NUMBER 1068\n",
      "Epoch [1/5] Loss_D: 1.3235 Loss_G: 0.4823\n",
      "BATCH NUMBER 1069\n",
      "Epoch [1/5] Loss_D: 1.2601 Loss_G: 0.5968\n",
      "BATCH NUMBER 1070\n",
      "Epoch [1/5] Loss_D: 1.1353 Loss_G: 0.5923\n",
      "BATCH NUMBER 1071\n",
      "Epoch [1/5] Loss_D: 1.3891 Loss_G: 0.5095\n",
      "BATCH NUMBER 1072\n",
      "Epoch [1/5] Loss_D: 1.3401 Loss_G: 0.5172\n",
      "BATCH NUMBER 1073\n",
      "Epoch [1/5] Loss_D: 1.1793 Loss_G: 0.5546\n",
      "BATCH NUMBER 1074\n",
      "Epoch [1/5] Loss_D: 1.1577 Loss_G: 0.5719\n",
      "BATCH NUMBER 1075\n",
      "Epoch [1/5] Loss_D: 1.1935 Loss_G: 0.5454\n",
      "BATCH NUMBER 1076\n",
      "Epoch [1/5] Loss_D: 1.2353 Loss_G: 0.5300\n",
      "BATCH NUMBER 1077\n",
      "Epoch [1/5] Loss_D: 1.3870 Loss_G: 0.4757\n",
      "BATCH NUMBER 1078\n",
      "Epoch [1/5] Loss_D: 1.2033 Loss_G: 0.5438\n",
      "BATCH NUMBER 1079\n",
      "Epoch [1/5] Loss_D: 1.1702 Loss_G: 0.5651\n",
      "BATCH NUMBER 1080\n",
      "Epoch [1/5] Loss_D: 1.1730 Loss_G: 0.5681\n",
      "BATCH NUMBER 1081\n",
      "Epoch [1/5] Loss_D: 1.2455 Loss_G: 0.5088\n",
      "BATCH NUMBER 1082\n",
      "Epoch [1/5] Loss_D: 1.1247 Loss_G: 0.6081\n",
      "BATCH NUMBER 1083\n",
      "Epoch [1/5] Loss_D: 1.1079 Loss_G: 0.6262\n",
      "BATCH NUMBER 1084\n",
      "Epoch [1/5] Loss_D: 1.3738 Loss_G: 0.5498\n",
      "BATCH NUMBER 1085\n",
      "Epoch [1/5] Loss_D: 1.3491 Loss_G: 0.4691\n",
      "BATCH NUMBER 1086\n",
      "Epoch [1/5] Loss_D: 1.1232 Loss_G: 0.6017\n",
      "BATCH NUMBER 1087\n",
      "Epoch [1/5] Loss_D: 1.1377 Loss_G: 0.5872\n",
      "BATCH NUMBER 1088\n",
      "Epoch [1/5] Loss_D: 1.1881 Loss_G: 0.5567\n",
      "BATCH NUMBER 1089\n",
      "Epoch [1/5] Loss_D: 1.1070 Loss_G: 0.6141\n",
      "BATCH NUMBER 1090\n",
      "Epoch [1/5] Loss_D: 1.2372 Loss_G: 0.5196\n",
      "BATCH NUMBER 1091\n",
      "Epoch [1/5] Loss_D: 1.2514 Loss_G: 0.5287\n",
      "BATCH NUMBER 1092\n",
      "Epoch [1/5] Loss_D: 1.2784 Loss_G: 0.5168\n",
      "BATCH NUMBER 1093\n",
      "Epoch [1/5] Loss_D: 1.3441 Loss_G: 0.4661\n",
      "BATCH NUMBER 1094\n",
      "Epoch [1/5] Loss_D: 1.2314 Loss_G: 0.5196\n",
      "BATCH NUMBER 1095\n",
      "Epoch [1/5] Loss_D: 1.1527 Loss_G: 0.5772\n",
      "BATCH NUMBER 1096\n",
      "Epoch [1/5] Loss_D: 1.1406 Loss_G: 0.5848\n",
      "BATCH NUMBER 1097\n",
      "Epoch [1/5] Loss_D: 1.2965 Loss_G: 0.5175\n",
      "BATCH NUMBER 1098\n",
      "Epoch [1/5] Loss_D: 1.2276 Loss_G: 0.5793\n",
      "BATCH NUMBER 1099\n",
      "Epoch [1/5] Loss_D: 1.2107 Loss_G: 0.5447\n",
      "BATCH NUMBER 1100\n",
      "Epoch [1/5] Loss_D: 1.1334 Loss_G: 0.5911\n",
      "BATCH NUMBER 1101\n",
      "Epoch [1/5] Loss_D: 1.2809 Loss_G: 0.4916\n",
      "BATCH NUMBER 1102\n",
      "Epoch [1/5] Loss_D: 1.2821 Loss_G: 0.5557\n",
      "BATCH NUMBER 1103\n",
      "Epoch [1/5] Loss_D: 1.1476 Loss_G: 0.5876\n",
      "BATCH NUMBER 1104\n",
      "Epoch [1/5] Loss_D: 1.3314 Loss_G: 0.5374\n",
      "BATCH NUMBER 1105\n",
      "Epoch [1/5] Loss_D: 1.2552 Loss_G: 0.5222\n",
      "BATCH NUMBER 1106\n",
      "Epoch [1/5] Loss_D: 1.1951 Loss_G: 0.5500\n",
      "BATCH NUMBER 1107\n",
      "Epoch [1/5] Loss_D: 1.3406 Loss_G: 0.5682\n",
      "BATCH NUMBER 1108\n",
      "Epoch [1/5] Loss_D: 1.1732 Loss_G: 0.5566\n",
      "BATCH NUMBER 1109\n",
      "Epoch [1/5] Loss_D: 1.2399 Loss_G: 0.5292\n",
      "BATCH NUMBER 1110\n",
      "Epoch [1/5] Loss_D: 1.1054 Loss_G: 0.6120\n",
      "BATCH NUMBER 1111\n",
      "Epoch [1/5] Loss_D: 1.2705 Loss_G: 0.5167\n",
      "BATCH NUMBER 1112\n",
      "Epoch [1/5] Loss_D: 1.3343 Loss_G: 0.5294\n",
      "BATCH NUMBER 1113\n",
      "Epoch [1/5] Loss_D: 1.1448 Loss_G: 0.5836\n",
      "BATCH NUMBER 1114\n",
      "Epoch [1/5] Loss_D: 1.2804 Loss_G: 0.4954\n",
      "BATCH NUMBER 1115\n",
      "Epoch [1/5] Loss_D: 1.1698 Loss_G: 0.5608\n",
      "BATCH NUMBER 1116\n",
      "Epoch [1/5] Loss_D: 1.3242 Loss_G: 0.5360\n",
      "BATCH NUMBER 1117\n",
      "Epoch [1/5] Loss_D: 1.3279 Loss_G: 0.5444\n",
      "BATCH NUMBER 1118\n",
      "Epoch [1/5] Loss_D: 1.2002 Loss_G: 0.5470\n",
      "BATCH NUMBER 1119\n",
      "Epoch [1/5] Loss_D: 1.1477 Loss_G: 0.5814\n",
      "BATCH NUMBER 1120\n",
      "Epoch [1/5] Loss_D: 1.1758 Loss_G: 0.5603\n",
      "BATCH NUMBER 1121\n",
      "Epoch [1/5] Loss_D: 1.1647 Loss_G: 0.5678\n",
      "BATCH NUMBER 1122\n",
      "Epoch [1/5] Loss_D: 1.2911 Loss_G: 0.4957\n",
      "BATCH NUMBER 1123\n",
      "Epoch [1/5] Loss_D: 1.1237 Loss_G: 0.5991\n",
      "BATCH NUMBER 1124\n",
      "Epoch [1/5] Loss_D: 1.3068 Loss_G: 0.4838\n",
      "BATCH NUMBER 1125\n",
      "Epoch [1/5] Loss_D: 1.2564 Loss_G: 0.5164\n",
      "BATCH NUMBER 1126\n",
      "Epoch [1/5] Loss_D: 1.1472 Loss_G: 0.5820\n",
      "BATCH NUMBER 1127\n",
      "Epoch [1/5] Loss_D: 1.2525 Loss_G: 0.5132\n",
      "BATCH NUMBER 1128\n",
      "Epoch [1/5] Loss_D: 1.2300 Loss_G: 0.5285\n",
      "BATCH NUMBER 1129\n",
      "Epoch [1/5] Loss_D: 1.2528 Loss_G: 0.5070\n",
      "BATCH NUMBER 1130\n",
      "Epoch [1/5] Loss_D: 1.1059 Loss_G: 0.6146\n",
      "BATCH NUMBER 1131\n",
      "Epoch [1/5] Loss_D: 1.2512 Loss_G: 0.5199\n",
      "BATCH NUMBER 1132\n",
      "Epoch [1/5] Loss_D: 1.2878 Loss_G: 0.4917\n",
      "BATCH NUMBER 1133\n",
      "Epoch [1/5] Loss_D: 1.3495 Loss_G: 0.4639\n",
      "BATCH NUMBER 1134\n",
      "Epoch [1/5] Loss_D: 1.2754 Loss_G: 0.5004\n",
      "BATCH NUMBER 1135\n",
      "Epoch [1/5] Loss_D: 1.2284 Loss_G: 0.5165\n",
      "BATCH NUMBER 1136\n",
      "Epoch [1/5] Loss_D: 1.1404 Loss_G: 0.5873\n",
      "BATCH NUMBER 1137\n",
      "Epoch [1/5] Loss_D: 1.1772 Loss_G: 0.6123\n",
      "BATCH NUMBER 1138\n",
      "Epoch [1/5] Loss_D: 1.1231 Loss_G: 0.6006\n",
      "BATCH NUMBER 1139\n",
      "Epoch [1/5] Loss_D: 1.1204 Loss_G: 0.6022\n",
      "BATCH NUMBER 1140\n",
      "Epoch [1/5] Loss_D: 1.2844 Loss_G: 0.4927\n",
      "BATCH NUMBER 1141\n",
      "Epoch [1/5] Loss_D: 1.1936 Loss_G: 0.6170\n",
      "BATCH NUMBER 1142\n",
      "Epoch [1/5] Loss_D: 1.1902 Loss_G: 0.5998\n",
      "BATCH NUMBER 1143\n",
      "Epoch [1/5] Loss_D: 1.3479 Loss_G: 0.5418\n",
      "BATCH NUMBER 1144\n",
      "Epoch [1/5] Loss_D: 1.1751 Loss_G: 0.5622\n",
      "BATCH NUMBER 1145\n",
      "Epoch [1/5] Loss_D: 1.2362 Loss_G: 0.5224\n",
      "BATCH NUMBER 1146\n",
      "Epoch [1/5] Loss_D: 1.1380 Loss_G: 0.5871\n",
      "BATCH NUMBER 1147\n",
      "Epoch [1/5] Loss_D: 1.2853 Loss_G: 0.6119\n",
      "BATCH NUMBER 1148\n",
      "Epoch [1/5] Loss_D: 1.2065 Loss_G: 0.5429\n",
      "BATCH NUMBER 1149\n",
      "Epoch [1/5] Loss_D: 1.1789 Loss_G: 0.5694\n",
      "BATCH NUMBER 1150\n",
      "Epoch [1/5] Loss_D: 1.1067 Loss_G: 0.6109\n",
      "BATCH NUMBER 1151\n",
      "Epoch [1/5] Loss_D: 1.2891 Loss_G: 0.4874\n",
      "BATCH NUMBER 1152\n",
      "Epoch [1/5] Loss_D: 1.1297 Loss_G: 0.5952\n",
      "BATCH NUMBER 1153\n",
      "Epoch [1/5] Loss_D: 1.3205 Loss_G: 0.5271\n",
      "BATCH NUMBER 1154\n",
      "Epoch [1/5] Loss_D: 1.1609 Loss_G: 0.5751\n",
      "BATCH NUMBER 1155\n",
      "Epoch [1/5] Loss_D: 1.6222 Loss_G: 0.4250\n",
      "BATCH NUMBER 1156\n",
      "Epoch [1/5] Loss_D: 1.1624 Loss_G: 0.5713\n",
      "BATCH NUMBER 1157\n",
      "Epoch [1/5] Loss_D: 1.1263 Loss_G: 0.5972\n",
      "BATCH NUMBER 1158\n",
      "Epoch [1/5] Loss_D: 1.4401 Loss_G: 0.4806\n",
      "BATCH NUMBER 1159\n",
      "Epoch [1/5] Loss_D: 1.3489 Loss_G: 0.4535\n",
      "BATCH NUMBER 1160\n",
      "Epoch [1/5] Loss_D: 1.1205 Loss_G: 0.6062\n",
      "BATCH NUMBER 1161\n",
      "Epoch [1/5] Loss_D: 1.1683 Loss_G: 0.5663\n",
      "BATCH NUMBER 1162\n",
      "Epoch [1/5] Loss_D: 1.3564 Loss_G: 0.4551\n",
      "BATCH NUMBER 1163\n",
      "Epoch [1/5] Loss_D: 1.2007 Loss_G: 0.5403\n",
      "BATCH NUMBER 1164\n",
      "Epoch [1/5] Loss_D: 1.3700 Loss_G: 0.5494\n",
      "BATCH NUMBER 1165\n",
      "Epoch [1/5] Loss_D: 1.3423 Loss_G: 0.5307\n",
      "BATCH NUMBER 1166\n",
      "Epoch [1/5] Loss_D: 1.1777 Loss_G: 0.5819\n",
      "BATCH NUMBER 1167\n",
      "Epoch [1/5] Loss_D: 1.1951 Loss_G: 0.5547\n",
      "BATCH NUMBER 1168\n",
      "Epoch [1/5] Loss_D: 1.1376 Loss_G: 0.5861\n",
      "BATCH NUMBER 1169\n",
      "Epoch [1/5] Loss_D: 1.2387 Loss_G: 0.5154\n",
      "BATCH NUMBER 1170\n",
      "Epoch [1/5] Loss_D: 1.3940 Loss_G: 0.4763\n",
      "BATCH NUMBER 1171\n",
      "Epoch [1/5] Loss_D: 1.2371 Loss_G: 0.5232\n",
      "BATCH NUMBER 1172\n",
      "Epoch [1/5] Loss_D: 1.3134 Loss_G: 0.4851\n",
      "BATCH NUMBER 1173\n",
      "Epoch [1/5] Loss_D: 1.1046 Loss_G: 0.6128\n",
      "BATCH NUMBER 1174\n",
      "Epoch [1/5] Loss_D: 1.1448 Loss_G: 0.5881\n",
      "BATCH NUMBER 1175\n",
      "Epoch [1/5] Loss_D: 1.2175 Loss_G: 0.5286\n",
      "BATCH NUMBER 1176\n",
      "Epoch [1/5] Loss_D: 1.2020 Loss_G: 0.5556\n",
      "BATCH NUMBER 1177\n",
      "Epoch [1/5] Loss_D: 1.4201 Loss_G: 0.5019\n",
      "BATCH NUMBER 1178\n",
      "Epoch [1/5] Loss_D: 1.0888 Loss_G: 0.6276\n",
      "BATCH NUMBER 1179\n",
      "Epoch [1/5] Loss_D: 1.2499 Loss_G: 0.5719\n",
      "BATCH NUMBER 1180\n",
      "Epoch [1/5] Loss_D: 1.1359 Loss_G: 0.5876\n",
      "BATCH NUMBER 1181\n",
      "Epoch [1/5] Loss_D: 1.3390 Loss_G: 0.4992\n",
      "BATCH NUMBER 1182\n",
      "Epoch [1/5] Loss_D: 1.1763 Loss_G: 0.5621\n",
      "BATCH NUMBER 1183\n",
      "Epoch [1/5] Loss_D: 1.2178 Loss_G: 0.5410\n",
      "BATCH NUMBER 1184\n",
      "Epoch [1/5] Loss_D: 1.0941 Loss_G: 0.6189\n",
      "BATCH NUMBER 1185\n",
      "Epoch [1/5] Loss_D: 1.1950 Loss_G: 0.5797\n",
      "BATCH NUMBER 1186\n",
      "Epoch [1/5] Loss_D: 1.1574 Loss_G: 0.5693\n",
      "BATCH NUMBER 1187\n",
      "Epoch [1/5] Loss_D: 1.0915 Loss_G: 0.6238\n",
      "BATCH NUMBER 1188\n",
      "Epoch [1/5] Loss_D: 1.3009 Loss_G: 0.5754\n",
      "BATCH NUMBER 1189\n",
      "Epoch [1/5] Loss_D: 1.2904 Loss_G: 0.5597\n",
      "BATCH NUMBER 1190\n",
      "Epoch [1/5] Loss_D: 1.1956 Loss_G: 0.5471\n",
      "BATCH NUMBER 1191\n",
      "Epoch [1/5] Loss_D: 1.2638 Loss_G: 0.5133\n",
      "BATCH NUMBER 1192\n",
      "Epoch [1/5] Loss_D: 1.1858 Loss_G: 0.5997\n",
      "BATCH NUMBER 1193\n",
      "Epoch [1/5] Loss_D: 1.1342 Loss_G: 0.6049\n",
      "BATCH NUMBER 1194\n",
      "Epoch [1/5] Loss_D: 1.1455 Loss_G: 0.5794\n",
      "BATCH NUMBER 1195\n",
      "Epoch [1/5] Loss_D: 1.1146 Loss_G: 0.6083\n",
      "BATCH NUMBER 1196\n",
      "Epoch [1/5] Loss_D: 1.1956 Loss_G: 0.5475\n",
      "BATCH NUMBER 1197\n",
      "Epoch [1/5] Loss_D: 1.2078 Loss_G: 0.5411\n",
      "BATCH NUMBER 1198\n",
      "Epoch [1/5] Loss_D: 1.3108 Loss_G: 0.5182\n",
      "BATCH NUMBER 1199\n",
      "Epoch [1/5] Loss_D: 1.1323 Loss_G: 0.5884\n",
      "BATCH NUMBER 1200\n",
      "Epoch [1/5] Loss_D: 1.2357 Loss_G: 0.5352\n",
      "BATCH NUMBER 1201\n",
      "Epoch [1/5] Loss_D: 1.2908 Loss_G: 0.4844\n",
      "BATCH NUMBER 1202\n",
      "Epoch [1/5] Loss_D: 1.2333 Loss_G: 0.5231\n",
      "BATCH NUMBER 1203\n",
      "Epoch [1/5] Loss_D: 1.4001 Loss_G: 0.5106\n",
      "BATCH NUMBER 1204\n",
      "Epoch [1/5] Loss_D: 1.1543 Loss_G: 0.5855\n",
      "BATCH NUMBER 1205\n",
      "Epoch [1/5] Loss_D: 1.1693 Loss_G: 0.5728\n",
      "BATCH NUMBER 1206\n",
      "Epoch [1/5] Loss_D: 1.1778 Loss_G: 0.5584\n",
      "BATCH NUMBER 1207\n",
      "Epoch [1/5] Loss_D: 1.2497 Loss_G: 0.5202\n",
      "BATCH NUMBER 1208\n",
      "Epoch [1/5] Loss_D: 1.4193 Loss_G: 0.4836\n",
      "BATCH NUMBER 1209\n",
      "Epoch [1/5] Loss_D: 1.1298 Loss_G: 0.6038\n",
      "BATCH NUMBER 1210\n",
      "Epoch [1/5] Loss_D: 1.1135 Loss_G: 0.6065\n",
      "BATCH NUMBER 1211\n",
      "Epoch [1/5] Loss_D: 1.2082 Loss_G: 0.5461\n",
      "BATCH NUMBER 1212\n",
      "Epoch [1/5] Loss_D: 1.1780 Loss_G: 0.5520\n",
      "BATCH NUMBER 1213\n",
      "Epoch [1/5] Loss_D: 1.3399 Loss_G: 0.4856\n",
      "BATCH NUMBER 1214\n",
      "Epoch [1/5] Loss_D: 1.2193 Loss_G: 0.5716\n",
      "BATCH NUMBER 1215\n",
      "Epoch [1/5] Loss_D: 1.2012 Loss_G: 0.5460\n",
      "BATCH NUMBER 1216\n",
      "Epoch [1/5] Loss_D: 1.4012 Loss_G: 0.4570\n",
      "BATCH NUMBER 1217\n",
      "Epoch [1/5] Loss_D: 1.4001 Loss_G: 0.4737\n",
      "BATCH NUMBER 1218\n",
      "Epoch [1/5] Loss_D: 1.1396 Loss_G: 0.5872\n",
      "BATCH NUMBER 1219\n",
      "Epoch [1/5] Loss_D: 1.2634 Loss_G: 0.5030\n",
      "BATCH NUMBER 1220\n",
      "Epoch [1/5] Loss_D: 1.2514 Loss_G: 0.5099\n",
      "BATCH NUMBER 1221\n",
      "Epoch [1/5] Loss_D: 1.2225 Loss_G: 0.5238\n",
      "BATCH NUMBER 1222\n",
      "Epoch [1/5] Loss_D: 1.1319 Loss_G: 0.5918\n",
      "BATCH NUMBER 1223\n",
      "Epoch [1/5] Loss_D: 1.2798 Loss_G: 0.5322\n",
      "BATCH NUMBER 1224\n",
      "Epoch [1/5] Loss_D: 1.1278 Loss_G: 0.6351\n",
      "BATCH NUMBER 1225\n",
      "Epoch [1/5] Loss_D: 1.1217 Loss_G: 0.6017\n",
      "BATCH NUMBER 1226\n",
      "Epoch [1/5] Loss_D: 1.1745 Loss_G: 0.5653\n",
      "BATCH NUMBER 1227\n",
      "Epoch [1/5] Loss_D: 1.3055 Loss_G: 0.5449\n",
      "BATCH NUMBER 1228\n",
      "Epoch [1/5] Loss_D: 1.2575 Loss_G: 0.5408\n",
      "BATCH NUMBER 1229\n",
      "Epoch [1/5] Loss_D: 1.4093 Loss_G: 0.5131\n",
      "BATCH NUMBER 1230\n",
      "Epoch [1/5] Loss_D: 1.1415 Loss_G: 0.5861\n",
      "BATCH NUMBER 1231\n",
      "Epoch [1/5] Loss_D: 1.1811 Loss_G: 0.5671\n",
      "BATCH NUMBER 1232\n",
      "Epoch [1/5] Loss_D: 1.2806 Loss_G: 0.4941\n",
      "BATCH NUMBER 1233\n",
      "Epoch [1/5] Loss_D: 1.2235 Loss_G: 0.5482\n",
      "BATCH NUMBER 1234\n",
      "Epoch [1/5] Loss_D: 1.2220 Loss_G: 0.5648\n",
      "BATCH NUMBER 1235\n",
      "Epoch [1/5] Loss_D: 1.3482 Loss_G: 0.4560\n",
      "BATCH NUMBER 1236\n",
      "Epoch [1/5] Loss_D: 1.1832 Loss_G: 0.5579\n",
      "BATCH NUMBER 1237\n",
      "Epoch [1/5] Loss_D: 1.1387 Loss_G: 0.5934\n",
      "BATCH NUMBER 1238\n",
      "Epoch [1/5] Loss_D: 1.1536 Loss_G: 0.5828\n",
      "BATCH NUMBER 1239\n",
      "Epoch [1/5] Loss_D: 1.1274 Loss_G: 0.6056\n",
      "BATCH NUMBER 1240\n",
      "Epoch [1/5] Loss_D: 1.2768 Loss_G: 0.4881\n",
      "BATCH NUMBER 1241\n",
      "Epoch [1/5] Loss_D: 1.2632 Loss_G: 0.5342\n",
      "BATCH NUMBER 1242\n",
      "Epoch [1/5] Loss_D: 1.1189 Loss_G: 0.6071\n",
      "BATCH NUMBER 1243\n",
      "Epoch [1/5] Loss_D: 1.1592 Loss_G: 0.5780\n",
      "BATCH NUMBER 1244\n",
      "Epoch [1/5] Loss_D: 1.4415 Loss_G: 0.4040\n",
      "BATCH NUMBER 1245\n",
      "Epoch [1/5] Loss_D: 1.2020 Loss_G: 0.5440\n",
      "BATCH NUMBER 1246\n",
      "Epoch [1/5] Loss_D: 1.2434 Loss_G: 0.5235\n",
      "BATCH NUMBER 1247\n",
      "Epoch [1/5] Loss_D: 1.1456 Loss_G: 0.5785\n",
      "BATCH NUMBER 1248\n",
      "Epoch [1/5] Loss_D: 1.1405 Loss_G: 0.5851\n",
      "BATCH NUMBER 1249\n",
      "Epoch [1/5] Loss_D: 1.2649 Loss_G: 0.5081\n",
      "BATCH NUMBER 1250\n",
      "Epoch [1/5] Loss_D: 1.1773 Loss_G: 0.5617\n",
      "BATCH NUMBER 1251\n",
      "Epoch [1/5] Loss_D: 1.1582 Loss_G: 0.5877\n",
      "BATCH NUMBER 1252\n",
      "Epoch [1/5] Loss_D: 1.2429 Loss_G: 0.5244\n",
      "BATCH NUMBER 1253\n",
      "Epoch [1/5] Loss_D: 1.1792 Loss_G: 0.5630\n",
      "BATCH NUMBER 1254\n",
      "Epoch [1/5] Loss_D: 1.4055 Loss_G: 0.4318\n",
      "BATCH NUMBER 1255\n",
      "Epoch [1/5] Loss_D: 1.3508 Loss_G: 0.5259\n",
      "BATCH NUMBER 1256\n",
      "Epoch [1/5] Loss_D: 1.1915 Loss_G: 0.5461\n",
      "BATCH NUMBER 1257\n",
      "Epoch [1/5] Loss_D: 1.2119 Loss_G: 0.5293\n",
      "BATCH NUMBER 1258\n",
      "Epoch [1/5] Loss_D: 1.2626 Loss_G: 0.5104\n",
      "BATCH NUMBER 1259\n",
      "Epoch [1/5] Loss_D: 1.1638 Loss_G: 0.5720\n",
      "BATCH NUMBER 1260\n",
      "Epoch [1/5] Loss_D: 1.1809 Loss_G: 0.5605\n",
      "BATCH NUMBER 1261\n",
      "Epoch [1/5] Loss_D: 1.2091 Loss_G: 0.5677\n",
      "BATCH NUMBER 1262\n",
      "Epoch [1/5] Loss_D: 1.2675 Loss_G: 0.5248\n",
      "BATCH NUMBER 1263\n",
      "Epoch [1/5] Loss_D: 1.2046 Loss_G: 0.5521\n",
      "BATCH NUMBER 1264\n",
      "Epoch [1/5] Loss_D: 1.1262 Loss_G: 0.5980\n",
      "BATCH NUMBER 1265\n",
      "Epoch [1/5] Loss_D: 1.1869 Loss_G: 0.5489\n",
      "BATCH NUMBER 1266\n",
      "Epoch [1/5] Loss_D: 1.1664 Loss_G: 0.5631\n",
      "BATCH NUMBER 1267\n",
      "Epoch [1/5] Loss_D: 1.1341 Loss_G: 0.5913\n",
      "BATCH NUMBER 1268\n",
      "Epoch [1/5] Loss_D: 1.1977 Loss_G: 0.5412\n",
      "BATCH NUMBER 1269\n",
      "Epoch [1/5] Loss_D: 1.1299 Loss_G: 0.6087\n",
      "BATCH NUMBER 1270\n",
      "Epoch [1/5] Loss_D: 1.1144 Loss_G: 0.6044\n",
      "BATCH NUMBER 1271\n",
      "Epoch [1/5] Loss_D: 1.2553 Loss_G: 0.5168\n",
      "BATCH NUMBER 1272\n",
      "Epoch [1/5] Loss_D: 1.1275 Loss_G: 0.5951\n",
      "BATCH NUMBER 1273\n",
      "Epoch [1/5] Loss_D: 1.1622 Loss_G: 0.5660\n",
      "BATCH NUMBER 1274\n",
      "Epoch [1/5] Loss_D: 1.1921 Loss_G: 0.5503\n",
      "BATCH NUMBER 1275\n",
      "Epoch [1/5] Loss_D: 1.3522 Loss_G: 0.5716\n",
      "BATCH NUMBER 1276\n",
      "Epoch [1/5] Loss_D: 1.2685 Loss_G: 0.5721\n",
      "BATCH NUMBER 1277\n",
      "Epoch [1/5] Loss_D: 1.1556 Loss_G: 0.5799\n",
      "BATCH NUMBER 1278\n",
      "Epoch [1/5] Loss_D: 1.1981 Loss_G: 0.5524\n",
      "BATCH NUMBER 1279\n",
      "Epoch [1/5] Loss_D: 1.1389 Loss_G: 0.5859\n",
      "BATCH NUMBER 1280\n",
      "Epoch [1/5] Loss_D: 1.1274 Loss_G: 0.5937\n",
      "BATCH NUMBER 1281\n",
      "Epoch [1/5] Loss_D: 1.1139 Loss_G: 0.6056\n",
      "BATCH NUMBER 1282\n",
      "Epoch [1/5] Loss_D: 1.1861 Loss_G: 0.6575\n",
      "BATCH NUMBER 1283\n",
      "Epoch [1/5] Loss_D: 1.2092 Loss_G: 0.5375\n",
      "BATCH NUMBER 1284\n",
      "Epoch [1/5] Loss_D: 1.4677 Loss_G: 0.4192\n",
      "BATCH NUMBER 1285\n",
      "Epoch [1/5] Loss_D: 1.2245 Loss_G: 0.5390\n",
      "BATCH NUMBER 1286\n",
      "Epoch [1/5] Loss_D: 1.1843 Loss_G: 0.5514\n",
      "BATCH NUMBER 1287\n",
      "Epoch [1/5] Loss_D: 1.2886 Loss_G: 0.5009\n",
      "BATCH NUMBER 1288\n",
      "Epoch [1/5] Loss_D: 1.2610 Loss_G: 0.5181\n",
      "BATCH NUMBER 1289\n",
      "Epoch [1/5] Loss_D: 1.0889 Loss_G: 0.6258\n",
      "BATCH NUMBER 1290\n",
      "Epoch [1/5] Loss_D: 1.2445 Loss_G: 0.5211\n",
      "BATCH NUMBER 1291\n",
      "Epoch [1/5] Loss_D: 1.4153 Loss_G: 0.5027\n",
      "BATCH NUMBER 1292\n",
      "Epoch [1/5] Loss_D: 1.1779 Loss_G: 0.5624\n",
      "BATCH NUMBER 1293\n",
      "Epoch [1/5] Loss_D: 1.1377 Loss_G: 0.5894\n",
      "BATCH NUMBER 1294\n",
      "Epoch [1/5] Loss_D: 1.1787 Loss_G: 0.5662\n",
      "BATCH NUMBER 1295\n",
      "Epoch [1/5] Loss_D: 1.4013 Loss_G: 0.4817\n",
      "BATCH NUMBER 1296\n",
      "Epoch [1/5] Loss_D: 1.1733 Loss_G: 0.6110\n",
      "BATCH NUMBER 1297\n",
      "Epoch [1/5] Loss_D: 1.3246 Loss_G: 0.4739\n",
      "BATCH NUMBER 1298\n",
      "Epoch [1/5] Loss_D: 1.1769 Loss_G: 0.5611\n",
      "BATCH NUMBER 1299\n",
      "Epoch [1/5] Loss_D: 1.2567 Loss_G: 0.5093\n",
      "BATCH NUMBER 1300\n",
      "Epoch [1/5] Loss_D: 1.4383 Loss_G: 0.4389\n",
      "BATCH NUMBER 1301\n",
      "Epoch [1/5] Loss_D: 1.1635 Loss_G: 0.5700\n",
      "BATCH NUMBER 1302\n",
      "Epoch [1/5] Loss_D: 1.2741 Loss_G: 0.5137\n",
      "BATCH NUMBER 1303\n",
      "Epoch [1/5] Loss_D: 1.2666 Loss_G: 0.5244\n",
      "BATCH NUMBER 1304\n",
      "Epoch [1/5] Loss_D: 1.3971 Loss_G: 0.4373\n",
      "BATCH NUMBER 1305\n",
      "Epoch [1/5] Loss_D: 1.2894 Loss_G: 0.5027\n",
      "BATCH NUMBER 1306\n",
      "Epoch [1/5] Loss_D: 1.2275 Loss_G: 0.5379\n",
      "BATCH NUMBER 1307\n",
      "Epoch [1/5] Loss_D: 1.1074 Loss_G: 0.6138\n",
      "BATCH NUMBER 1308\n",
      "Epoch [1/5] Loss_D: 1.0975 Loss_G: 0.6181\n",
      "BATCH NUMBER 1309\n",
      "Epoch [1/5] Loss_D: 1.1359 Loss_G: 0.6002\n",
      "BATCH NUMBER 1310\n",
      "Epoch [1/5] Loss_D: 1.1112 Loss_G: 0.6075\n",
      "BATCH NUMBER 1311\n",
      "Epoch [1/5] Loss_D: 1.2179 Loss_G: 0.5336\n",
      "BATCH NUMBER 1312\n",
      "Epoch [1/5] Loss_D: 1.1447 Loss_G: 0.5891\n",
      "BATCH NUMBER 1313\n",
      "Epoch [1/5] Loss_D: 1.2057 Loss_G: 0.5543\n",
      "BATCH NUMBER 1314\n",
      "Epoch [1/5] Loss_D: 1.1245 Loss_G: 0.5918\n",
      "BATCH NUMBER 1315\n",
      "Epoch [1/5] Loss_D: 1.1914 Loss_G: 0.5722\n",
      "BATCH NUMBER 1316\n",
      "Epoch [1/5] Loss_D: 1.1574 Loss_G: 0.5867\n",
      "BATCH NUMBER 1317\n",
      "Epoch [1/5] Loss_D: 1.1504 Loss_G: 0.5763\n",
      "BATCH NUMBER 1318\n",
      "Epoch [1/5] Loss_D: 1.1265 Loss_G: 0.5967\n",
      "BATCH NUMBER 1319\n",
      "Epoch [1/5] Loss_D: 1.1110 Loss_G: 0.6240\n",
      "BATCH NUMBER 1320\n",
      "Epoch [1/5] Loss_D: 1.1084 Loss_G: 0.6185\n",
      "BATCH NUMBER 1321\n",
      "Epoch [1/5] Loss_D: 1.3139 Loss_G: 0.5030\n",
      "BATCH NUMBER 1322\n",
      "Epoch [1/5] Loss_D: 1.1229 Loss_G: 0.5991\n",
      "BATCH NUMBER 1323\n",
      "Epoch [1/5] Loss_D: 1.1526 Loss_G: 0.5774\n",
      "BATCH NUMBER 1324\n",
      "Epoch [1/5] Loss_D: 1.0699 Loss_G: 0.6405\n",
      "BATCH NUMBER 1325\n",
      "Epoch [1/5] Loss_D: 1.3941 Loss_G: 0.4926\n",
      "BATCH NUMBER 1326\n",
      "Epoch [1/5] Loss_D: 1.2596 Loss_G: 0.5179\n",
      "BATCH NUMBER 1327\n",
      "Epoch [1/5] Loss_D: 1.1258 Loss_G: 0.5979\n",
      "BATCH NUMBER 1328\n",
      "Epoch [1/5] Loss_D: 1.3406 Loss_G: 0.5537\n",
      "BATCH NUMBER 1329\n",
      "Epoch [1/5] Loss_D: 1.1084 Loss_G: 0.6127\n",
      "BATCH NUMBER 1330\n",
      "Epoch [1/5] Loss_D: 1.1719 Loss_G: 0.5718\n",
      "BATCH NUMBER 1331\n",
      "Epoch [1/5] Loss_D: 1.3207 Loss_G: 0.4822\n",
      "BATCH NUMBER 1332\n",
      "Epoch [1/5] Loss_D: 1.1484 Loss_G: 0.5823\n",
      "BATCH NUMBER 1333\n",
      "Epoch [1/5] Loss_D: 1.0823 Loss_G: 0.6308\n",
      "BATCH NUMBER 1334\n",
      "Epoch [1/5] Loss_D: 1.2846 Loss_G: 0.5328\n",
      "BATCH NUMBER 1335\n",
      "Epoch [1/5] Loss_D: 1.2078 Loss_G: 0.5459\n",
      "BATCH NUMBER 1336\n",
      "Epoch [1/5] Loss_D: 1.2127 Loss_G: 0.5817\n",
      "BATCH NUMBER 1337\n",
      "Epoch [1/5] Loss_D: 1.1913 Loss_G: 0.5476\n",
      "BATCH NUMBER 1338\n",
      "Epoch [1/5] Loss_D: 1.1351 Loss_G: 0.6113\n",
      "BATCH NUMBER 1339\n",
      "Epoch [1/5] Loss_D: 1.2419 Loss_G: 0.5243\n",
      "BATCH NUMBER 1340\n",
      "Epoch [1/5] Loss_D: 1.0842 Loss_G: 0.6291\n",
      "BATCH NUMBER 1341\n",
      "Epoch [1/5] Loss_D: 1.2239 Loss_G: 0.5478\n",
      "BATCH NUMBER 1342\n",
      "Epoch [1/5] Loss_D: 1.2151 Loss_G: 0.5516\n",
      "BATCH NUMBER 1343\n",
      "Epoch [1/5] Loss_D: 1.2383 Loss_G: 0.5242\n",
      "BATCH NUMBER 1344\n",
      "Epoch [1/5] Loss_D: 1.1825 Loss_G: 0.5593\n",
      "BATCH NUMBER 1345\n",
      "Epoch [1/5] Loss_D: 1.3841 Loss_G: 0.4565\n",
      "BATCH NUMBER 1346\n",
      "Epoch [1/5] Loss_D: 1.2510 Loss_G: 0.5138\n",
      "BATCH NUMBER 1347\n",
      "Epoch [1/5] Loss_D: 1.1141 Loss_G: 0.6053\n",
      "BATCH NUMBER 1348\n",
      "Epoch [1/5] Loss_D: 1.2505 Loss_G: 0.5151\n",
      "BATCH NUMBER 1349\n",
      "Epoch [1/5] Loss_D: 1.4122 Loss_G: 0.4327\n",
      "BATCH NUMBER 1350\n",
      "Epoch [1/5] Loss_D: 1.1608 Loss_G: 0.5831\n",
      "BATCH NUMBER 1351\n",
      "Epoch [1/5] Loss_D: 1.3023 Loss_G: 0.4953\n",
      "BATCH NUMBER 1352\n",
      "Epoch [1/5] Loss_D: 1.2556 Loss_G: 0.5203\n",
      "BATCH NUMBER 1353\n",
      "Epoch [1/5] Loss_D: 1.1406 Loss_G: 0.5812\n",
      "BATCH NUMBER 1354\n",
      "Epoch [1/5] Loss_D: 1.2722 Loss_G: 0.5823\n",
      "BATCH NUMBER 1355\n",
      "Epoch [1/5] Loss_D: 1.2828 Loss_G: 0.4925\n",
      "BATCH NUMBER 1356\n",
      "Epoch [1/5] Loss_D: 1.2909 Loss_G: 0.4942\n",
      "BATCH NUMBER 1357\n",
      "Epoch [1/5] Loss_D: 1.1586 Loss_G: 0.5885\n",
      "BATCH NUMBER 1358\n",
      "Epoch [1/5] Loss_D: 1.0842 Loss_G: 0.6315\n",
      "BATCH NUMBER 1359\n",
      "Epoch [1/5] Loss_D: 1.2004 Loss_G: 0.5453\n",
      "BATCH NUMBER 1360\n",
      "Epoch [1/5] Loss_D: 1.1203 Loss_G: 0.5989\n",
      "BATCH NUMBER 1361\n",
      "Epoch [1/5] Loss_D: 1.2282 Loss_G: 0.5334\n",
      "BATCH NUMBER 1362\n",
      "Epoch [1/5] Loss_D: 1.2653 Loss_G: 0.5116\n",
      "BATCH NUMBER 1363\n",
      "Epoch [1/5] Loss_D: 1.1571 Loss_G: 0.5762\n",
      "BATCH NUMBER 1364\n",
      "Epoch [1/5] Loss_D: 1.2673 Loss_G: 0.5549\n",
      "BATCH NUMBER 1365\n",
      "Epoch [1/5] Loss_D: 1.1779 Loss_G: 0.5612\n",
      "BATCH NUMBER 1366\n",
      "Epoch [1/5] Loss_D: 1.1110 Loss_G: 0.6098\n",
      "BATCH NUMBER 1367\n",
      "Epoch [1/5] Loss_D: 1.2255 Loss_G: 0.5325\n",
      "BATCH NUMBER 1368\n",
      "Epoch [1/5] Loss_D: 1.1775 Loss_G: 0.5573\n",
      "BATCH NUMBER 1369\n",
      "Epoch [1/5] Loss_D: 1.1497 Loss_G: 0.5836\n",
      "BATCH NUMBER 1370\n",
      "Epoch [1/5] Loss_D: 1.0839 Loss_G: 0.6281\n",
      "BATCH NUMBER 1371\n",
      "Epoch [1/5] Loss_D: 1.2446 Loss_G: 0.5546\n",
      "BATCH NUMBER 1372\n",
      "Epoch [1/5] Loss_D: 1.2988 Loss_G: 0.4874\n",
      "BATCH NUMBER 1373\n",
      "Epoch [1/5] Loss_D: 1.4358 Loss_G: 0.4135\n",
      "BATCH NUMBER 1374\n",
      "Epoch [1/5] Loss_D: 1.1989 Loss_G: 0.5717\n",
      "BATCH NUMBER 1375\n",
      "Epoch [1/5] Loss_D: 1.2289 Loss_G: 0.5316\n",
      "BATCH NUMBER 1376\n",
      "Epoch [1/5] Loss_D: 1.3214 Loss_G: 0.4763\n",
      "BATCH NUMBER 1377\n",
      "Epoch [1/5] Loss_D: 1.1730 Loss_G: 0.5630\n",
      "BATCH NUMBER 1378\n",
      "Epoch [1/5] Loss_D: 1.1332 Loss_G: 0.5964\n",
      "BATCH NUMBER 1379\n",
      "Epoch [1/5] Loss_D: 1.4006 Loss_G: 0.4887\n",
      "BATCH NUMBER 1380\n",
      "Epoch [1/5] Loss_D: 1.2595 Loss_G: 0.5350\n",
      "BATCH NUMBER 1381\n",
      "Epoch [1/5] Loss_D: 1.2664 Loss_G: 0.5155\n",
      "BATCH NUMBER 1382\n",
      "Epoch [1/5] Loss_D: 1.1505 Loss_G: 0.5816\n",
      "BATCH NUMBER 1383\n",
      "Epoch [1/5] Loss_D: 1.0848 Loss_G: 0.6350\n",
      "BATCH NUMBER 1384\n",
      "Epoch [1/5] Loss_D: 1.2705 Loss_G: 0.5178\n",
      "BATCH NUMBER 1385\n",
      "Epoch [1/5] Loss_D: 1.1506 Loss_G: 0.5828\n",
      "BATCH NUMBER 1386\n",
      "Epoch [1/5] Loss_D: 1.1180 Loss_G: 0.6101\n",
      "BATCH NUMBER 1387\n",
      "Epoch [1/5] Loss_D: 1.1825 Loss_G: 0.5597\n",
      "BATCH NUMBER 1388\n",
      "Epoch [1/5] Loss_D: 1.2958 Loss_G: 0.5274\n",
      "BATCH NUMBER 1389\n",
      "Epoch [1/5] Loss_D: 1.2326 Loss_G: 0.5415\n",
      "BATCH NUMBER 1390\n",
      "Epoch [1/5] Loss_D: 1.0754 Loss_G: 0.6473\n",
      "BATCH NUMBER 1391\n",
      "Epoch [1/5] Loss_D: 1.1469 Loss_G: 0.5796\n",
      "BATCH NUMBER 1392\n",
      "Epoch [1/5] Loss_D: 1.2280 Loss_G: 0.5309\n",
      "BATCH NUMBER 1393\n",
      "Epoch [1/5] Loss_D: 1.2039 Loss_G: 0.5497\n",
      "BATCH NUMBER 1394\n",
      "Epoch [1/5] Loss_D: 1.2916 Loss_G: 0.5199\n",
      "BATCH NUMBER 1395\n",
      "Epoch [1/5] Loss_D: 1.1665 Loss_G: 0.5707\n",
      "BATCH NUMBER 1396\n",
      "Epoch [1/5] Loss_D: 1.1377 Loss_G: 0.5851\n",
      "BATCH NUMBER 1397\n",
      "Epoch [1/5] Loss_D: 1.2148 Loss_G: 0.5341\n",
      "BATCH NUMBER 1398\n",
      "Epoch [1/5] Loss_D: 1.1712 Loss_G: 0.5737\n",
      "BATCH NUMBER 1399\n",
      "Epoch [1/5] Loss_D: 1.1695 Loss_G: 0.5785\n",
      "BATCH NUMBER 1400\n",
      "Epoch [1/5] Loss_D: 1.1612 Loss_G: 0.5892\n",
      "BATCH NUMBER 1401\n",
      "Epoch [1/5] Loss_D: 1.3483 Loss_G: 0.4720\n",
      "BATCH NUMBER 1402\n",
      "Epoch [1/5] Loss_D: 1.1822 Loss_G: 0.5648\n",
      "BATCH NUMBER 1403\n",
      "Epoch [1/5] Loss_D: 1.3461 Loss_G: 0.4627\n",
      "BATCH NUMBER 1404\n",
      "Epoch [1/5] Loss_D: 1.1908 Loss_G: 0.5572\n",
      "BATCH NUMBER 1405\n",
      "Epoch [1/5] Loss_D: 1.1640 Loss_G: 0.5627\n",
      "BATCH NUMBER 1406\n",
      "Epoch [1/5] Loss_D: 1.1561 Loss_G: 0.5682\n",
      "BATCH NUMBER 1407\n",
      "Epoch [1/5] Loss_D: 1.2750 Loss_G: 0.5061\n",
      "BATCH NUMBER 1408\n",
      "Epoch [1/5] Loss_D: 1.1643 Loss_G: 0.5762\n",
      "BATCH NUMBER 1409\n",
      "Epoch [1/5] Loss_D: 1.1042 Loss_G: 0.6209\n",
      "BATCH NUMBER 1410\n",
      "Epoch [1/5] Loss_D: 1.1213 Loss_G: 0.6011\n",
      "BATCH NUMBER 1411\n",
      "Epoch [1/5] Loss_D: 1.1406 Loss_G: 0.5884\n",
      "BATCH NUMBER 1412\n",
      "Epoch [1/5] Loss_D: 1.2058 Loss_G: 0.5979\n",
      "BATCH NUMBER 1413\n",
      "Epoch [1/5] Loss_D: 1.1835 Loss_G: 0.5620\n",
      "BATCH NUMBER 1414\n",
      "Epoch [1/5] Loss_D: 1.1893 Loss_G: 0.5512\n",
      "BATCH NUMBER 1415\n",
      "Epoch [1/5] Loss_D: 1.1937 Loss_G: 0.5530\n",
      "BATCH NUMBER 1416\n",
      "Epoch [1/5] Loss_D: 1.2676 Loss_G: 0.5050\n",
      "BATCH NUMBER 1417\n",
      "Epoch [1/5] Loss_D: 1.2125 Loss_G: 0.5861\n",
      "BATCH NUMBER 1418\n",
      "Epoch [1/5] Loss_D: 1.1772 Loss_G: 0.5573\n",
      "BATCH NUMBER 1419\n",
      "Epoch [1/5] Loss_D: 1.1876 Loss_G: 0.5686\n",
      "BATCH NUMBER 1420\n",
      "Epoch [1/5] Loss_D: 1.1564 Loss_G: 0.5794\n",
      "BATCH NUMBER 1421\n",
      "Epoch [1/5] Loss_D: 1.1804 Loss_G: 0.6457\n",
      "BATCH NUMBER 1422\n",
      "Epoch [1/5] Loss_D: 1.1917 Loss_G: 0.5623\n",
      "BATCH NUMBER 1423\n",
      "Epoch [1/5] Loss_D: 1.3671 Loss_G: 0.5535\n",
      "BATCH NUMBER 1424\n",
      "Epoch [1/5] Loss_D: 1.2357 Loss_G: 0.5407\n",
      "BATCH NUMBER 1425\n",
      "Epoch [1/5] Loss_D: 1.1354 Loss_G: 0.5860\n",
      "BATCH NUMBER 1426\n",
      "Epoch [1/5] Loss_D: 1.1796 Loss_G: 0.5599\n",
      "BATCH NUMBER 1427\n",
      "Epoch [1/5] Loss_D: 1.2013 Loss_G: 0.5470\n",
      "BATCH NUMBER 1428\n",
      "Epoch [1/5] Loss_D: 1.1225 Loss_G: 0.5971\n",
      "BATCH NUMBER 1429\n",
      "Epoch [1/5] Loss_D: 1.2471 Loss_G: 0.5229\n",
      "BATCH NUMBER 1430\n",
      "Epoch [1/5] Loss_D: 1.3610 Loss_G: 0.4506\n",
      "BATCH NUMBER 1431\n",
      "Epoch [1/5] Loss_D: 1.2069 Loss_G: 0.5486\n",
      "BATCH NUMBER 1432\n",
      "Epoch [1/5] Loss_D: 1.1172 Loss_G: 0.6032\n",
      "BATCH NUMBER 1433\n",
      "Epoch [1/5] Loss_D: 1.2533 Loss_G: 0.5132\n",
      "BATCH NUMBER 1434\n",
      "Epoch [1/5] Loss_D: 1.2646 Loss_G: 0.5104\n",
      "BATCH NUMBER 1435\n",
      "Epoch [1/5] Loss_D: 1.2894 Loss_G: 0.5968\n",
      "BATCH NUMBER 1436\n",
      "Epoch [1/5] Loss_D: 1.2304 Loss_G: 0.5339\n",
      "BATCH NUMBER 1437\n",
      "Epoch [1/5] Loss_D: 1.1375 Loss_G: 0.5861\n",
      "BATCH NUMBER 1438\n",
      "Epoch [1/5] Loss_D: 1.2920 Loss_G: 0.4863\n",
      "BATCH NUMBER 1439\n",
      "Epoch [1/5] Loss_D: 1.2046 Loss_G: 0.5454\n",
      "BATCH NUMBER 1440\n",
      "Epoch [1/5] Loss_D: 1.0754 Loss_G: 0.6355\n",
      "BATCH NUMBER 1441\n",
      "Epoch [1/5] Loss_D: 1.2358 Loss_G: 0.6202\n",
      "BATCH NUMBER 1442\n",
      "Epoch [1/5] Loss_D: 1.1813 Loss_G: 0.5809\n",
      "BATCH NUMBER 1443\n",
      "Epoch [1/5] Loss_D: 1.3061 Loss_G: 0.4845\n",
      "BATCH NUMBER 1444\n",
      "Epoch [1/5] Loss_D: 1.2789 Loss_G: 0.4977\n",
      "BATCH NUMBER 1445\n",
      "Epoch [1/5] Loss_D: 1.2312 Loss_G: 0.5421\n",
      "BATCH NUMBER 1446\n",
      "Epoch [1/5] Loss_D: 1.2567 Loss_G: 0.5252\n",
      "BATCH NUMBER 1447\n",
      "Epoch [1/5] Loss_D: 1.4141 Loss_G: 0.5133\n",
      "BATCH NUMBER 1448\n",
      "Epoch [1/5] Loss_D: 1.1428 Loss_G: 0.5882\n",
      "BATCH NUMBER 1449\n",
      "Epoch [1/5] Loss_D: 1.1670 Loss_G: 0.5716\n",
      "BATCH NUMBER 1450\n",
      "Epoch [1/5] Loss_D: 1.1110 Loss_G: 0.6098\n",
      "BATCH NUMBER 1451\n",
      "Epoch [1/5] Loss_D: 1.2418 Loss_G: 0.5405\n",
      "BATCH NUMBER 1452\n",
      "Epoch [1/5] Loss_D: 1.2012 Loss_G: 0.5559\n",
      "BATCH NUMBER 1453\n",
      "Epoch [1/5] Loss_D: 1.2349 Loss_G: 0.5446\n",
      "BATCH NUMBER 1454\n",
      "Epoch [1/5] Loss_D: 1.4442 Loss_G: 0.5051\n",
      "BATCH NUMBER 1455\n",
      "Epoch [1/5] Loss_D: 1.1660 Loss_G: 0.5652\n",
      "BATCH NUMBER 1456\n",
      "Epoch [1/5] Loss_D: 1.3673 Loss_G: 0.5536\n",
      "BATCH NUMBER 1457\n",
      "Epoch [1/5] Loss_D: 1.1661 Loss_G: 0.5776\n",
      "BATCH NUMBER 1458\n",
      "Epoch [1/5] Loss_D: 1.2618 Loss_G: 0.5210\n",
      "BATCH NUMBER 1459\n",
      "Epoch [1/5] Loss_D: 1.1141 Loss_G: 0.6057\n",
      "BATCH NUMBER 1460\n",
      "Epoch [1/5] Loss_D: 1.2077 Loss_G: 0.5375\n",
      "BATCH NUMBER 1461\n",
      "Epoch [1/5] Loss_D: 1.1194 Loss_G: 0.6202\n",
      "BATCH NUMBER 1462\n",
      "Epoch [1/5] Loss_D: 1.3460 Loss_G: 0.4664\n",
      "BATCH NUMBER 1463\n",
      "Epoch [1/5] Loss_D: 1.1094 Loss_G: 0.6118\n",
      "BATCH NUMBER 1464\n",
      "Epoch [1/5] Loss_D: 1.1301 Loss_G: 0.6166\n",
      "BATCH NUMBER 1465\n",
      "Epoch [1/5] Loss_D: 1.1270 Loss_G: 0.5931\n",
      "BATCH NUMBER 1466\n",
      "Epoch [1/5] Loss_D: 1.2375 Loss_G: 0.5315\n",
      "BATCH NUMBER 1467\n",
      "Epoch [1/5] Loss_D: 1.1373 Loss_G: 0.5940\n",
      "BATCH NUMBER 1468\n",
      "Epoch [1/5] Loss_D: 1.0897 Loss_G: 0.6445\n",
      "BATCH NUMBER 1469\n",
      "Epoch [1/5] Loss_D: 1.3138 Loss_G: 0.4711\n",
      "BATCH NUMBER 1470\n",
      "Epoch [1/5] Loss_D: 1.3216 Loss_G: 0.4996\n",
      "BATCH NUMBER 1471\n",
      "Epoch [1/5] Loss_D: 1.3725 Loss_G: 0.5595\n",
      "BATCH NUMBER 1472\n",
      "Epoch [1/5] Loss_D: 1.1488 Loss_G: 0.5959\n",
      "BATCH NUMBER 1473\n",
      "Epoch [1/5] Loss_D: 1.3167 Loss_G: 0.4784\n",
      "BATCH NUMBER 1474\n",
      "Epoch [1/5] Loss_D: 1.1839 Loss_G: 0.5537\n",
      "BATCH NUMBER 1475\n",
      "Epoch [1/5] Loss_D: 1.1799 Loss_G: 0.6049\n",
      "BATCH NUMBER 1476\n",
      "Epoch [1/5] Loss_D: 1.1887 Loss_G: 0.5547\n",
      "BATCH NUMBER 1477\n",
      "Epoch [1/5] Loss_D: 1.2651 Loss_G: 0.5224\n",
      "BATCH NUMBER 1478\n",
      "Epoch [1/5] Loss_D: 1.2333 Loss_G: 0.5236\n",
      "BATCH NUMBER 1479\n",
      "Epoch [1/5] Loss_D: 1.1392 Loss_G: 0.5953\n",
      "BATCH NUMBER 1480\n",
      "Epoch [1/5] Loss_D: 1.2458 Loss_G: 0.5302\n",
      "BATCH NUMBER 1481\n",
      "Epoch [1/5] Loss_D: 1.2330 Loss_G: 0.5326\n",
      "BATCH NUMBER 1482\n",
      "Epoch [1/5] Loss_D: 1.1640 Loss_G: 0.5654\n",
      "BATCH NUMBER 1483\n",
      "Epoch [1/5] Loss_D: 1.1857 Loss_G: 0.5559\n",
      "BATCH NUMBER 1484\n",
      "Epoch [1/5] Loss_D: 1.2398 Loss_G: 0.5283\n",
      "BATCH NUMBER 1485\n",
      "Epoch [1/5] Loss_D: 1.2682 Loss_G: 0.5049\n",
      "BATCH NUMBER 1486\n",
      "Epoch [1/5] Loss_D: 1.3257 Loss_G: 0.4739\n",
      "BATCH NUMBER 1487\n",
      "Epoch [1/5] Loss_D: 1.2554 Loss_G: 0.5234\n",
      "BATCH NUMBER 1488\n",
      "Epoch [1/5] Loss_D: 1.2322 Loss_G: 0.5483\n",
      "BATCH NUMBER 1489\n",
      "Epoch [1/5] Loss_D: 1.2097 Loss_G: 0.5511\n",
      "BATCH NUMBER 1490\n",
      "Epoch [1/5] Loss_D: 1.1403 Loss_G: 0.5927\n",
      "BATCH NUMBER 1491\n",
      "Epoch [1/5] Loss_D: 1.2373 Loss_G: 0.5237\n",
      "BATCH NUMBER 1492\n",
      "Epoch [1/5] Loss_D: 1.1255 Loss_G: 0.5959\n",
      "BATCH NUMBER 1493\n",
      "Epoch [1/5] Loss_D: 1.2320 Loss_G: 0.5480\n",
      "BATCH NUMBER 1494\n",
      "Epoch [1/5] Loss_D: 1.1500 Loss_G: 0.5829\n",
      "BATCH NUMBER 1495\n",
      "Epoch [1/5] Loss_D: 1.1126 Loss_G: 0.6135\n",
      "BATCH NUMBER 1496\n",
      "Epoch [1/5] Loss_D: 1.3018 Loss_G: 0.5165\n",
      "BATCH NUMBER 1497\n",
      "Epoch [1/5] Loss_D: 1.2479 Loss_G: 0.5905\n",
      "BATCH NUMBER 1498\n",
      "Epoch [1/5] Loss_D: 1.1643 Loss_G: 0.6135\n",
      "BATCH NUMBER 1499\n",
      "Epoch [1/5] Loss_D: 1.1727 Loss_G: 0.5757\n",
      "BATCH NUMBER 1500\n",
      "Epoch [1/5] Loss_D: 1.1825 Loss_G: 0.5607\n",
      "BATCH NUMBER 1501\n",
      "Epoch [1/5] Loss_D: 1.1445 Loss_G: 0.5898\n",
      "BATCH NUMBER 1502\n",
      "Epoch [1/5] Loss_D: 1.1149 Loss_G: 0.6021\n",
      "BATCH NUMBER 1503\n",
      "Epoch [1/5] Loss_D: 1.2309 Loss_G: 0.5644\n",
      "BATCH NUMBER 1504\n",
      "Epoch [1/5] Loss_D: 1.0810 Loss_G: 0.6312\n",
      "BATCH NUMBER 1505\n",
      "Epoch [1/5] Loss_D: 1.1285 Loss_G: 0.6045\n",
      "BATCH NUMBER 1506\n",
      "Epoch [1/5] Loss_D: 1.2914 Loss_G: 0.4881\n",
      "BATCH NUMBER 1507\n",
      "Epoch [1/5] Loss_D: 1.1928 Loss_G: 0.5587\n",
      "BATCH NUMBER 1508\n",
      "Epoch [1/5] Loss_D: 1.1731 Loss_G: 0.5729\n",
      "BATCH NUMBER 1509\n",
      "Epoch [1/5] Loss_D: 1.2482 Loss_G: 0.5837\n",
      "BATCH NUMBER 1510\n",
      "Epoch [1/5] Loss_D: 1.1328 Loss_G: 0.5956\n",
      "BATCH NUMBER 1511\n",
      "Epoch [1/5] Loss_D: 1.2741 Loss_G: 0.5064\n",
      "BATCH NUMBER 1512\n",
      "Epoch [1/5] Loss_D: 1.2209 Loss_G: 0.5442\n",
      "BATCH NUMBER 1513\n",
      "Epoch [1/5] Loss_D: 1.1401 Loss_G: 0.5876\n",
      "BATCH NUMBER 1514\n",
      "Epoch [1/5] Loss_D: 1.1139 Loss_G: 0.6148\n",
      "BATCH NUMBER 1515\n",
      "Epoch [1/5] Loss_D: 1.1742 Loss_G: 0.5616\n",
      "BATCH NUMBER 1516\n",
      "Epoch [1/5] Loss_D: 1.1519 Loss_G: 0.5838\n",
      "BATCH NUMBER 1517\n",
      "Epoch [1/5] Loss_D: 1.1445 Loss_G: 0.5827\n",
      "BATCH NUMBER 1518\n",
      "Epoch [1/5] Loss_D: 1.1780 Loss_G: 0.5617\n",
      "BATCH NUMBER 1519\n",
      "Epoch [1/5] Loss_D: 1.2205 Loss_G: 0.5333\n",
      "BATCH NUMBER 1520\n",
      "Epoch [1/5] Loss_D: 1.2020 Loss_G: 0.5479\n",
      "BATCH NUMBER 1521\n",
      "Epoch [1/5] Loss_D: 1.2089 Loss_G: 0.5427\n",
      "BATCH NUMBER 1522\n",
      "Epoch [1/5] Loss_D: 1.1224 Loss_G: 0.5995\n",
      "BATCH NUMBER 1523\n",
      "Epoch [1/5] Loss_D: 1.0694 Loss_G: 0.6398\n",
      "BATCH NUMBER 1524\n",
      "Epoch [1/5] Loss_D: 1.1427 Loss_G: 0.5907\n",
      "BATCH NUMBER 1525\n",
      "Epoch [1/5] Loss_D: 1.1350 Loss_G: 0.5882\n",
      "BATCH NUMBER 1526\n",
      "Epoch [1/5] Loss_D: 1.2243 Loss_G: 0.5426\n",
      "BATCH NUMBER 1527\n",
      "Epoch [1/5] Loss_D: 1.2759 Loss_G: 0.5027\n",
      "BATCH NUMBER 1528\n",
      "Epoch [1/5] Loss_D: 1.1362 Loss_G: 0.5970\n",
      "BATCH NUMBER 1529\n",
      "Epoch [1/5] Loss_D: 1.2166 Loss_G: 0.6032\n",
      "BATCH NUMBER 1530\n",
      "Epoch [1/5] Loss_D: 1.3040 Loss_G: 0.4783\n",
      "BATCH NUMBER 1531\n",
      "Epoch [1/5] Loss_D: 1.2361 Loss_G: 0.5231\n",
      "BATCH NUMBER 1532\n",
      "Epoch [1/5] Loss_D: 1.1869 Loss_G: 0.6196\n",
      "BATCH NUMBER 1533\n",
      "Epoch [1/5] Loss_D: 1.3670 Loss_G: 0.5359\n",
      "BATCH NUMBER 1534\n",
      "Epoch [1/5] Loss_D: 1.1284 Loss_G: 0.5947\n",
      "BATCH NUMBER 1535\n",
      "Epoch [1/5] Loss_D: 1.2533 Loss_G: 0.5744\n",
      "BATCH NUMBER 1536\n",
      "Epoch [1/5] Loss_D: 1.2285 Loss_G: 0.5275\n",
      "BATCH NUMBER 1537\n",
      "Epoch [1/5] Loss_D: 1.2687 Loss_G: 0.5142\n",
      "BATCH NUMBER 1538\n",
      "Epoch [1/5] Loss_D: 1.2942 Loss_G: 0.4930\n",
      "BATCH NUMBER 1539\n",
      "Epoch [1/5] Loss_D: 1.1182 Loss_G: 0.6013\n",
      "BATCH NUMBER 1540\n",
      "Epoch [1/5] Loss_D: 1.1924 Loss_G: 0.5435\n",
      "BATCH NUMBER 1541\n",
      "Epoch [1/5] Loss_D: 1.3444 Loss_G: 0.5988\n",
      "BATCH NUMBER 1542\n",
      "Epoch [1/5] Loss_D: 1.3086 Loss_G: 0.5675\n",
      "BATCH NUMBER 1543\n",
      "Epoch [1/5] Loss_D: 1.1930 Loss_G: 0.5596\n",
      "BATCH NUMBER 1544\n",
      "Epoch [1/5] Loss_D: 1.1761 Loss_G: 0.5653\n",
      "BATCH NUMBER 1545\n",
      "Epoch [1/5] Loss_D: 1.3056 Loss_G: 0.4721\n",
      "BATCH NUMBER 1546\n",
      "Epoch [1/5] Loss_D: 1.2475 Loss_G: 0.5156\n",
      "BATCH NUMBER 1547\n",
      "Epoch [1/5] Loss_D: 1.2223 Loss_G: 0.5450\n",
      "BATCH NUMBER 1548\n",
      "Epoch [1/5] Loss_D: 1.3136 Loss_G: 0.5476\n",
      "BATCH NUMBER 1549\n",
      "Epoch [1/5] Loss_D: 1.1652 Loss_G: 0.5649\n",
      "BATCH NUMBER 1550\n",
      "Epoch [1/5] Loss_D: 1.0775 Loss_G: 0.6413\n",
      "BATCH NUMBER 1551\n",
      "Epoch [1/5] Loss_D: 1.1855 Loss_G: 0.5945\n",
      "BATCH NUMBER 1552\n",
      "Epoch [1/5] Loss_D: 1.1933 Loss_G: 0.5602\n",
      "BATCH NUMBER 1553\n",
      "Epoch [1/5] Loss_D: 1.5286 Loss_G: 0.5784\n",
      "BATCH NUMBER 1554\n",
      "Epoch [1/5] Loss_D: 1.1145 Loss_G: 0.6053\n",
      "BATCH NUMBER 1555\n",
      "Epoch [1/5] Loss_D: 1.0899 Loss_G: 0.6234\n",
      "BATCH NUMBER 1556\n",
      "Epoch [1/5] Loss_D: 1.1748 Loss_G: 0.5747\n",
      "BATCH NUMBER 1557\n",
      "Epoch [1/5] Loss_D: 1.2434 Loss_G: 0.5320\n",
      "BATCH NUMBER 1558\n",
      "Epoch [1/5] Loss_D: 1.1050 Loss_G: 0.6085\n",
      "BATCH NUMBER 1559\n",
      "Epoch [1/5] Loss_D: 1.2828 Loss_G: 0.4981\n",
      "BATCH NUMBER 1560\n",
      "Epoch [1/5] Loss_D: 1.1268 Loss_G: 0.5943\n",
      "BATCH NUMBER 1561\n",
      "Epoch [1/5] Loss_D: 1.2686 Loss_G: 0.5061\n",
      "BATCH NUMBER 1562\n",
      "Epoch [1/5] Loss_D: 1.2336 Loss_G: 0.5390\n",
      "BATCH NUMBER 1563\n",
      "Epoch [1/5] Loss_D: 1.3251 Loss_G: 0.5376\n",
      "BATCH NUMBER 1564\n",
      "Epoch [1/5] Loss_D: 1.4137 Loss_G: 0.4934\n",
      "BATCH NUMBER 1565\n",
      "Epoch [1/5] Loss_D: 1.0726 Loss_G: 0.6385\n",
      "BATCH NUMBER 1566\n",
      "Epoch [1/5] Loss_D: 1.1766 Loss_G: 0.5719\n",
      "BATCH NUMBER 1567\n",
      "Epoch [1/5] Loss_D: 1.1152 Loss_G: 0.6104\n",
      "BATCH NUMBER 1568\n",
      "Epoch [1/5] Loss_D: 1.3016 Loss_G: 0.5377\n",
      "BATCH NUMBER 1569\n",
      "Epoch [1/5] Loss_D: 1.0788 Loss_G: 0.6362\n",
      "BATCH NUMBER 1570\n",
      "Epoch [1/5] Loss_D: 1.1946 Loss_G: 0.5547\n",
      "BATCH NUMBER 1571\n",
      "Epoch [1/5] Loss_D: 1.2795 Loss_G: 0.5243\n",
      "BATCH NUMBER 1572\n",
      "Epoch [1/5] Loss_D: 1.1993 Loss_G: 0.5764\n",
      "BATCH NUMBER 1573\n",
      "Epoch [1/5] Loss_D: 1.1376 Loss_G: 0.5866\n",
      "BATCH NUMBER 1574\n",
      "Epoch [1/5] Loss_D: 1.1906 Loss_G: 0.5622\n",
      "BATCH NUMBER 1575\n",
      "Epoch [1/5] Loss_D: 1.1267 Loss_G: 0.6017\n",
      "BATCH NUMBER 1576\n",
      "Epoch [1/5] Loss_D: 1.1324 Loss_G: 0.5951\n",
      "BATCH NUMBER 1577\n",
      "Epoch [1/5] Loss_D: 1.1746 Loss_G: 0.5709\n",
      "BATCH NUMBER 1578\n",
      "Epoch [1/5] Loss_D: 1.0877 Loss_G: 0.6264\n",
      "BATCH NUMBER 1579\n",
      "Epoch [1/5] Loss_D: 1.1297 Loss_G: 0.6033\n",
      "BATCH NUMBER 1580\n",
      "Epoch [1/5] Loss_D: 1.1838 Loss_G: 0.5972\n",
      "BATCH NUMBER 1581\n",
      "Epoch [1/5] Loss_D: 1.2221 Loss_G: 0.5373\n",
      "BATCH NUMBER 1582\n",
      "Epoch [1/5] Loss_D: 1.2548 Loss_G: 0.5208\n",
      "BATCH NUMBER 1583\n",
      "Epoch [1/5] Loss_D: 1.1310 Loss_G: 0.5902\n",
      "BATCH NUMBER 1584\n",
      "Epoch [1/5] Loss_D: 1.1452 Loss_G: 0.5795\n",
      "BATCH NUMBER 1585\n",
      "Epoch [1/5] Loss_D: 1.2266 Loss_G: 0.5316\n",
      "BATCH NUMBER 1586\n",
      "Epoch [1/5] Loss_D: 1.2682 Loss_G: 0.5033\n",
      "BATCH NUMBER 1587\n",
      "Epoch [1/5] Loss_D: 1.1975 Loss_G: 0.5550\n",
      "BATCH NUMBER 1588\n",
      "Epoch [1/5] Loss_D: 1.1012 Loss_G: 0.6138\n",
      "BATCH NUMBER 1589\n",
      "Epoch [1/5] Loss_D: 1.3472 Loss_G: 0.5241\n",
      "BATCH NUMBER 1590\n",
      "Epoch [1/5] Loss_D: 1.2380 Loss_G: 0.5501\n",
      "BATCH NUMBER 1591\n",
      "Epoch [1/5] Loss_D: 1.3072 Loss_G: 0.5024\n",
      "BATCH NUMBER 1592\n",
      "Epoch [1/5] Loss_D: 1.1275 Loss_G: 0.5943\n",
      "BATCH NUMBER 1593\n",
      "Epoch [1/5] Loss_D: 1.1114 Loss_G: 0.6111\n",
      "BATCH NUMBER 1594\n",
      "Epoch [1/5] Loss_D: 1.2177 Loss_G: 0.5456\n",
      "BATCH NUMBER 1595\n",
      "Epoch [1/5] Loss_D: 1.1054 Loss_G: 0.6169\n",
      "BATCH NUMBER 1596\n",
      "Epoch [1/5] Loss_D: 1.2017 Loss_G: 0.5511\n",
      "BATCH NUMBER 1597\n",
      "Epoch [1/5] Loss_D: 1.1640 Loss_G: 0.5752\n",
      "BATCH NUMBER 1598\n",
      "Epoch [1/5] Loss_D: 1.2597 Loss_G: 0.5232\n",
      "BATCH NUMBER 1599\n",
      "Epoch [1/5] Loss_D: 1.1882 Loss_G: 0.5645\n",
      "BATCH NUMBER 1600\n",
      "Epoch [1/5] Loss_D: 1.2045 Loss_G: 0.5490\n",
      "BATCH NUMBER 1601\n",
      "Epoch [1/5] Loss_D: 1.2142 Loss_G: 0.5689\n",
      "BATCH NUMBER 1602\n",
      "Epoch [1/5] Loss_D: 1.0931 Loss_G: 0.6223\n",
      "BATCH NUMBER 1603\n",
      "Epoch [1/5] Loss_D: 1.1050 Loss_G: 0.6128\n",
      "BATCH NUMBER 1604\n",
      "Epoch [1/5] Loss_D: 1.1401 Loss_G: 0.5838\n",
      "BATCH NUMBER 1605\n",
      "Epoch [1/5] Loss_D: 1.1995 Loss_G: 0.5680\n",
      "BATCH NUMBER 1606\n",
      "Epoch [1/5] Loss_D: 1.2255 Loss_G: 0.5264\n",
      "BATCH NUMBER 1607\n",
      "Epoch [1/5] Loss_D: 1.3139 Loss_G: 0.5179\n",
      "BATCH NUMBER 1608\n",
      "Epoch [1/5] Loss_D: 1.2809 Loss_G: 0.5034\n",
      "BATCH NUMBER 1609\n",
      "Epoch [1/5] Loss_D: 1.0844 Loss_G: 0.6303\n",
      "BATCH NUMBER 1610\n",
      "Epoch [1/5] Loss_D: 1.1695 Loss_G: 0.5909\n",
      "BATCH NUMBER 1611\n",
      "Epoch [1/5] Loss_D: 1.3050 Loss_G: 0.4847\n",
      "BATCH NUMBER 1612\n",
      "Epoch [1/5] Loss_D: 1.2027 Loss_G: 0.6108\n",
      "BATCH NUMBER 1613\n",
      "Epoch [1/5] Loss_D: 1.2008 Loss_G: 0.6073\n",
      "BATCH NUMBER 1614\n",
      "Epoch [1/5] Loss_D: 1.3351 Loss_G: 0.5339\n",
      "BATCH NUMBER 1615\n",
      "Epoch [1/5] Loss_D: 1.3523 Loss_G: 0.5128\n",
      "BATCH NUMBER 1616\n",
      "Epoch [1/5] Loss_D: 1.2006 Loss_G: 0.5533\n",
      "BATCH NUMBER 1617\n",
      "Epoch [1/5] Loss_D: 1.2471 Loss_G: 0.5266\n",
      "BATCH NUMBER 1618\n",
      "Epoch [1/5] Loss_D: 1.1150 Loss_G: 0.6043\n",
      "BATCH NUMBER 1619\n",
      "Epoch [1/5] Loss_D: 1.2746 Loss_G: 0.5073\n",
      "BATCH NUMBER 1620\n",
      "Epoch [1/5] Loss_D: 1.1925 Loss_G: 0.5509\n",
      "BATCH NUMBER 1621\n",
      "Epoch [1/5] Loss_D: 1.1591 Loss_G: 0.5759\n",
      "BATCH NUMBER 1622\n",
      "Epoch [1/5] Loss_D: 1.0889 Loss_G: 0.6271\n",
      "BATCH NUMBER 1623\n",
      "Epoch [1/5] Loss_D: 1.1340 Loss_G: 0.5921\n",
      "BATCH NUMBER 1624\n",
      "Epoch [1/5] Loss_D: 1.1321 Loss_G: 0.5902\n",
      "BATCH NUMBER 1625\n",
      "Epoch [1/5] Loss_D: 1.2011 Loss_G: 0.5482\n",
      "BATCH NUMBER 1626\n",
      "Epoch [1/5] Loss_D: 1.4827 Loss_G: 0.4907\n",
      "BATCH NUMBER 1627\n",
      "Epoch [1/5] Loss_D: 1.1053 Loss_G: 0.6290\n",
      "BATCH NUMBER 1628\n",
      "Epoch [1/5] Loss_D: 1.1438 Loss_G: 0.5881\n",
      "BATCH NUMBER 1629\n",
      "Epoch [1/5] Loss_D: 1.1366 Loss_G: 0.5904\n",
      "BATCH NUMBER 1630\n",
      "Epoch [1/5] Loss_D: 1.3750 Loss_G: 0.5088\n",
      "BATCH NUMBER 1631\n",
      "Epoch [1/5] Loss_D: 1.1307 Loss_G: 0.6071\n",
      "BATCH NUMBER 1632\n",
      "Epoch [1/5] Loss_D: 1.3956 Loss_G: 0.4739\n",
      "BATCH NUMBER 1633\n",
      "Epoch [1/5] Loss_D: 1.1656 Loss_G: 0.5715\n",
      "BATCH NUMBER 1634\n",
      "Epoch [1/5] Loss_D: 1.3129 Loss_G: 0.4836\n",
      "BATCH NUMBER 1635\n",
      "Epoch [1/5] Loss_D: 1.2567 Loss_G: 0.5105\n",
      "BATCH NUMBER 1636\n",
      "Epoch [1/5] Loss_D: 1.1161 Loss_G: 0.6275\n",
      "BATCH NUMBER 1637\n",
      "Epoch [1/5] Loss_D: 1.2122 Loss_G: 0.5669\n",
      "BATCH NUMBER 1638\n",
      "Epoch [1/5] Loss_D: 1.2280 Loss_G: 0.5443\n",
      "BATCH NUMBER 1639\n",
      "Epoch [1/5] Loss_D: 1.1203 Loss_G: 0.6025\n",
      "BATCH NUMBER 1640\n",
      "Epoch [1/5] Loss_D: 1.1628 Loss_G: 0.5831\n",
      "BATCH NUMBER 1641\n",
      "Epoch [1/5] Loss_D: 1.1466 Loss_G: 0.5845\n",
      "BATCH NUMBER 1642\n",
      "Epoch [1/5] Loss_D: 1.1629 Loss_G: 0.5751\n",
      "BATCH NUMBER 1643\n",
      "Epoch [1/5] Loss_D: 1.1992 Loss_G: 0.5599\n",
      "BATCH NUMBER 1644\n",
      "Epoch [1/5] Loss_D: 1.2435 Loss_G: 0.5289\n",
      "BATCH NUMBER 1645\n",
      "Epoch [1/5] Loss_D: 1.1772 Loss_G: 0.5615\n",
      "BATCH NUMBER 1646\n",
      "Epoch [1/5] Loss_D: 1.3234 Loss_G: 0.4811\n",
      "BATCH NUMBER 1647\n",
      "Epoch [1/5] Loss_D: 1.1452 Loss_G: 0.5905\n",
      "BATCH NUMBER 1648\n",
      "Epoch [1/5] Loss_D: 1.2672 Loss_G: 0.5730\n",
      "BATCH NUMBER 1649\n",
      "Epoch [1/5] Loss_D: 1.2523 Loss_G: 0.5181\n",
      "BATCH NUMBER 1650\n",
      "Epoch [1/5] Loss_D: 1.2189 Loss_G: 0.5422\n",
      "BATCH NUMBER 1651\n",
      "Epoch [1/5] Loss_D: 1.0513 Loss_G: 0.6543\n",
      "BATCH NUMBER 1652\n",
      "Epoch [1/5] Loss_D: 1.3671 Loss_G: 0.4587\n",
      "BATCH NUMBER 1653\n",
      "Epoch [1/5] Loss_D: 1.2194 Loss_G: 0.5440\n",
      "BATCH NUMBER 1654\n",
      "Epoch [1/5] Loss_D: 1.2763 Loss_G: 0.5087\n",
      "BATCH NUMBER 1655\n",
      "Epoch [1/5] Loss_D: 1.1457 Loss_G: 0.5889\n",
      "BATCH NUMBER 1656\n",
      "Epoch [1/5] Loss_D: 1.2507 Loss_G: 0.5144\n",
      "BATCH NUMBER 1657\n",
      "Epoch [1/5] Loss_D: 1.1601 Loss_G: 0.5754\n",
      "BATCH NUMBER 1658\n",
      "Epoch [1/5] Loss_D: 1.0530 Loss_G: 0.6538\n",
      "BATCH NUMBER 1659\n",
      "Epoch [1/5] Loss_D: 1.1940 Loss_G: 0.5482\n",
      "BATCH NUMBER 1660\n",
      "Epoch [1/5] Loss_D: 1.2205 Loss_G: 0.5322\n",
      "BATCH NUMBER 1661\n",
      "Epoch [1/5] Loss_D: 1.1885 Loss_G: 0.5621\n",
      "BATCH NUMBER 1662\n",
      "Epoch [1/5] Loss_D: 1.2568 Loss_G: 0.5135\n",
      "BATCH NUMBER 1663\n",
      "Epoch [1/5] Loss_D: 1.2971 Loss_G: 0.4958\n",
      "BATCH NUMBER 1664\n",
      "Epoch [1/5] Loss_D: 1.2990 Loss_G: 0.4892\n",
      "BATCH NUMBER 1665\n",
      "Epoch [1/5] Loss_D: 1.3682 Loss_G: 0.4673\n",
      "BATCH NUMBER 1666\n",
      "Epoch [1/5] Loss_D: 1.1031 Loss_G: 0.6215\n",
      "BATCH NUMBER 1667\n",
      "Epoch [1/5] Loss_D: 1.1666 Loss_G: 0.6254\n",
      "BATCH NUMBER 1668\n",
      "Epoch [1/5] Loss_D: 1.2054 Loss_G: 0.5521\n",
      "BATCH NUMBER 1669\n",
      "Epoch [1/5] Loss_D: 1.1786 Loss_G: 0.5674\n",
      "BATCH NUMBER 1670\n",
      "Epoch [1/5] Loss_D: 1.1591 Loss_G: 0.5790\n",
      "BATCH NUMBER 1671\n",
      "Epoch [1/5] Loss_D: 1.1630 Loss_G: 0.5655\n",
      "BATCH NUMBER 1672\n",
      "Epoch [1/5] Loss_D: 1.0812 Loss_G: 0.6333\n",
      "BATCH NUMBER 1673\n",
      "Epoch [1/5] Loss_D: 1.2733 Loss_G: 0.5072\n",
      "BATCH NUMBER 1674\n",
      "Epoch [1/5] Loss_D: 1.1403 Loss_G: 0.5890\n",
      "BATCH NUMBER 1675\n",
      "Epoch [1/5] Loss_D: 1.2590 Loss_G: 0.5138\n",
      "BATCH NUMBER 1676\n",
      "Epoch [1/5] Loss_D: 1.2812 Loss_G: 0.5154\n",
      "BATCH NUMBER 1677\n",
      "Epoch [1/5] Loss_D: 1.2938 Loss_G: 0.4945\n",
      "BATCH NUMBER 1678\n",
      "Epoch [1/5] Loss_D: 1.1283 Loss_G: 0.6188\n",
      "BATCH NUMBER 1679\n",
      "Epoch [1/5] Loss_D: 1.2285 Loss_G: 0.5363\n",
      "BATCH NUMBER 1680\n",
      "Epoch [1/5] Loss_D: 1.1784 Loss_G: 0.5649\n",
      "BATCH NUMBER 1681\n",
      "Epoch [1/5] Loss_D: 1.3009 Loss_G: 0.5023\n",
      "BATCH NUMBER 1682\n",
      "Epoch [1/5] Loss_D: 1.0633 Loss_G: 0.6475\n",
      "BATCH NUMBER 1683\n",
      "Epoch [1/5] Loss_D: 1.2287 Loss_G: 0.5349\n",
      "BATCH NUMBER 1684\n",
      "Epoch [1/5] Loss_D: 1.3598 Loss_G: 0.5228\n",
      "BATCH NUMBER 1685\n",
      "Epoch [1/5] Loss_D: 1.1976 Loss_G: 0.5650\n",
      "BATCH NUMBER 1686\n",
      "Epoch [1/5] Loss_D: 1.0788 Loss_G: 0.6326\n",
      "BATCH NUMBER 1687\n",
      "Epoch [1/5] Loss_D: 1.1535 Loss_G: 0.5791\n",
      "BATCH NUMBER 1688\n",
      "Epoch [1/5] Loss_D: 1.3409 Loss_G: 0.4798\n",
      "BATCH NUMBER 1689\n",
      "Epoch [1/5] Loss_D: 1.1367 Loss_G: 0.5852\n",
      "BATCH NUMBER 1690\n",
      "Epoch [1/5] Loss_D: 1.1822 Loss_G: 0.5734\n",
      "BATCH NUMBER 1691\n",
      "Epoch [1/5] Loss_D: 1.3605 Loss_G: 0.5380\n",
      "BATCH NUMBER 1692\n",
      "Epoch [1/5] Loss_D: 1.2013 Loss_G: 0.5650\n",
      "BATCH NUMBER 1693\n",
      "Epoch [1/5] Loss_D: 1.1670 Loss_G: 0.6178\n",
      "BATCH NUMBER 1694\n",
      "Epoch [1/5] Loss_D: 1.1572 Loss_G: 0.5721\n",
      "BATCH NUMBER 1695\n",
      "Epoch [1/5] Loss_D: 1.1460 Loss_G: 0.5844\n",
      "BATCH NUMBER 1696\n",
      "Epoch [1/5] Loss_D: 1.1437 Loss_G: 0.5823\n",
      "BATCH NUMBER 1697\n",
      "Epoch [1/5] Loss_D: 1.1193 Loss_G: 0.6006\n",
      "BATCH NUMBER 1698\n",
      "Epoch [1/5] Loss_D: 1.2137 Loss_G: 0.5440\n",
      "BATCH NUMBER 1699\n",
      "Epoch [1/5] Loss_D: 1.2699 Loss_G: 0.5167\n",
      "BATCH NUMBER 1700\n",
      "Epoch [1/5] Loss_D: 1.2890 Loss_G: 0.4935\n",
      "BATCH NUMBER 1701\n",
      "Epoch [1/5] Loss_D: 1.2134 Loss_G: 0.6054\n",
      "BATCH NUMBER 1702\n",
      "Epoch [1/5] Loss_D: 1.2779 Loss_G: 0.5079\n",
      "BATCH NUMBER 1703\n",
      "Epoch [1/5] Loss_D: 1.2542 Loss_G: 0.6041\n",
      "BATCH NUMBER 1704\n",
      "Epoch [1/5] Loss_D: 1.2586 Loss_G: 0.5262\n",
      "BATCH NUMBER 1705\n",
      "Epoch [1/5] Loss_D: 1.2932 Loss_G: 0.5297\n",
      "BATCH NUMBER 1706\n",
      "Epoch [1/5] Loss_D: 1.1215 Loss_G: 0.6006\n",
      "BATCH NUMBER 1707\n",
      "Epoch [1/5] Loss_D: 1.1702 Loss_G: 0.5633\n",
      "BATCH NUMBER 1708\n",
      "Epoch [1/5] Loss_D: 1.3066 Loss_G: 0.4877\n",
      "BATCH NUMBER 1709\n",
      "Epoch [1/5] Loss_D: 1.1316 Loss_G: 0.5949\n",
      "BATCH NUMBER 1710\n",
      "Epoch [1/5] Loss_D: 1.0580 Loss_G: 0.6512\n",
      "BATCH NUMBER 1711\n",
      "Epoch [1/5] Loss_D: 1.1238 Loss_G: 0.5985\n",
      "BATCH NUMBER 1712\n",
      "Epoch [1/5] Loss_D: 1.3424 Loss_G: 0.5550\n",
      "BATCH NUMBER 1713\n",
      "Epoch [1/5] Loss_D: 1.2222 Loss_G: 0.5808\n",
      "BATCH NUMBER 1714\n",
      "Epoch [1/5] Loss_D: 1.2039 Loss_G: 0.5484\n",
      "BATCH NUMBER 1715\n",
      "Epoch [1/5] Loss_D: 1.1197 Loss_G: 0.6074\n",
      "BATCH NUMBER 1716\n",
      "Epoch [1/5] Loss_D: 1.1598 Loss_G: 0.5819\n",
      "BATCH NUMBER 1717\n",
      "Epoch [1/5] Loss_D: 1.3414 Loss_G: 0.5503\n",
      "BATCH NUMBER 1718\n",
      "Epoch [1/5] Loss_D: 1.1248 Loss_G: 0.6094\n",
      "BATCH NUMBER 1719\n",
      "Epoch [1/5] Loss_D: 1.2376 Loss_G: 0.5352\n",
      "BATCH NUMBER 1720\n",
      "Epoch [1/5] Loss_D: 1.3403 Loss_G: 0.5349\n",
      "BATCH NUMBER 1721\n",
      "Epoch [1/5] Loss_D: 1.1714 Loss_G: 0.5675\n",
      "BATCH NUMBER 1722\n",
      "Epoch [1/5] Loss_D: 1.2775 Loss_G: 0.4944\n",
      "BATCH NUMBER 1723\n",
      "Epoch [1/5] Loss_D: 1.1031 Loss_G: 0.6123\n",
      "BATCH NUMBER 1724\n",
      "Epoch [1/5] Loss_D: 1.0942 Loss_G: 0.6176\n",
      "BATCH NUMBER 1725\n",
      "Epoch [1/5] Loss_D: 1.2533 Loss_G: 0.5237\n",
      "BATCH NUMBER 1726\n",
      "Epoch [1/5] Loss_D: 1.1881 Loss_G: 0.5621\n",
      "BATCH NUMBER 1727\n",
      "Epoch [1/5] Loss_D: 1.2179 Loss_G: 0.5772\n",
      "BATCH NUMBER 1728\n",
      "Epoch [1/5] Loss_D: 1.2792 Loss_G: 0.4967\n",
      "BATCH NUMBER 1729\n",
      "Epoch [1/5] Loss_D: 1.0620 Loss_G: 0.6462\n",
      "BATCH NUMBER 1730\n",
      "Epoch [1/5] Loss_D: 1.3696 Loss_G: 0.5721\n",
      "BATCH NUMBER 1731\n",
      "Epoch [1/5] Loss_D: 1.1201 Loss_G: 0.6015\n",
      "BATCH NUMBER 1732\n",
      "Epoch [1/5] Loss_D: 1.1916 Loss_G: 0.6206\n",
      "BATCH NUMBER 1733\n",
      "Epoch [1/5] Loss_D: 1.3334 Loss_G: 0.5122\n",
      "BATCH NUMBER 1734\n",
      "Epoch [1/5] Loss_D: 1.1838 Loss_G: 0.5646\n",
      "BATCH NUMBER 1735\n",
      "Epoch [1/5] Loss_D: 1.1782 Loss_G: 0.5695\n",
      "BATCH NUMBER 1736\n",
      "Epoch [1/5] Loss_D: 1.1840 Loss_G: 0.5569\n",
      "BATCH NUMBER 1737\n",
      "Epoch [1/5] Loss_D: 1.1173 Loss_G: 0.6119\n",
      "BATCH NUMBER 1738\n",
      "Epoch [1/5] Loss_D: 1.0520 Loss_G: 0.6562\n",
      "BATCH NUMBER 1739\n",
      "Epoch [1/5] Loss_D: 1.1872 Loss_G: 0.6043\n",
      "BATCH NUMBER 1740\n",
      "Epoch [1/5] Loss_D: 1.1423 Loss_G: 0.5925\n",
      "BATCH NUMBER 1741\n",
      "Epoch [1/5] Loss_D: 1.1751 Loss_G: 0.5799\n",
      "BATCH NUMBER 1742\n",
      "Epoch [1/5] Loss_D: 1.2066 Loss_G: 0.5532\n",
      "BATCH NUMBER 1743\n",
      "Epoch [1/5] Loss_D: 1.2166 Loss_G: 0.5650\n",
      "BATCH NUMBER 1744\n",
      "Epoch [1/5] Loss_D: 1.1206 Loss_G: 0.5986\n",
      "BATCH NUMBER 1745\n",
      "Epoch [1/5] Loss_D: 1.1953 Loss_G: 0.5561\n",
      "BATCH NUMBER 1746\n",
      "Epoch [1/5] Loss_D: 1.0794 Loss_G: 0.6333\n",
      "BATCH NUMBER 1747\n",
      "Epoch [1/5] Loss_D: 1.1073 Loss_G: 0.6099\n",
      "BATCH NUMBER 1748\n",
      "Epoch [1/5] Loss_D: 1.1508 Loss_G: 0.5817\n",
      "BATCH NUMBER 1749\n",
      "Epoch [1/5] Loss_D: 1.0974 Loss_G: 0.6195\n",
      "BATCH NUMBER 1750\n",
      "Epoch [1/5] Loss_D: 1.3176 Loss_G: 0.4813\n",
      "BATCH NUMBER 1751\n",
      "Epoch [1/5] Loss_D: 1.1687 Loss_G: 0.5910\n",
      "BATCH NUMBER 1752\n",
      "Epoch [1/5] Loss_D: 1.1075 Loss_G: 0.6120\n",
      "BATCH NUMBER 1753\n",
      "Epoch [1/5] Loss_D: 1.1869 Loss_G: 0.5614\n",
      "BATCH NUMBER 1754\n",
      "Epoch [1/5] Loss_D: 1.1506 Loss_G: 0.5838\n",
      "BATCH NUMBER 1755\n",
      "Epoch [1/5] Loss_D: 1.2601 Loss_G: 0.5278\n",
      "BATCH NUMBER 1756\n",
      "Epoch [1/5] Loss_D: 1.2641 Loss_G: 0.5224\n",
      "BATCH NUMBER 1757\n",
      "Epoch [1/5] Loss_D: 1.1790 Loss_G: 0.5672\n",
      "BATCH NUMBER 1758\n",
      "Epoch [1/5] Loss_D: 1.2345 Loss_G: 0.5885\n",
      "BATCH NUMBER 1759\n",
      "Epoch [1/5] Loss_D: 1.1834 Loss_G: 0.5552\n",
      "BATCH NUMBER 1760\n",
      "Epoch [1/5] Loss_D: 1.2665 Loss_G: 0.5057\n",
      "BATCH NUMBER 1761\n",
      "Epoch [1/5] Loss_D: 1.2168 Loss_G: 0.5462\n",
      "BATCH NUMBER 1762\n",
      "Epoch [1/5] Loss_D: 1.0891 Loss_G: 0.6220\n",
      "BATCH NUMBER 1763\n",
      "Epoch [1/5] Loss_D: 1.2488 Loss_G: 0.5334\n",
      "BATCH NUMBER 1764\n",
      "Epoch [1/5] Loss_D: 1.2041 Loss_G: 0.5339\n",
      "BATCH NUMBER 1765\n",
      "Epoch [1/5] Loss_D: 1.1757 Loss_G: 0.5618\n",
      "BATCH NUMBER 1766\n",
      "Epoch [1/5] Loss_D: 1.1222 Loss_G: 0.5989\n",
      "BATCH NUMBER 1767\n",
      "Epoch [1/5] Loss_D: 1.2477 Loss_G: 0.5284\n",
      "BATCH NUMBER 1768\n",
      "Epoch [1/5] Loss_D: 1.2463 Loss_G: 0.5519\n",
      "BATCH NUMBER 1769\n",
      "Epoch [1/5] Loss_D: 1.2459 Loss_G: 0.5246\n",
      "BATCH NUMBER 1770\n",
      "Epoch [1/5] Loss_D: 1.2575 Loss_G: 0.5121\n",
      "BATCH NUMBER 1771\n",
      "Epoch [1/5] Loss_D: 1.0942 Loss_G: 0.6175\n",
      "BATCH NUMBER 1772\n",
      "Epoch [1/5] Loss_D: 1.2756 Loss_G: 0.5106\n",
      "BATCH NUMBER 1773\n",
      "Epoch [1/5] Loss_D: 1.2600 Loss_G: 0.5146\n",
      "BATCH NUMBER 1774\n",
      "Epoch [1/5] Loss_D: 1.0764 Loss_G: 0.6372\n",
      "BATCH NUMBER 1775\n",
      "Epoch [1/5] Loss_D: 1.1427 Loss_G: 0.5878\n",
      "BATCH NUMBER 1776\n",
      "Epoch [1/5] Loss_D: 1.1445 Loss_G: 0.5878\n",
      "BATCH NUMBER 1777\n",
      "Epoch [1/5] Loss_D: 1.1121 Loss_G: 0.6056\n",
      "BATCH NUMBER 1778\n",
      "Epoch [1/5] Loss_D: 1.0941 Loss_G: 0.6186\n",
      "BATCH NUMBER 1779\n",
      "Epoch [1/5] Loss_D: 1.1604 Loss_G: 0.5885\n",
      "BATCH NUMBER 1780\n",
      "Epoch [1/5] Loss_D: 1.2229 Loss_G: 0.5326\n",
      "BATCH NUMBER 1781\n",
      "Epoch [1/5] Loss_D: 1.1165 Loss_G: 0.6031\n",
      "BATCH NUMBER 1782\n",
      "Epoch [1/5] Loss_D: 1.1713 Loss_G: 0.5863\n",
      "BATCH NUMBER 1783\n",
      "Epoch [1/5] Loss_D: 1.2116 Loss_G: 0.5499\n",
      "BATCH NUMBER 1784\n",
      "Epoch [1/5] Loss_D: 1.1483 Loss_G: 0.5908\n",
      "BATCH NUMBER 1785\n",
      "Epoch [1/5] Loss_D: 1.0308 Loss_G: 0.6720\n",
      "BATCH NUMBER 1786\n",
      "Epoch [1/5] Loss_D: 1.1281 Loss_G: 0.6019\n",
      "BATCH NUMBER 1787\n",
      "Epoch [1/5] Loss_D: 1.1115 Loss_G: 0.6052\n",
      "BATCH NUMBER 1788\n",
      "Epoch [1/5] Loss_D: 1.2797 Loss_G: 0.5018\n",
      "BATCH NUMBER 1789\n",
      "Epoch [1/5] Loss_D: 1.1163 Loss_G: 0.6111\n",
      "BATCH NUMBER 1790\n",
      "Epoch [1/5] Loss_D: 1.2843 Loss_G: 0.4963\n",
      "BATCH NUMBER 1791\n",
      "Epoch [1/5] Loss_D: 1.1806 Loss_G: 0.5577\n",
      "BATCH NUMBER 1792\n",
      "Epoch [1/5] Loss_D: 1.1295 Loss_G: 0.5994\n",
      "BATCH NUMBER 1793\n",
      "Epoch [1/5] Loss_D: 1.2282 Loss_G: 0.5325\n",
      "BATCH NUMBER 1794\n",
      "Epoch [1/5] Loss_D: 1.1508 Loss_G: 0.5834\n",
      "BATCH NUMBER 1795\n",
      "Epoch [1/5] Loss_D: 1.1593 Loss_G: 0.5824\n",
      "BATCH NUMBER 1796\n",
      "Epoch [1/5] Loss_D: 1.2172 Loss_G: 0.5706\n",
      "BATCH NUMBER 1797\n",
      "Epoch [1/5] Loss_D: 1.2843 Loss_G: 0.5077\n",
      "BATCH NUMBER 1798\n",
      "Epoch [1/5] Loss_D: 1.1335 Loss_G: 0.5882\n",
      "BATCH NUMBER 1799\n",
      "Epoch [1/5] Loss_D: 1.2813 Loss_G: 0.5080\n",
      "BATCH NUMBER 1800\n",
      "Epoch [1/5] Loss_D: 1.1114 Loss_G: 0.6100\n",
      "BATCH NUMBER 1801\n",
      "Epoch [1/5] Loss_D: 1.2769 Loss_G: 0.5042\n",
      "BATCH NUMBER 1802\n",
      "Epoch [1/5] Loss_D: 1.3477 Loss_G: 0.5420\n",
      "BATCH NUMBER 1803\n",
      "Epoch [1/5] Loss_D: 1.1398 Loss_G: 0.6030\n",
      "BATCH NUMBER 1804\n",
      "Epoch [1/5] Loss_D: 1.2332 Loss_G: 0.5313\n",
      "BATCH NUMBER 1805\n",
      "Epoch [1/5] Loss_D: 1.2169 Loss_G: 0.5433\n",
      "BATCH NUMBER 1806\n",
      "Epoch [1/5] Loss_D: 1.0662 Loss_G: 0.6424\n",
      "BATCH NUMBER 1807\n",
      "Epoch [1/5] Loss_D: 1.3924 Loss_G: 0.4369\n",
      "BATCH NUMBER 1808\n",
      "Epoch [1/5] Loss_D: 1.1892 Loss_G: 0.5692\n",
      "BATCH NUMBER 1809\n",
      "Epoch [1/5] Loss_D: 1.1171 Loss_G: 0.6110\n",
      "BATCH NUMBER 1810\n",
      "Epoch [1/5] Loss_D: 1.2530 Loss_G: 0.5369\n",
      "BATCH NUMBER 1811\n",
      "Epoch [1/5] Loss_D: 1.3020 Loss_G: 0.5898\n",
      "BATCH NUMBER 1812\n",
      "Epoch [1/5] Loss_D: 1.1479 Loss_G: 0.5999\n",
      "BATCH NUMBER 1813\n",
      "Epoch [1/5] Loss_D: 1.1726 Loss_G: 0.5951\n",
      "BATCH NUMBER 1814\n",
      "Epoch [1/5] Loss_D: 1.1453 Loss_G: 0.5860\n",
      "BATCH NUMBER 1815\n",
      "Epoch [1/5] Loss_D: 1.1025 Loss_G: 0.6196\n",
      "BATCH NUMBER 1816\n",
      "Epoch [1/5] Loss_D: 1.3781 Loss_G: 0.5518\n",
      "BATCH NUMBER 1817\n",
      "Epoch [1/5] Loss_D: 1.1944 Loss_G: 0.5495\n",
      "BATCH NUMBER 1818\n",
      "Epoch [1/5] Loss_D: 1.1905 Loss_G: 0.5503\n",
      "BATCH NUMBER 1819\n",
      "Epoch [1/5] Loss_D: 1.1130 Loss_G: 0.6068\n",
      "BATCH NUMBER 1820\n",
      "Epoch [1/5] Loss_D: 1.1573 Loss_G: 0.5725\n",
      "BATCH NUMBER 1821\n",
      "Epoch [1/5] Loss_D: 1.2235 Loss_G: 0.5742\n",
      "BATCH NUMBER 1822\n",
      "Epoch [1/5] Loss_D: 1.3427 Loss_G: 0.4689\n",
      "BATCH NUMBER 1823\n",
      "Epoch [1/5] Loss_D: 1.1904 Loss_G: 0.5419\n",
      "BATCH NUMBER 1824\n",
      "Epoch [1/5] Loss_D: 1.2336 Loss_G: 0.5973\n",
      "BATCH NUMBER 1825\n",
      "Epoch [1/5] Loss_D: 1.1010 Loss_G: 0.6214\n",
      "BATCH NUMBER 1826\n",
      "Epoch [1/5] Loss_D: 1.0921 Loss_G: 0.6226\n",
      "BATCH NUMBER 1827\n",
      "Epoch [1/5] Loss_D: 1.2897 Loss_G: 0.5655\n",
      "BATCH NUMBER 1828\n",
      "Epoch [1/5] Loss_D: 1.2085 Loss_G: 0.5433\n",
      "BATCH NUMBER 1829\n",
      "Epoch [1/5] Loss_D: 1.2791 Loss_G: 0.5079\n",
      "BATCH NUMBER 1830\n",
      "Epoch [1/5] Loss_D: 1.1265 Loss_G: 0.6000\n",
      "BATCH NUMBER 1831\n",
      "Epoch [1/5] Loss_D: 1.0961 Loss_G: 0.6187\n",
      "BATCH NUMBER 1832\n",
      "Epoch [1/5] Loss_D: 1.0786 Loss_G: 0.6333\n",
      "BATCH NUMBER 1833\n",
      "Epoch [1/5] Loss_D: 1.1125 Loss_G: 0.6061\n",
      "BATCH NUMBER 1834\n",
      "Epoch [1/5] Loss_D: 1.1301 Loss_G: 0.5907\n",
      "BATCH NUMBER 1835\n",
      "Epoch [1/5] Loss_D: 1.3060 Loss_G: 0.5190\n",
      "BATCH NUMBER 1836\n",
      "Epoch [1/5] Loss_D: 1.2174 Loss_G: 0.5288\n",
      "BATCH NUMBER 1837\n",
      "Epoch [1/5] Loss_D: 1.2298 Loss_G: 0.5413\n",
      "BATCH NUMBER 1838\n",
      "Epoch [1/5] Loss_D: 1.4632 Loss_G: 0.6030\n",
      "BATCH NUMBER 1839\n",
      "Epoch [1/5] Loss_D: 1.1769 Loss_G: 0.5680\n",
      "BATCH NUMBER 1840\n",
      "Epoch [1/5] Loss_D: 1.2038 Loss_G: 0.5551\n",
      "BATCH NUMBER 1841\n",
      "Epoch [1/5] Loss_D: 1.1565 Loss_G: 0.5858\n",
      "BATCH NUMBER 1842\n",
      "Epoch [1/5] Loss_D: 1.3014 Loss_G: 0.4917\n",
      "BATCH NUMBER 1843\n",
      "Epoch [1/5] Loss_D: 1.0862 Loss_G: 0.6265\n",
      "BATCH NUMBER 1844\n",
      "Epoch [1/5] Loss_D: 1.1014 Loss_G: 0.6147\n",
      "BATCH NUMBER 1845\n",
      "Epoch [1/5] Loss_D: 1.1215 Loss_G: 0.6058\n",
      "BATCH NUMBER 1846\n",
      "Epoch [1/5] Loss_D: 1.4598 Loss_G: 0.4199\n",
      "BATCH NUMBER 1847\n",
      "Epoch [1/5] Loss_D: 1.1347 Loss_G: 0.5972\n",
      "BATCH NUMBER 1848\n",
      "Epoch [1/5] Loss_D: 1.2147 Loss_G: 0.6195\n",
      "BATCH NUMBER 1849\n",
      "Epoch [1/5] Loss_D: 1.1180 Loss_G: 0.6030\n",
      "BATCH NUMBER 1850\n",
      "Epoch [1/5] Loss_D: 1.0759 Loss_G: 0.6366\n",
      "BATCH NUMBER 1851\n",
      "Epoch [1/5] Loss_D: 1.1516 Loss_G: 0.5720\n",
      "BATCH NUMBER 1852\n",
      "Epoch [1/5] Loss_D: 1.0920 Loss_G: 0.6290\n",
      "BATCH NUMBER 1853\n",
      "Epoch [1/5] Loss_D: 1.1093 Loss_G: 0.6080\n",
      "BATCH NUMBER 1854\n",
      "Epoch [1/5] Loss_D: 1.0953 Loss_G: 0.6185\n",
      "BATCH NUMBER 1855\n",
      "Epoch [1/5] Loss_D: 1.1627 Loss_G: 0.5709\n",
      "BATCH NUMBER 1856\n",
      "Epoch [1/5] Loss_D: 1.1892 Loss_G: 0.5468\n",
      "BATCH NUMBER 1857\n",
      "Epoch [1/5] Loss_D: 1.1218 Loss_G: 0.6043\n",
      "BATCH NUMBER 1858\n",
      "Epoch [1/5] Loss_D: 1.1709 Loss_G: 0.5727\n",
      "BATCH NUMBER 1859\n",
      "Epoch [1/5] Loss_D: 1.1504 Loss_G: 0.5852\n",
      "BATCH NUMBER 1860\n",
      "Epoch [1/5] Loss_D: 1.1123 Loss_G: 0.6066\n",
      "BATCH NUMBER 1861\n",
      "Epoch [1/5] Loss_D: 1.2119 Loss_G: 0.5554\n",
      "BATCH NUMBER 1862\n",
      "Epoch [1/5] Loss_D: 1.1427 Loss_G: 0.6004\n",
      "BATCH NUMBER 1863\n",
      "Epoch [1/5] Loss_D: 1.0724 Loss_G: 0.6382\n",
      "BATCH NUMBER 1864\n",
      "Epoch [1/5] Loss_D: 1.1096 Loss_G: 0.6076\n",
      "BATCH NUMBER 1865\n",
      "Epoch [1/5] Loss_D: 1.2724 Loss_G: 0.4972\n",
      "BATCH NUMBER 1866\n",
      "Epoch [1/5] Loss_D: 1.1791 Loss_G: 0.5869\n",
      "BATCH NUMBER 1867\n",
      "Epoch [1/5] Loss_D: 1.2228 Loss_G: 0.5543\n",
      "BATCH NUMBER 1868\n",
      "Epoch [1/5] Loss_D: 1.1730 Loss_G: 0.5670\n",
      "BATCH NUMBER 1869\n",
      "Epoch [1/5] Loss_D: 1.3868 Loss_G: 0.4861\n",
      "BATCH NUMBER 1870\n",
      "Epoch [1/5] Loss_D: 1.1751 Loss_G: 0.5714\n",
      "BATCH NUMBER 1871\n",
      "Epoch [1/5] Loss_D: 1.1105 Loss_G: 0.6261\n",
      "BATCH NUMBER 1872\n",
      "Epoch [1/5] Loss_D: 1.2928 Loss_G: 0.5419\n",
      "BATCH NUMBER 1873\n",
      "Epoch [1/5] Loss_D: 1.1488 Loss_G: 0.5829\n",
      "BATCH NUMBER 1874\n",
      "Epoch [1/5] Loss_D: 1.2046 Loss_G: 0.5475\n",
      "BATCH NUMBER 1875\n",
      "Epoch [1/5] Loss_D: 1.3397 Loss_G: 0.4698\n",
      "BATCH NUMBER 1876\n",
      "Epoch [1/5] Loss_D: 1.1969 Loss_G: 0.5445\n",
      "BATCH NUMBER 1877\n",
      "Epoch [1/5] Loss_D: 1.3058 Loss_G: 0.4873\n",
      "BATCH NUMBER 1878\n",
      "Epoch [1/5] Loss_D: 1.5053 Loss_G: 0.5224\n",
      "BATCH NUMBER 1879\n",
      "Epoch [1/5] Loss_D: 1.1347 Loss_G: 0.5938\n",
      "BATCH NUMBER 1880\n",
      "Epoch [1/5] Loss_D: 1.0942 Loss_G: 0.6239\n",
      "BATCH NUMBER 1881\n",
      "Epoch [1/5] Loss_D: 1.1558 Loss_G: 0.5838\n",
      "BATCH NUMBER 1882\n",
      "Epoch [1/5] Loss_D: 1.2444 Loss_G: 0.6090\n",
      "BATCH NUMBER 1883\n",
      "Epoch [1/5] Loss_D: 1.1633 Loss_G: 0.5869\n",
      "BATCH NUMBER 1884\n",
      "Epoch [1/5] Loss_D: 1.1293 Loss_G: 0.5923\n",
      "BATCH NUMBER 1885\n",
      "Epoch [1/5] Loss_D: 1.1759 Loss_G: 0.5561\n",
      "BATCH NUMBER 1886\n",
      "Epoch [1/5] Loss_D: 1.1529 Loss_G: 0.5917\n",
      "BATCH NUMBER 1887\n",
      "Epoch [1/5] Loss_D: 1.1485 Loss_G: 0.5793\n",
      "BATCH NUMBER 1888\n",
      "Epoch [1/5] Loss_D: 1.1321 Loss_G: 0.5934\n",
      "BATCH NUMBER 1889\n",
      "Epoch [1/5] Loss_D: 1.5617 Loss_G: 0.5471\n",
      "BATCH NUMBER 1890\n",
      "Epoch [1/5] Loss_D: 1.2060 Loss_G: 0.5403\n",
      "BATCH NUMBER 1891\n",
      "Epoch [1/5] Loss_D: 1.1827 Loss_G: 0.5665\n",
      "BATCH NUMBER 1892\n",
      "Epoch [1/5] Loss_D: 1.1891 Loss_G: 0.5561\n",
      "BATCH NUMBER 1893\n",
      "Epoch [1/5] Loss_D: 1.2244 Loss_G: 0.5462\n",
      "BATCH NUMBER 1894\n",
      "Epoch [1/5] Loss_D: 1.1322 Loss_G: 0.5877\n",
      "BATCH NUMBER 1895\n",
      "Epoch [1/5] Loss_D: 1.4095 Loss_G: 0.4743\n",
      "BATCH NUMBER 1896\n",
      "Epoch [1/5] Loss_D: 1.2149 Loss_G: 0.5468\n",
      "BATCH NUMBER 1897\n",
      "Epoch [1/5] Loss_D: 1.2161 Loss_G: 0.5368\n",
      "BATCH NUMBER 1898\n",
      "Epoch [1/5] Loss_D: 1.3509 Loss_G: 0.5754\n",
      "BATCH NUMBER 1899\n",
      "Epoch [1/5] Loss_D: 1.1663 Loss_G: 0.5704\n",
      "BATCH NUMBER 1900\n",
      "Epoch [1/5] Loss_D: 1.1172 Loss_G: 0.6117\n",
      "BATCH NUMBER 1901\n",
      "Epoch [1/5] Loss_D: 1.2423 Loss_G: 0.5179\n",
      "BATCH NUMBER 1902\n",
      "Epoch [1/5] Loss_D: 1.1613 Loss_G: 0.5732\n",
      "BATCH NUMBER 1903\n",
      "Epoch [1/5] Loss_D: 1.1586 Loss_G: 0.5784\n",
      "BATCH NUMBER 1904\n",
      "Epoch [1/5] Loss_D: 1.4230 Loss_G: 0.4229\n",
      "BATCH NUMBER 1905\n",
      "Epoch [1/5] Loss_D: 1.1962 Loss_G: 0.5526\n",
      "BATCH NUMBER 1906\n",
      "Epoch [1/5] Loss_D: 1.1648 Loss_G: 0.6330\n",
      "BATCH NUMBER 1907\n",
      "Epoch [1/5] Loss_D: 1.2310 Loss_G: 0.5422\n",
      "BATCH NUMBER 1908\n",
      "Epoch [1/5] Loss_D: 1.2373 Loss_G: 0.5320\n",
      "BATCH NUMBER 1909\n",
      "Epoch [1/5] Loss_D: 1.1382 Loss_G: 0.5825\n",
      "BATCH NUMBER 1910\n",
      "Epoch [1/5] Loss_D: 1.2963 Loss_G: 0.5782\n",
      "BATCH NUMBER 1911\n",
      "Epoch [1/5] Loss_D: 1.0858 Loss_G: 0.6266\n",
      "BATCH NUMBER 1912\n",
      "Epoch [1/5] Loss_D: 1.1029 Loss_G: 0.6130\n",
      "BATCH NUMBER 1913\n",
      "Epoch [1/5] Loss_D: 1.2019 Loss_G: 0.5441\n",
      "BATCH NUMBER 1914\n",
      "Epoch [1/5] Loss_D: 1.1094 Loss_G: 0.6088\n",
      "BATCH NUMBER 1915\n",
      "Epoch [1/5] Loss_D: 1.0457 Loss_G: 0.6627\n",
      "BATCH NUMBER 1916\n",
      "Epoch [1/5] Loss_D: 1.1761 Loss_G: 0.6130\n",
      "BATCH NUMBER 1917\n",
      "Epoch [1/5] Loss_D: 1.2928 Loss_G: 0.4995\n",
      "BATCH NUMBER 1918\n",
      "Epoch [1/5] Loss_D: 1.2168 Loss_G: 0.5396\n",
      "BATCH NUMBER 1919\n",
      "Epoch [1/5] Loss_D: 1.1889 Loss_G: 0.5607\n",
      "BATCH NUMBER 1920\n",
      "Epoch [1/5] Loss_D: 1.4991 Loss_G: 0.6037\n",
      "BATCH NUMBER 1921\n",
      "Epoch [1/5] Loss_D: 1.2567 Loss_G: 0.5224\n",
      "BATCH NUMBER 1922\n",
      "Epoch [1/5] Loss_D: 1.1292 Loss_G: 0.6176\n",
      "BATCH NUMBER 1923\n",
      "Epoch [1/5] Loss_D: 1.0824 Loss_G: 0.6298\n",
      "BATCH NUMBER 1924\n",
      "Epoch [1/5] Loss_D: 1.1641 Loss_G: 0.5838\n",
      "BATCH NUMBER 1925\n",
      "Epoch [1/5] Loss_D: 1.0977 Loss_G: 0.6167\n",
      "BATCH NUMBER 1926\n",
      "Epoch [1/5] Loss_D: 1.2042 Loss_G: 0.6054\n",
      "BATCH NUMBER 1927\n",
      "Epoch [1/5] Loss_D: 1.1618 Loss_G: 0.5741\n",
      "BATCH NUMBER 1928\n",
      "Epoch [1/5] Loss_D: 1.0785 Loss_G: 0.6324\n",
      "BATCH NUMBER 1929\n",
      "Epoch [1/5] Loss_D: 1.0656 Loss_G: 0.6452\n",
      "BATCH NUMBER 1930\n",
      "Epoch [1/5] Loss_D: 1.1121 Loss_G: 0.6284\n",
      "BATCH NUMBER 1931\n",
      "Epoch [1/5] Loss_D: 1.0620 Loss_G: 0.6462\n",
      "BATCH NUMBER 1932\n",
      "Epoch [1/5] Loss_D: 1.1196 Loss_G: 0.6070\n",
      "BATCH NUMBER 1933\n",
      "Epoch [1/5] Loss_D: 1.1204 Loss_G: 0.6066\n",
      "BATCH NUMBER 1934\n",
      "Epoch [1/5] Loss_D: 1.1212 Loss_G: 0.6061\n",
      "BATCH NUMBER 1935\n",
      "Epoch [1/5] Loss_D: 1.2105 Loss_G: 0.5452\n",
      "BATCH NUMBER 1936\n",
      "Epoch [1/5] Loss_D: 1.1778 Loss_G: 0.5717\n",
      "BATCH NUMBER 1937\n",
      "Epoch [1/5] Loss_D: 1.2004 Loss_G: 0.5421\n",
      "BATCH NUMBER 1938\n",
      "Epoch [1/5] Loss_D: 1.1539 Loss_G: 0.5882\n",
      "BATCH NUMBER 1939\n",
      "Epoch [1/5] Loss_D: 1.1123 Loss_G: 0.6142\n",
      "BATCH NUMBER 1940\n",
      "Epoch [1/5] Loss_D: 1.3354 Loss_G: 0.4787\n",
      "BATCH NUMBER 1941\n",
      "Epoch [1/5] Loss_D: 1.1275 Loss_G: 0.5955\n",
      "BATCH NUMBER 1942\n",
      "Epoch [1/5] Loss_D: 1.1973 Loss_G: 0.5581\n",
      "BATCH NUMBER 1943\n",
      "Epoch [1/5] Loss_D: 1.2155 Loss_G: 0.5544\n",
      "BATCH NUMBER 1944\n",
      "Epoch [1/5] Loss_D: 1.1881 Loss_G: 0.5496\n",
      "BATCH NUMBER 1945\n",
      "Epoch [1/5] Loss_D: 1.3626 Loss_G: 0.5416\n",
      "BATCH NUMBER 1946\n",
      "Epoch [1/5] Loss_D: 1.1917 Loss_G: 0.5459\n",
      "BATCH NUMBER 1947\n",
      "Epoch [1/5] Loss_D: 1.3694 Loss_G: 0.5627\n",
      "BATCH NUMBER 1948\n",
      "Epoch [1/5] Loss_D: 1.1126 Loss_G: 0.6053\n",
      "BATCH NUMBER 1949\n",
      "Epoch [1/5] Loss_D: 1.1441 Loss_G: 0.5823\n",
      "BATCH NUMBER 1950\n",
      "Epoch [1/5] Loss_D: 1.1133 Loss_G: 0.6082\n",
      "BATCH NUMBER 1951\n",
      "Epoch [1/5] Loss_D: 1.0895 Loss_G: 0.6199\n",
      "BATCH NUMBER 1952\n",
      "Epoch [1/5] Loss_D: 1.2074 Loss_G: 0.5515\n",
      "BATCH NUMBER 1953\n",
      "Epoch [1/5] Loss_D: 1.1283 Loss_G: 0.5934\n",
      "BATCH NUMBER 1954\n",
      "Epoch [1/5] Loss_D: 1.1524 Loss_G: 0.5760\n",
      "BATCH NUMBER 1955\n",
      "Epoch [1/5] Loss_D: 1.2475 Loss_G: 0.5168\n",
      "BATCH NUMBER 1956\n",
      "Epoch [1/5] Loss_D: 1.1944 Loss_G: 0.5642\n",
      "BATCH NUMBER 1957\n",
      "Epoch [1/5] Loss_D: 1.1527 Loss_G: 0.5790\n",
      "BATCH NUMBER 1958\n",
      "Epoch [1/5] Loss_D: 1.1056 Loss_G: 0.6153\n",
      "BATCH NUMBER 1959\n",
      "Epoch [1/5] Loss_D: 1.2892 Loss_G: 0.5012\n",
      "BATCH NUMBER 1960\n",
      "Epoch [1/5] Loss_D: 1.2723 Loss_G: 0.5072\n",
      "BATCH NUMBER 1961\n",
      "Epoch [1/5] Loss_D: 1.1498 Loss_G: 0.5828\n",
      "BATCH NUMBER 1962\n",
      "Epoch [1/5] Loss_D: 1.2657 Loss_G: 0.5078\n",
      "BATCH NUMBER 1963\n",
      "Epoch [1/5] Loss_D: 1.2043 Loss_G: 0.5548\n",
      "BATCH NUMBER 1964\n",
      "Epoch [1/5] Loss_D: 1.4375 Loss_G: 0.5651\n",
      "BATCH NUMBER 1965\n",
      "Epoch [1/5] Loss_D: 1.1407 Loss_G: 0.5830\n",
      "BATCH NUMBER 1966\n",
      "Epoch [1/5] Loss_D: 1.2859 Loss_G: 0.4984\n",
      "BATCH NUMBER 1967\n",
      "Epoch [1/5] Loss_D: 1.4248 Loss_G: 0.4216\n",
      "BATCH NUMBER 1968\n",
      "Epoch [1/5] Loss_D: 1.1136 Loss_G: 0.6065\n",
      "BATCH NUMBER 1969\n",
      "Epoch [1/5] Loss_D: 1.1070 Loss_G: 0.6194\n",
      "BATCH NUMBER 1970\n",
      "Epoch [1/5] Loss_D: 1.1049 Loss_G: 0.6119\n",
      "BATCH NUMBER 1971\n",
      "Epoch [1/5] Loss_D: 1.2243 Loss_G: 0.5377\n",
      "BATCH NUMBER 1972\n",
      "Epoch [1/5] Loss_D: 1.1266 Loss_G: 0.5954\n",
      "BATCH NUMBER 1973\n",
      "Epoch [1/5] Loss_D: 1.2796 Loss_G: 0.5603\n",
      "BATCH NUMBER 1974\n",
      "Epoch [1/5] Loss_D: 1.2442 Loss_G: 0.5378\n",
      "BATCH NUMBER 1975\n",
      "Epoch [1/5] Loss_D: 1.0832 Loss_G: 0.6329\n",
      "BATCH NUMBER 1976\n",
      "Epoch [1/5] Loss_D: 1.1776 Loss_G: 0.5571\n",
      "BATCH NUMBER 1977\n",
      "Epoch [1/5] Loss_D: 1.2593 Loss_G: 0.5136\n",
      "BATCH NUMBER 1978\n",
      "Epoch [1/5] Loss_D: 1.1917 Loss_G: 0.5611\n",
      "BATCH NUMBER 1979\n",
      "Epoch [1/5] Loss_D: 1.1370 Loss_G: 0.5950\n",
      "BATCH NUMBER 1980\n",
      "Epoch [1/5] Loss_D: 1.4077 Loss_G: 0.4365\n",
      "BATCH NUMBER 1981\n",
      "Epoch [1/5] Loss_D: 1.1519 Loss_G: 0.5896\n",
      "BATCH NUMBER 1982\n",
      "Epoch [1/5] Loss_D: 1.0595 Loss_G: 0.6483\n",
      "BATCH NUMBER 1983\n",
      "Epoch [1/5] Loss_D: 1.1779 Loss_G: 0.5871\n",
      "BATCH NUMBER 1984\n",
      "Epoch [1/5] Loss_D: 1.1898 Loss_G: 0.5509\n",
      "BATCH NUMBER 1985\n",
      "Epoch [1/5] Loss_D: 1.3333 Loss_G: 0.5933\n",
      "BATCH NUMBER 1986\n",
      "Epoch [1/5] Loss_D: 1.0999 Loss_G: 0.6279\n",
      "BATCH NUMBER 1987\n",
      "Epoch [1/5] Loss_D: 1.1186 Loss_G: 0.6108\n",
      "BATCH NUMBER 1988\n",
      "Epoch [1/5] Loss_D: 1.3032 Loss_G: 0.5781\n",
      "BATCH NUMBER 1989\n",
      "Epoch [1/5] Loss_D: 1.1657 Loss_G: 0.6169\n",
      "BATCH NUMBER 1990\n",
      "Epoch [1/5] Loss_D: 1.2797 Loss_G: 0.5034\n",
      "BATCH NUMBER 1991\n",
      "Epoch [1/5] Loss_D: 1.1233 Loss_G: 0.5981\n",
      "BATCH NUMBER 1992\n",
      "Epoch [1/5] Loss_D: 1.0667 Loss_G: 0.6417\n",
      "BATCH NUMBER 1993\n",
      "Epoch [1/5] Loss_D: 1.0593 Loss_G: 0.6470\n",
      "BATCH NUMBER 1994\n",
      "Epoch [1/5] Loss_D: 1.1335 Loss_G: 0.5907\n",
      "BATCH NUMBER 1995\n",
      "Epoch [1/5] Loss_D: 1.1575 Loss_G: 0.5893\n",
      "BATCH NUMBER 1996\n",
      "Epoch [1/5] Loss_D: 1.1973 Loss_G: 0.5553\n",
      "BATCH NUMBER 1997\n",
      "Epoch [1/5] Loss_D: 1.1483 Loss_G: 0.5838\n",
      "BATCH NUMBER 1998\n",
      "Epoch [1/5] Loss_D: 1.4151 Loss_G: 0.4946\n",
      "BATCH NUMBER 1999\n",
      "Epoch [1/5] Loss_D: 1.1337 Loss_G: 0.5971\n",
      "BATCH NUMBER 2000\n",
      "Epoch [2/5] Loss_D: 1.1858 Loss_G: 0.6020\n",
      "BATCH NUMBER 2001\n",
      "Epoch [2/5] Loss_D: 1.3043 Loss_G: 0.5553\n",
      "BATCH NUMBER 2002\n",
      "Epoch [2/5] Loss_D: 1.1023 Loss_G: 0.6112\n",
      "BATCH NUMBER 2003\n",
      "Epoch [2/5] Loss_D: 1.1345 Loss_G: 0.5884\n",
      "BATCH NUMBER 2004\n",
      "Epoch [2/5] Loss_D: 1.1902 Loss_G: 0.6216\n",
      "BATCH NUMBER 2005\n",
      "Epoch [2/5] Loss_D: 1.2686 Loss_G: 0.5713\n",
      "BATCH NUMBER 2006\n",
      "Epoch [2/5] Loss_D: 1.1573 Loss_G: 0.5987\n",
      "BATCH NUMBER 2007\n",
      "Epoch [2/5] Loss_D: 1.3468 Loss_G: 0.4788\n",
      "BATCH NUMBER 2008\n",
      "Epoch [2/5] Loss_D: 1.1897 Loss_G: 0.5674\n",
      "BATCH NUMBER 2009\n",
      "Epoch [2/5] Loss_D: 1.2167 Loss_G: 0.5330\n",
      "BATCH NUMBER 2010\n",
      "Epoch [2/5] Loss_D: 1.0502 Loss_G: 0.6569\n",
      "BATCH NUMBER 2011\n",
      "Epoch [2/5] Loss_D: 1.1499 Loss_G: 0.5859\n",
      "BATCH NUMBER 2012\n",
      "Epoch [2/5] Loss_D: 1.1801 Loss_G: 0.5572\n",
      "BATCH NUMBER 2013\n",
      "Epoch [2/5] Loss_D: 1.1929 Loss_G: 0.6178\n",
      "BATCH NUMBER 2014\n",
      "Epoch [2/5] Loss_D: 1.1446 Loss_G: 0.5870\n",
      "BATCH NUMBER 2015\n",
      "Epoch [2/5] Loss_D: 1.0937 Loss_G: 0.6217\n",
      "BATCH NUMBER 2016\n",
      "Epoch [2/5] Loss_D: 1.0697 Loss_G: 0.6397\n",
      "BATCH NUMBER 2017\n",
      "Epoch [2/5] Loss_D: 1.0950 Loss_G: 0.6302\n",
      "BATCH NUMBER 2018\n",
      "Epoch [2/5] Loss_D: 1.1658 Loss_G: 0.5770\n",
      "BATCH NUMBER 2019\n",
      "Epoch [2/5] Loss_D: 1.3034 Loss_G: 0.4902\n",
      "BATCH NUMBER 2020\n",
      "Epoch [2/5] Loss_D: 1.2138 Loss_G: 0.5819\n",
      "BATCH NUMBER 2021\n",
      "Epoch [2/5] Loss_D: 1.2302 Loss_G: 0.5354\n",
      "BATCH NUMBER 2022\n",
      "Epoch [2/5] Loss_D: 1.2146 Loss_G: 0.5443\n",
      "BATCH NUMBER 2023\n",
      "Epoch [2/5] Loss_D: 1.1766 Loss_G: 0.5679\n",
      "BATCH NUMBER 2024\n",
      "Epoch [2/5] Loss_D: 1.2716 Loss_G: 0.5119\n",
      "BATCH NUMBER 2025\n",
      "Epoch [2/5] Loss_D: 1.2890 Loss_G: 0.4931\n",
      "BATCH NUMBER 2026\n",
      "Epoch [2/5] Loss_D: 1.2291 Loss_G: 0.5304\n",
      "BATCH NUMBER 2027\n",
      "Epoch [2/5] Loss_D: 1.0856 Loss_G: 0.6276\n",
      "BATCH NUMBER 2028\n",
      "Epoch [2/5] Loss_D: 1.2909 Loss_G: 0.4947\n",
      "BATCH NUMBER 2029\n",
      "Epoch [2/5] Loss_D: 1.1214 Loss_G: 0.6079\n",
      "BATCH NUMBER 2030\n",
      "Epoch [2/5] Loss_D: 1.1285 Loss_G: 0.5981\n",
      "BATCH NUMBER 2031\n",
      "Epoch [2/5] Loss_D: 1.0647 Loss_G: 0.6423\n",
      "BATCH NUMBER 2032\n",
      "Epoch [2/5] Loss_D: 1.4085 Loss_G: 0.6032\n",
      "BATCH NUMBER 2033\n",
      "Epoch [2/5] Loss_D: 1.2148 Loss_G: 0.5451\n",
      "BATCH NUMBER 2034\n",
      "Epoch [2/5] Loss_D: 1.2327 Loss_G: 0.5388\n",
      "BATCH NUMBER 2035\n",
      "Epoch [2/5] Loss_D: 1.0972 Loss_G: 0.6244\n",
      "BATCH NUMBER 2036\n",
      "Epoch [2/5] Loss_D: 1.1566 Loss_G: 0.5812\n",
      "BATCH NUMBER 2037\n",
      "Epoch [2/5] Loss_D: 1.1574 Loss_G: 0.5951\n",
      "BATCH NUMBER 2038\n",
      "Epoch [2/5] Loss_D: 1.1679 Loss_G: 0.5667\n",
      "BATCH NUMBER 2039\n",
      "Epoch [2/5] Loss_D: 1.0870 Loss_G: 0.6241\n",
      "BATCH NUMBER 2040\n",
      "Epoch [2/5] Loss_D: 1.0975 Loss_G: 0.6164\n",
      "BATCH NUMBER 2041\n",
      "Epoch [2/5] Loss_D: 1.1063 Loss_G: 0.6411\n",
      "BATCH NUMBER 2042\n",
      "Epoch [2/5] Loss_D: 1.2166 Loss_G: 0.5434\n",
      "BATCH NUMBER 2043\n",
      "Epoch [2/5] Loss_D: 1.1705 Loss_G: 0.5643\n",
      "BATCH NUMBER 2044\n",
      "Epoch [2/5] Loss_D: 1.1103 Loss_G: 0.6077\n",
      "BATCH NUMBER 2045\n",
      "Epoch [2/5] Loss_D: 1.1276 Loss_G: 0.5931\n",
      "BATCH NUMBER 2046\n",
      "Epoch [2/5] Loss_D: 1.1642 Loss_G: 0.6004\n",
      "BATCH NUMBER 2047\n",
      "Epoch [2/5] Loss_D: 1.1806 Loss_G: 0.5909\n",
      "BATCH NUMBER 2048\n",
      "Epoch [2/5] Loss_D: 1.1709 Loss_G: 0.5762\n",
      "BATCH NUMBER 2049\n",
      "Epoch [2/5] Loss_D: 1.1684 Loss_G: 0.5834\n",
      "BATCH NUMBER 2050\n",
      "Epoch [2/5] Loss_D: 1.1888 Loss_G: 0.5559\n",
      "BATCH NUMBER 2051\n",
      "Epoch [2/5] Loss_D: 1.0928 Loss_G: 0.6229\n",
      "BATCH NUMBER 2052\n",
      "Epoch [2/5] Loss_D: 1.2425 Loss_G: 0.5995\n",
      "BATCH NUMBER 2053\n",
      "Epoch [2/5] Loss_D: 1.1108 Loss_G: 0.6225\n",
      "BATCH NUMBER 2054\n",
      "Epoch [2/5] Loss_D: 1.1178 Loss_G: 0.6101\n",
      "BATCH NUMBER 2055\n",
      "Epoch [2/5] Loss_D: 1.3609 Loss_G: 0.4658\n",
      "BATCH NUMBER 2056\n",
      "Epoch [2/5] Loss_D: 1.1937 Loss_G: 0.5475\n",
      "BATCH NUMBER 2057\n",
      "Epoch [2/5] Loss_D: 1.1715 Loss_G: 0.5721\n",
      "BATCH NUMBER 2058\n",
      "Epoch [2/5] Loss_D: 1.2241 Loss_G: 0.5489\n",
      "BATCH NUMBER 2059\n",
      "Epoch [2/5] Loss_D: 1.1767 Loss_G: 0.5812\n",
      "BATCH NUMBER 2060\n",
      "Epoch [2/5] Loss_D: 1.1556 Loss_G: 0.5802\n",
      "BATCH NUMBER 2061\n",
      "Epoch [2/5] Loss_D: 1.2961 Loss_G: 0.4984\n",
      "BATCH NUMBER 2062\n",
      "Epoch [2/5] Loss_D: 1.1063 Loss_G: 0.6254\n",
      "BATCH NUMBER 2063\n",
      "Epoch [2/5] Loss_D: 1.2151 Loss_G: 0.5354\n",
      "BATCH NUMBER 2064\n",
      "Epoch [2/5] Loss_D: 1.2092 Loss_G: 0.5498\n",
      "BATCH NUMBER 2065\n",
      "Epoch [2/5] Loss_D: 1.1504 Loss_G: 0.5916\n",
      "BATCH NUMBER 2066\n",
      "Epoch [2/5] Loss_D: 1.1999 Loss_G: 0.5890\n",
      "BATCH NUMBER 2067\n",
      "Epoch [2/5] Loss_D: 1.0967 Loss_G: 0.6150\n",
      "BATCH NUMBER 2068\n",
      "Epoch [2/5] Loss_D: 1.0900 Loss_G: 0.6253\n",
      "BATCH NUMBER 2069\n",
      "Epoch [2/5] Loss_D: 1.1186 Loss_G: 0.5999\n",
      "BATCH NUMBER 2070\n",
      "Epoch [2/5] Loss_D: 1.1247 Loss_G: 0.5964\n",
      "BATCH NUMBER 2071\n",
      "Epoch [2/5] Loss_D: 1.0780 Loss_G: 0.6358\n",
      "BATCH NUMBER 2072\n",
      "Epoch [2/5] Loss_D: 1.1638 Loss_G: 0.5730\n",
      "BATCH NUMBER 2073\n",
      "Epoch [2/5] Loss_D: 1.1535 Loss_G: 0.5867\n",
      "BATCH NUMBER 2074\n",
      "Epoch [2/5] Loss_D: 1.2284 Loss_G: 0.5383\n",
      "BATCH NUMBER 2075\n",
      "Epoch [2/5] Loss_D: 1.3450 Loss_G: 0.4627\n",
      "BATCH NUMBER 2076\n",
      "Epoch [2/5] Loss_D: 1.1369 Loss_G: 0.5946\n",
      "BATCH NUMBER 2077\n",
      "Epoch [2/5] Loss_D: 1.2929 Loss_G: 0.5068\n",
      "BATCH NUMBER 2078\n",
      "Epoch [2/5] Loss_D: 1.1394 Loss_G: 0.5993\n",
      "BATCH NUMBER 2079\n",
      "Epoch [2/5] Loss_D: 1.2374 Loss_G: 0.5268\n",
      "BATCH NUMBER 2080\n",
      "Epoch [2/5] Loss_D: 1.0484 Loss_G: 0.6574\n",
      "BATCH NUMBER 2081\n",
      "Epoch [2/5] Loss_D: 1.2587 Loss_G: 0.5257\n",
      "BATCH NUMBER 2082\n",
      "Epoch [2/5] Loss_D: 1.2528 Loss_G: 0.5116\n",
      "BATCH NUMBER 2083\n",
      "Epoch [2/5] Loss_D: 1.0920 Loss_G: 0.6218\n",
      "BATCH NUMBER 2084\n",
      "Epoch [2/5] Loss_D: 1.1278 Loss_G: 0.5954\n",
      "BATCH NUMBER 2085\n",
      "Epoch [2/5] Loss_D: 1.1953 Loss_G: 0.5438\n",
      "BATCH NUMBER 2086\n",
      "Epoch [2/5] Loss_D: 1.2696 Loss_G: 0.5157\n",
      "BATCH NUMBER 2087\n",
      "Epoch [2/5] Loss_D: 1.1053 Loss_G: 0.6126\n",
      "BATCH NUMBER 2088\n",
      "Epoch [2/5] Loss_D: 1.0986 Loss_G: 0.6167\n",
      "BATCH NUMBER 2089\n",
      "Epoch [2/5] Loss_D: 1.2375 Loss_G: 0.5356\n",
      "BATCH NUMBER 2090\n",
      "Epoch [2/5] Loss_D: 1.0822 Loss_G: 0.6296\n",
      "BATCH NUMBER 2091\n",
      "Epoch [2/5] Loss_D: 1.2370 Loss_G: 0.5341\n",
      "BATCH NUMBER 2092\n",
      "Epoch [2/5] Loss_D: 1.2268 Loss_G: 0.5238\n",
      "BATCH NUMBER 2093\n",
      "Epoch [2/5] Loss_D: 1.3192 Loss_G: 0.4845\n",
      "BATCH NUMBER 2094\n",
      "Epoch [2/5] Loss_D: 1.1248 Loss_G: 0.6024\n",
      "BATCH NUMBER 2095\n",
      "Epoch [2/5] Loss_D: 1.1474 Loss_G: 0.6073\n",
      "BATCH NUMBER 2096\n",
      "Epoch [2/5] Loss_D: 1.3011 Loss_G: 0.5489\n",
      "BATCH NUMBER 2097\n",
      "Epoch [2/5] Loss_D: 1.0851 Loss_G: 0.6327\n",
      "BATCH NUMBER 2098\n",
      "Epoch [2/5] Loss_D: 1.1557 Loss_G: 0.5809\n",
      "BATCH NUMBER 2099\n",
      "Epoch [2/5] Loss_D: 1.5802 Loss_G: 0.5311\n",
      "BATCH NUMBER 2100\n",
      "Epoch [2/5] Loss_D: 1.1411 Loss_G: 0.5982\n",
      "BATCH NUMBER 2101\n",
      "Epoch [2/5] Loss_D: 1.2109 Loss_G: 0.5540\n",
      "BATCH NUMBER 2102\n",
      "Epoch [2/5] Loss_D: 1.1255 Loss_G: 0.6290\n",
      "BATCH NUMBER 2103\n",
      "Epoch [2/5] Loss_D: 1.1077 Loss_G: 0.6176\n",
      "BATCH NUMBER 2104\n",
      "Epoch [2/5] Loss_D: 1.2405 Loss_G: 0.5324\n",
      "BATCH NUMBER 2105\n",
      "Epoch [2/5] Loss_D: 1.1699 Loss_G: 0.5723\n",
      "BATCH NUMBER 2106\n",
      "Epoch [2/5] Loss_D: 1.0798 Loss_G: 0.6298\n",
      "BATCH NUMBER 2107\n",
      "Epoch [2/5] Loss_D: 1.1193 Loss_G: 0.5999\n",
      "BATCH NUMBER 2108\n",
      "Epoch [2/5] Loss_D: 1.3309 Loss_G: 0.4805\n",
      "BATCH NUMBER 2109\n",
      "Epoch [2/5] Loss_D: 1.1534 Loss_G: 0.5992\n",
      "BATCH NUMBER 2110\n",
      "Epoch [2/5] Loss_D: 1.1537 Loss_G: 0.5879\n",
      "BATCH NUMBER 2111\n",
      "Epoch [2/5] Loss_D: 1.1473 Loss_G: 0.6236\n",
      "BATCH NUMBER 2112\n",
      "Epoch [2/5] Loss_D: 1.0807 Loss_G: 0.6302\n",
      "BATCH NUMBER 2113\n",
      "Epoch [2/5] Loss_D: 1.2139 Loss_G: 0.5557\n",
      "BATCH NUMBER 2114\n",
      "Epoch [2/5] Loss_D: 1.3798 Loss_G: 0.4529\n",
      "BATCH NUMBER 2115\n",
      "Epoch [2/5] Loss_D: 1.3063 Loss_G: 0.4871\n",
      "BATCH NUMBER 2116\n",
      "Epoch [2/5] Loss_D: 1.4283 Loss_G: 0.6463\n",
      "BATCH NUMBER 2117\n",
      "Epoch [2/5] Loss_D: 1.3249 Loss_G: 0.4836\n",
      "BATCH NUMBER 2118\n",
      "Epoch [2/5] Loss_D: 1.0776 Loss_G: 0.6344\n",
      "BATCH NUMBER 2119\n",
      "Epoch [2/5] Loss_D: 1.1998 Loss_G: 0.5584\n",
      "BATCH NUMBER 2120\n",
      "Epoch [2/5] Loss_D: 1.0825 Loss_G: 0.6303\n",
      "BATCH NUMBER 2121\n",
      "Epoch [2/5] Loss_D: 1.2255 Loss_G: 0.5376\n",
      "BATCH NUMBER 2122\n",
      "Epoch [2/5] Loss_D: 1.2254 Loss_G: 0.5403\n",
      "BATCH NUMBER 2123\n",
      "Epoch [2/5] Loss_D: 1.1035 Loss_G: 0.6112\n",
      "BATCH NUMBER 2124\n",
      "Epoch [2/5] Loss_D: 1.1041 Loss_G: 0.6153\n",
      "BATCH NUMBER 2125\n",
      "Epoch [2/5] Loss_D: 1.0882 Loss_G: 0.6251\n",
      "BATCH NUMBER 2126\n",
      "Epoch [2/5] Loss_D: 1.1223 Loss_G: 0.5954\n",
      "BATCH NUMBER 2127\n",
      "Epoch [2/5] Loss_D: 1.1778 Loss_G: 0.6005\n",
      "BATCH NUMBER 2128\n",
      "Epoch [2/5] Loss_D: 1.1942 Loss_G: 0.5528\n",
      "BATCH NUMBER 2129\n",
      "Epoch [2/5] Loss_D: 1.1249 Loss_G: 0.6092\n",
      "BATCH NUMBER 2130\n",
      "Epoch [2/5] Loss_D: 1.0635 Loss_G: 0.6469\n",
      "BATCH NUMBER 2131\n",
      "Epoch [2/5] Loss_D: 1.1869 Loss_G: 0.5643\n",
      "BATCH NUMBER 2132\n",
      "Epoch [2/5] Loss_D: 1.1033 Loss_G: 0.6209\n",
      "BATCH NUMBER 2133\n",
      "Epoch [2/5] Loss_D: 1.2707 Loss_G: 0.5721\n",
      "BATCH NUMBER 2134\n",
      "Epoch [2/5] Loss_D: 1.1696 Loss_G: 0.5708\n",
      "BATCH NUMBER 2135\n",
      "Epoch [2/5] Loss_D: 1.3343 Loss_G: 0.5034\n",
      "BATCH NUMBER 2136\n",
      "Epoch [2/5] Loss_D: 1.1500 Loss_G: 0.5761\n",
      "BATCH NUMBER 2137\n",
      "Epoch [2/5] Loss_D: 1.1719 Loss_G: 0.5609\n",
      "BATCH NUMBER 2138\n",
      "Epoch [2/5] Loss_D: 1.2206 Loss_G: 0.5406\n",
      "BATCH NUMBER 2139\n",
      "Epoch [2/5] Loss_D: 1.3739 Loss_G: 0.4497\n",
      "BATCH NUMBER 2140\n",
      "Epoch [2/5] Loss_D: 1.1296 Loss_G: 0.6087\n",
      "BATCH NUMBER 2141\n",
      "Epoch [2/5] Loss_D: 1.1690 Loss_G: 0.5817\n",
      "BATCH NUMBER 2142\n",
      "Epoch [2/5] Loss_D: 1.1875 Loss_G: 0.5770\n",
      "BATCH NUMBER 2143\n",
      "Epoch [2/5] Loss_D: 1.1181 Loss_G: 0.6267\n",
      "BATCH NUMBER 2144\n",
      "Epoch [2/5] Loss_D: 1.1778 Loss_G: 0.5698\n",
      "BATCH NUMBER 2145\n",
      "Epoch [2/5] Loss_D: 1.1174 Loss_G: 0.6100\n",
      "BATCH NUMBER 2146\n",
      "Epoch [2/5] Loss_D: 1.1026 Loss_G: 0.6162\n",
      "BATCH NUMBER 2147\n",
      "Epoch [2/5] Loss_D: 1.1211 Loss_G: 0.6090\n",
      "BATCH NUMBER 2148\n",
      "Epoch [2/5] Loss_D: 1.1121 Loss_G: 0.6139\n",
      "BATCH NUMBER 2149\n",
      "Epoch [2/5] Loss_D: 1.1317 Loss_G: 0.6039\n",
      "BATCH NUMBER 2150\n",
      "Epoch [2/5] Loss_D: 1.1845 Loss_G: 0.5785\n",
      "BATCH NUMBER 2151\n",
      "Epoch [2/5] Loss_D: 1.1568 Loss_G: 0.5768\n",
      "BATCH NUMBER 2152\n",
      "Epoch [2/5] Loss_D: 1.0919 Loss_G: 0.6209\n",
      "BATCH NUMBER 2153\n",
      "Epoch [2/5] Loss_D: 1.1606 Loss_G: 0.5735\n",
      "BATCH NUMBER 2154\n",
      "Epoch [2/5] Loss_D: 1.0797 Loss_G: 0.6315\n",
      "BATCH NUMBER 2155\n",
      "Epoch [2/5] Loss_D: 1.3407 Loss_G: 0.4677\n",
      "BATCH NUMBER 2156\n",
      "Epoch [2/5] Loss_D: 1.1490 Loss_G: 0.5803\n",
      "BATCH NUMBER 2157\n",
      "Epoch [2/5] Loss_D: 1.1281 Loss_G: 0.6046\n",
      "BATCH NUMBER 2158\n",
      "Epoch [2/5] Loss_D: 1.3790 Loss_G: 0.4800\n",
      "BATCH NUMBER 2159\n",
      "Epoch [2/5] Loss_D: 1.1078 Loss_G: 0.6314\n",
      "BATCH NUMBER 2160\n",
      "Epoch [2/5] Loss_D: 1.2369 Loss_G: 0.5357\n",
      "BATCH NUMBER 2161\n",
      "Epoch [2/5] Loss_D: 1.1061 Loss_G: 0.6299\n",
      "BATCH NUMBER 2162\n",
      "Epoch [2/5] Loss_D: 1.3291 Loss_G: 0.6204\n",
      "BATCH NUMBER 2163\n",
      "Epoch [2/5] Loss_D: 1.4597 Loss_G: 0.4218\n",
      "BATCH NUMBER 2164\n",
      "Epoch [2/5] Loss_D: 1.2456 Loss_G: 0.5266\n",
      "BATCH NUMBER 2165\n",
      "Epoch [2/5] Loss_D: 1.2064 Loss_G: 0.5623\n",
      "BATCH NUMBER 2166\n",
      "Epoch [2/5] Loss_D: 1.1001 Loss_G: 0.6234\n",
      "BATCH NUMBER 2167\n",
      "Epoch [2/5] Loss_D: 1.1622 Loss_G: 0.5791\n",
      "BATCH NUMBER 2168\n",
      "Epoch [2/5] Loss_D: 1.0613 Loss_G: 0.6451\n",
      "BATCH NUMBER 2169\n",
      "Epoch [2/5] Loss_D: 1.1651 Loss_G: 0.5767\n",
      "BATCH NUMBER 2170\n",
      "Epoch [2/5] Loss_D: 1.0976 Loss_G: 0.6252\n",
      "BATCH NUMBER 2171\n",
      "Epoch [2/5] Loss_D: 1.1113 Loss_G: 0.6054\n",
      "BATCH NUMBER 2172\n",
      "Epoch [2/5] Loss_D: 1.1075 Loss_G: 0.6153\n",
      "BATCH NUMBER 2173\n",
      "Epoch [2/5] Loss_D: 1.0734 Loss_G: 0.6451\n",
      "BATCH NUMBER 2174\n",
      "Epoch [2/5] Loss_D: 1.1060 Loss_G: 0.6091\n",
      "BATCH NUMBER 2175\n",
      "Epoch [2/5] Loss_D: 1.1117 Loss_G: 0.6150\n",
      "BATCH NUMBER 2176\n",
      "Epoch [2/5] Loss_D: 1.1741 Loss_G: 0.5696\n",
      "BATCH NUMBER 2177\n",
      "Epoch [2/5] Loss_D: 1.1099 Loss_G: 0.6238\n",
      "BATCH NUMBER 2178\n",
      "Epoch [2/5] Loss_D: 1.2052 Loss_G: 0.5524\n",
      "BATCH NUMBER 2179\n",
      "Epoch [2/5] Loss_D: 1.0592 Loss_G: 0.6479\n",
      "BATCH NUMBER 2180\n",
      "Epoch [2/5] Loss_D: 1.2230 Loss_G: 0.5729\n",
      "BATCH NUMBER 2181\n",
      "Epoch [2/5] Loss_D: 1.4494 Loss_G: 0.5840\n",
      "BATCH NUMBER 2182\n",
      "Epoch [2/5] Loss_D: 1.1973 Loss_G: 0.5808\n",
      "BATCH NUMBER 2183\n",
      "Epoch [2/5] Loss_D: 1.1025 Loss_G: 0.6194\n",
      "BATCH NUMBER 2184\n",
      "Epoch [2/5] Loss_D: 1.2457 Loss_G: 0.5130\n",
      "BATCH NUMBER 2185\n",
      "Epoch [2/5] Loss_D: 1.1940 Loss_G: 0.5551\n",
      "BATCH NUMBER 2186\n",
      "Epoch [2/5] Loss_D: 1.3186 Loss_G: 0.4877\n",
      "BATCH NUMBER 2187\n",
      "Epoch [2/5] Loss_D: 1.1182 Loss_G: 0.5974\n",
      "BATCH NUMBER 2188\n",
      "Epoch [2/5] Loss_D: 1.0547 Loss_G: 0.6530\n",
      "BATCH NUMBER 2189\n",
      "Epoch [2/5] Loss_D: 1.1461 Loss_G: 0.5916\n",
      "BATCH NUMBER 2190\n",
      "Epoch [2/5] Loss_D: 1.1475 Loss_G: 0.5788\n",
      "BATCH NUMBER 2191\n",
      "Epoch [2/5] Loss_D: 1.0724 Loss_G: 0.6379\n",
      "BATCH NUMBER 2192\n",
      "Epoch [2/5] Loss_D: 1.1046 Loss_G: 0.6184\n",
      "BATCH NUMBER 2193\n",
      "Epoch [2/5] Loss_D: 1.0586 Loss_G: 0.6500\n",
      "BATCH NUMBER 2194\n",
      "Epoch [2/5] Loss_D: 1.1446 Loss_G: 0.6151\n",
      "BATCH NUMBER 2195\n",
      "Epoch [2/5] Loss_D: 1.2695 Loss_G: 0.5181\n",
      "BATCH NUMBER 2196\n",
      "Epoch [2/5] Loss_D: 1.1032 Loss_G: 0.6140\n",
      "BATCH NUMBER 2197\n",
      "Epoch [2/5] Loss_D: 1.1042 Loss_G: 0.6209\n",
      "BATCH NUMBER 2198\n",
      "Epoch [2/5] Loss_D: 1.0981 Loss_G: 0.6168\n",
      "BATCH NUMBER 2199\n",
      "Epoch [2/5] Loss_D: 1.2719 Loss_G: 0.5146\n",
      "BATCH NUMBER 2200\n",
      "Epoch [2/5] Loss_D: 1.1072 Loss_G: 0.6160\n",
      "BATCH NUMBER 2201\n",
      "Epoch [2/5] Loss_D: 1.1420 Loss_G: 0.5910\n",
      "BATCH NUMBER 2202\n",
      "Epoch [2/5] Loss_D: 1.1733 Loss_G: 0.5699\n",
      "BATCH NUMBER 2203\n",
      "Epoch [2/5] Loss_D: 1.1306 Loss_G: 0.5950\n",
      "BATCH NUMBER 2204\n",
      "Epoch [2/5] Loss_D: 1.1339 Loss_G: 0.5960\n",
      "BATCH NUMBER 2205\n",
      "Epoch [2/5] Loss_D: 1.1138 Loss_G: 0.6024\n",
      "BATCH NUMBER 2206\n",
      "Epoch [2/5] Loss_D: 1.0961 Loss_G: 0.6173\n",
      "BATCH NUMBER 2207\n",
      "Epoch [2/5] Loss_D: 1.1222 Loss_G: 0.6042\n",
      "BATCH NUMBER 2208\n",
      "Epoch [2/5] Loss_D: 1.1065 Loss_G: 0.6128\n",
      "BATCH NUMBER 2209\n",
      "Epoch [2/5] Loss_D: 1.1536 Loss_G: 0.5841\n",
      "BATCH NUMBER 2210\n",
      "Epoch [2/5] Loss_D: 1.2720 Loss_G: 0.5020\n",
      "BATCH NUMBER 2211\n",
      "Epoch [2/5] Loss_D: 1.1754 Loss_G: 0.5642\n",
      "BATCH NUMBER 2212\n",
      "Epoch [2/5] Loss_D: 1.1563 Loss_G: 0.5856\n",
      "BATCH NUMBER 2213\n",
      "Epoch [2/5] Loss_D: 1.1308 Loss_G: 0.6106\n",
      "BATCH NUMBER 2214\n",
      "Epoch [2/5] Loss_D: 1.2856 Loss_G: 0.5001\n",
      "BATCH NUMBER 2215\n",
      "Epoch [2/5] Loss_D: 1.1237 Loss_G: 0.5964\n",
      "BATCH NUMBER 2216\n",
      "Epoch [2/5] Loss_D: 1.2025 Loss_G: 0.5751\n",
      "BATCH NUMBER 2217\n",
      "Epoch [2/5] Loss_D: 1.0565 Loss_G: 0.6500\n",
      "BATCH NUMBER 2218\n",
      "Epoch [2/5] Loss_D: 1.1220 Loss_G: 0.6151\n",
      "BATCH NUMBER 2219\n",
      "Epoch [2/5] Loss_D: 1.1950 Loss_G: 0.5541\n",
      "BATCH NUMBER 2220\n",
      "Epoch [2/5] Loss_D: 1.1678 Loss_G: 0.5747\n",
      "BATCH NUMBER 2221\n",
      "Epoch [2/5] Loss_D: 1.0587 Loss_G: 0.6485\n",
      "BATCH NUMBER 2222\n",
      "Epoch [2/5] Loss_D: 1.1821 Loss_G: 0.5965\n",
      "BATCH NUMBER 2223\n",
      "Epoch [2/5] Loss_D: 1.2973 Loss_G: 0.4930\n",
      "BATCH NUMBER 2224\n",
      "Epoch [2/5] Loss_D: 1.1082 Loss_G: 0.6240\n",
      "BATCH NUMBER 2225\n",
      "Epoch [2/5] Loss_D: 1.1242 Loss_G: 0.5990\n",
      "BATCH NUMBER 2226\n",
      "Epoch [2/5] Loss_D: 1.2532 Loss_G: 0.5223\n",
      "BATCH NUMBER 2227\n",
      "Epoch [2/5] Loss_D: 1.1607 Loss_G: 0.5734\n",
      "BATCH NUMBER 2228\n",
      "Epoch [2/5] Loss_D: 1.0817 Loss_G: 0.6251\n",
      "BATCH NUMBER 2229\n",
      "Epoch [2/5] Loss_D: 1.1523 Loss_G: 0.5929\n",
      "BATCH NUMBER 2230\n",
      "Epoch [2/5] Loss_D: 1.2002 Loss_G: 0.5524\n",
      "BATCH NUMBER 2231\n",
      "Epoch [2/5] Loss_D: 1.2752 Loss_G: 0.5518\n",
      "BATCH NUMBER 2232\n",
      "Epoch [2/5] Loss_D: 1.2280 Loss_G: 0.5374\n",
      "BATCH NUMBER 2233\n",
      "Epoch [2/5] Loss_D: 1.1051 Loss_G: 0.6128\n",
      "BATCH NUMBER 2234\n",
      "Epoch [2/5] Loss_D: 1.1875 Loss_G: 0.5612\n",
      "BATCH NUMBER 2235\n",
      "Epoch [2/5] Loss_D: 1.1383 Loss_G: 0.5929\n",
      "BATCH NUMBER 2236\n",
      "Epoch [2/5] Loss_D: 1.2647 Loss_G: 0.5186\n",
      "BATCH NUMBER 2237\n",
      "Epoch [2/5] Loss_D: 1.1267 Loss_G: 0.5994\n",
      "BATCH NUMBER 2238\n",
      "Epoch [2/5] Loss_D: 1.1071 Loss_G: 0.6160\n",
      "BATCH NUMBER 2239\n",
      "Epoch [2/5] Loss_D: 1.2234 Loss_G: 0.5426\n",
      "BATCH NUMBER 2240\n",
      "Epoch [2/5] Loss_D: 1.3628 Loss_G: 0.4482\n",
      "BATCH NUMBER 2241\n",
      "Epoch [2/5] Loss_D: 1.2277 Loss_G: 0.5377\n",
      "BATCH NUMBER 2242\n",
      "Epoch [2/5] Loss_D: 1.1323 Loss_G: 0.5989\n",
      "BATCH NUMBER 2243\n",
      "Epoch [2/5] Loss_D: 1.1079 Loss_G: 0.6158\n",
      "BATCH NUMBER 2244\n",
      "Epoch [2/5] Loss_D: 1.2361 Loss_G: 0.5298\n",
      "BATCH NUMBER 2245\n",
      "Epoch [2/5] Loss_D: 1.2048 Loss_G: 0.5397\n",
      "BATCH NUMBER 2246\n",
      "Epoch [2/5] Loss_D: 1.1196 Loss_G: 0.6033\n",
      "BATCH NUMBER 2247\n",
      "Epoch [2/5] Loss_D: 1.1802 Loss_G: 0.5825\n",
      "BATCH NUMBER 2248\n",
      "Epoch [2/5] Loss_D: 1.0869 Loss_G: 0.6257\n",
      "BATCH NUMBER 2249\n",
      "Epoch [2/5] Loss_D: 1.1202 Loss_G: 0.6155\n",
      "BATCH NUMBER 2250\n",
      "Epoch [2/5] Loss_D: 1.1378 Loss_G: 0.5938\n",
      "BATCH NUMBER 2251\n",
      "Epoch [2/5] Loss_D: 1.1009 Loss_G: 0.6227\n",
      "BATCH NUMBER 2252\n",
      "Epoch [2/5] Loss_D: 1.1305 Loss_G: 0.5899\n",
      "BATCH NUMBER 2253\n",
      "Epoch [2/5] Loss_D: 1.1523 Loss_G: 0.5884\n",
      "BATCH NUMBER 2254\n",
      "Epoch [2/5] Loss_D: 1.0569 Loss_G: 0.6504\n",
      "BATCH NUMBER 2255\n",
      "Epoch [2/5] Loss_D: 1.1742 Loss_G: 0.6316\n",
      "BATCH NUMBER 2256\n",
      "Epoch [2/5] Loss_D: 1.1339 Loss_G: 0.5953\n",
      "BATCH NUMBER 2257\n",
      "Epoch [2/5] Loss_D: 1.1458 Loss_G: 0.5798\n",
      "BATCH NUMBER 2258\n",
      "Epoch [2/5] Loss_D: 1.0889 Loss_G: 0.6259\n",
      "BATCH NUMBER 2259\n",
      "Epoch [2/5] Loss_D: 1.4038 Loss_G: 0.4351\n",
      "BATCH NUMBER 2260\n",
      "Epoch [2/5] Loss_D: 1.1405 Loss_G: 0.5948\n",
      "BATCH NUMBER 2261\n",
      "Epoch [2/5] Loss_D: 1.1335 Loss_G: 0.5916\n",
      "BATCH NUMBER 2262\n",
      "Epoch [2/5] Loss_D: 1.0981 Loss_G: 0.6255\n",
      "BATCH NUMBER 2263\n",
      "Epoch [2/5] Loss_D: 1.2047 Loss_G: 0.5540\n",
      "BATCH NUMBER 2264\n",
      "Epoch [2/5] Loss_D: 1.2139 Loss_G: 0.5394\n",
      "BATCH NUMBER 2265\n",
      "Epoch [2/5] Loss_D: 1.2368 Loss_G: 0.5397\n",
      "BATCH NUMBER 2266\n",
      "Epoch [2/5] Loss_D: 1.1290 Loss_G: 0.6020\n",
      "BATCH NUMBER 2267\n",
      "Epoch [2/5] Loss_D: 1.1215 Loss_G: 0.6068\n",
      "BATCH NUMBER 2268\n",
      "Epoch [2/5] Loss_D: 1.0539 Loss_G: 0.6543\n",
      "BATCH NUMBER 2269\n",
      "Epoch [2/5] Loss_D: 1.1023 Loss_G: 0.6234\n",
      "BATCH NUMBER 2270\n",
      "Epoch [2/5] Loss_D: 1.1814 Loss_G: 0.6089\n",
      "BATCH NUMBER 2271\n",
      "Epoch [2/5] Loss_D: 1.2292 Loss_G: 0.5415\n",
      "BATCH NUMBER 2272\n",
      "Epoch [2/5] Loss_D: 1.0785 Loss_G: 0.6327\n",
      "BATCH NUMBER 2273\n",
      "Epoch [2/5] Loss_D: 1.4110 Loss_G: 0.4340\n",
      "BATCH NUMBER 2274\n",
      "Epoch [2/5] Loss_D: 1.1002 Loss_G: 0.6153\n",
      "BATCH NUMBER 2275\n",
      "Epoch [2/5] Loss_D: 1.2300 Loss_G: 0.5580\n",
      "BATCH NUMBER 2276\n",
      "Epoch [2/5] Loss_D: 1.2928 Loss_G: 0.5584\n",
      "BATCH NUMBER 2277\n",
      "Epoch [2/5] Loss_D: 1.2913 Loss_G: 0.5005\n",
      "BATCH NUMBER 2278\n",
      "Epoch [2/5] Loss_D: 1.2301 Loss_G: 0.5269\n",
      "BATCH NUMBER 2279\n",
      "Epoch [2/5] Loss_D: 1.1848 Loss_G: 0.5737\n",
      "BATCH NUMBER 2280\n",
      "Epoch [2/5] Loss_D: 1.2436 Loss_G: 0.5240\n",
      "BATCH NUMBER 2281\n",
      "Epoch [2/5] Loss_D: 1.1348 Loss_G: 0.6019\n",
      "BATCH NUMBER 2282\n",
      "Epoch [2/5] Loss_D: 1.1007 Loss_G: 0.6154\n",
      "BATCH NUMBER 2283\n",
      "Epoch [2/5] Loss_D: 1.2368 Loss_G: 0.5225\n",
      "BATCH NUMBER 2284\n",
      "Epoch [2/5] Loss_D: 1.0988 Loss_G: 0.6165\n",
      "BATCH NUMBER 2285\n",
      "Epoch [2/5] Loss_D: 1.1264 Loss_G: 0.5979\n",
      "BATCH NUMBER 2286\n",
      "Epoch [2/5] Loss_D: 1.3302 Loss_G: 0.4839\n",
      "BATCH NUMBER 2287\n",
      "Epoch [2/5] Loss_D: 1.2881 Loss_G: 0.5010\n",
      "BATCH NUMBER 2288\n",
      "Epoch [2/5] Loss_D: 1.1516 Loss_G: 0.5890\n",
      "BATCH NUMBER 2289\n",
      "Epoch [2/5] Loss_D: 1.1231 Loss_G: 0.6058\n",
      "BATCH NUMBER 2290\n",
      "Epoch [2/5] Loss_D: 1.2100 Loss_G: 0.5442\n",
      "BATCH NUMBER 2291\n",
      "Epoch [2/5] Loss_D: 1.1096 Loss_G: 0.6147\n",
      "BATCH NUMBER 2292\n",
      "Epoch [2/5] Loss_D: 1.0870 Loss_G: 0.6264\n",
      "BATCH NUMBER 2293\n",
      "Epoch [2/5] Loss_D: 1.1524 Loss_G: 0.5958\n",
      "BATCH NUMBER 2294\n",
      "Epoch [2/5] Loss_D: 1.0387 Loss_G: 0.6691\n",
      "BATCH NUMBER 2295\n",
      "Epoch [2/5] Loss_D: 1.3891 Loss_G: 0.4443\n",
      "BATCH NUMBER 2296\n",
      "Epoch [2/5] Loss_D: 1.2363 Loss_G: 0.5576\n",
      "BATCH NUMBER 2297\n",
      "Epoch [2/5] Loss_D: 1.0765 Loss_G: 0.6351\n",
      "BATCH NUMBER 2298\n",
      "Epoch [2/5] Loss_D: 1.0802 Loss_G: 0.6401\n",
      "BATCH NUMBER 2299\n",
      "Epoch [2/5] Loss_D: 1.1553 Loss_G: 0.5799\n",
      "BATCH NUMBER 2300\n",
      "Epoch [2/5] Loss_D: 1.2583 Loss_G: 0.5204\n",
      "BATCH NUMBER 2301\n",
      "Epoch [2/5] Loss_D: 1.1869 Loss_G: 0.5636\n",
      "BATCH NUMBER 2302\n",
      "Epoch [2/5] Loss_D: 1.2089 Loss_G: 0.5589\n",
      "BATCH NUMBER 2303\n",
      "Epoch [2/5] Loss_D: 1.0823 Loss_G: 0.6284\n",
      "BATCH NUMBER 2304\n",
      "Epoch [2/5] Loss_D: 1.1298 Loss_G: 0.5982\n",
      "BATCH NUMBER 2305\n",
      "Epoch [2/5] Loss_D: 1.1755 Loss_G: 0.5634\n",
      "BATCH NUMBER 2306\n",
      "Epoch [2/5] Loss_D: 1.1001 Loss_G: 0.6157\n",
      "BATCH NUMBER 2307\n",
      "Epoch [2/5] Loss_D: 1.1882 Loss_G: 0.5536\n",
      "BATCH NUMBER 2308\n",
      "Epoch [2/5] Loss_D: 1.0827 Loss_G: 0.6489\n",
      "BATCH NUMBER 2309\n",
      "Epoch [2/5] Loss_D: 1.2077 Loss_G: 0.5572\n",
      "BATCH NUMBER 2310\n",
      "Epoch [2/5] Loss_D: 1.1241 Loss_G: 0.5952\n",
      "BATCH NUMBER 2311\n",
      "Epoch [2/5] Loss_D: 1.1327 Loss_G: 0.6015\n",
      "BATCH NUMBER 2312\n",
      "Epoch [2/5] Loss_D: 1.0929 Loss_G: 0.6227\n",
      "BATCH NUMBER 2313\n",
      "Epoch [2/5] Loss_D: 1.1645 Loss_G: 0.5690\n",
      "BATCH NUMBER 2314\n",
      "Epoch [2/5] Loss_D: 1.1455 Loss_G: 0.5889\n",
      "BATCH NUMBER 2315\n",
      "Epoch [2/5] Loss_D: 1.3047 Loss_G: 0.4902\n",
      "BATCH NUMBER 2316\n",
      "Epoch [2/5] Loss_D: 1.1993 Loss_G: 0.5566\n",
      "BATCH NUMBER 2317\n",
      "Epoch [2/5] Loss_D: 1.1643 Loss_G: 0.6453\n",
      "BATCH NUMBER 2318\n",
      "Epoch [2/5] Loss_D: 1.2350 Loss_G: 0.5327\n",
      "BATCH NUMBER 2319\n",
      "Epoch [2/5] Loss_D: 1.1058 Loss_G: 0.6122\n",
      "BATCH NUMBER 2320\n",
      "Epoch [2/5] Loss_D: 1.0721 Loss_G: 0.6417\n",
      "BATCH NUMBER 2321\n",
      "Epoch [2/5] Loss_D: 1.0753 Loss_G: 0.6435\n",
      "BATCH NUMBER 2322\n",
      "Epoch [2/5] Loss_D: 1.0762 Loss_G: 0.6352\n",
      "BATCH NUMBER 2323\n",
      "Epoch [2/5] Loss_D: 1.3001 Loss_G: 0.4912\n",
      "BATCH NUMBER 2324\n",
      "Epoch [2/5] Loss_D: 1.1166 Loss_G: 0.6025\n",
      "BATCH NUMBER 2325\n",
      "Epoch [2/5] Loss_D: 1.2444 Loss_G: 0.5301\n",
      "BATCH NUMBER 2326\n",
      "Epoch [2/5] Loss_D: 1.0835 Loss_G: 0.6281\n",
      "BATCH NUMBER 2327\n",
      "Epoch [2/5] Loss_D: 1.1299 Loss_G: 0.6025\n",
      "BATCH NUMBER 2328\n",
      "Epoch [2/5] Loss_D: 1.1876 Loss_G: 0.5507\n",
      "BATCH NUMBER 2329\n",
      "Epoch [2/5] Loss_D: 1.1811 Loss_G: 0.5887\n",
      "BATCH NUMBER 2330\n",
      "Epoch [2/5] Loss_D: 1.2878 Loss_G: 0.5049\n",
      "BATCH NUMBER 2331\n",
      "Epoch [2/5] Loss_D: 1.1566 Loss_G: 0.5799\n",
      "BATCH NUMBER 2332\n",
      "Epoch [2/5] Loss_D: 1.1420 Loss_G: 0.5806\n",
      "BATCH NUMBER 2333\n",
      "Epoch [2/5] Loss_D: 1.1309 Loss_G: 0.5970\n",
      "BATCH NUMBER 2334\n",
      "Epoch [2/5] Loss_D: 1.1209 Loss_G: 0.6035\n",
      "BATCH NUMBER 2335\n",
      "Epoch [2/5] Loss_D: 1.2116 Loss_G: 0.5715\n",
      "BATCH NUMBER 2336\n",
      "Epoch [2/5] Loss_D: 1.1118 Loss_G: 0.6135\n",
      "BATCH NUMBER 2337\n",
      "Epoch [2/5] Loss_D: 1.1965 Loss_G: 0.5585\n",
      "BATCH NUMBER 2338\n",
      "Epoch [2/5] Loss_D: 1.2347 Loss_G: 0.5553\n",
      "BATCH NUMBER 2339\n",
      "Epoch [2/5] Loss_D: 1.1020 Loss_G: 0.6152\n",
      "BATCH NUMBER 2340\n",
      "Epoch [2/5] Loss_D: 1.0741 Loss_G: 0.6366\n",
      "BATCH NUMBER 2341\n",
      "Epoch [2/5] Loss_D: 1.3085 Loss_G: 0.4928\n",
      "BATCH NUMBER 2342\n",
      "Epoch [2/5] Loss_D: 1.1736 Loss_G: 0.5702\n",
      "BATCH NUMBER 2343\n",
      "Epoch [2/5] Loss_D: 1.1115 Loss_G: 0.6160\n",
      "BATCH NUMBER 2344\n",
      "Epoch [2/5] Loss_D: 1.0909 Loss_G: 0.6232\n",
      "BATCH NUMBER 2345\n",
      "Epoch [2/5] Loss_D: 1.1814 Loss_G: 0.5519\n",
      "BATCH NUMBER 2346\n",
      "Epoch [2/5] Loss_D: 1.1728 Loss_G: 0.5726\n",
      "BATCH NUMBER 2347\n",
      "Epoch [2/5] Loss_D: 1.1598 Loss_G: 0.5846\n",
      "BATCH NUMBER 2348\n",
      "Epoch [2/5] Loss_D: 1.1427 Loss_G: 0.5966\n",
      "BATCH NUMBER 2349\n",
      "Epoch [2/5] Loss_D: 1.1240 Loss_G: 0.6171\n",
      "BATCH NUMBER 2350\n",
      "Epoch [2/5] Loss_D: 1.3650 Loss_G: 0.4723\n",
      "BATCH NUMBER 2351\n",
      "Epoch [2/5] Loss_D: 1.3309 Loss_G: 0.4826\n",
      "BATCH NUMBER 2352\n",
      "Epoch [2/5] Loss_D: 1.0463 Loss_G: 0.6578\n",
      "BATCH NUMBER 2353\n",
      "Epoch [2/5] Loss_D: 1.1725 Loss_G: 0.5795\n",
      "BATCH NUMBER 2354\n",
      "Epoch [2/5] Loss_D: 1.2277 Loss_G: 0.5352\n",
      "BATCH NUMBER 2355\n",
      "Epoch [2/5] Loss_D: 1.0771 Loss_G: 0.6324\n",
      "BATCH NUMBER 2356\n",
      "Epoch [2/5] Loss_D: 1.3090 Loss_G: 0.4922\n",
      "BATCH NUMBER 2357\n",
      "Epoch [2/5] Loss_D: 1.1202 Loss_G: 0.6063\n",
      "BATCH NUMBER 2358\n",
      "Epoch [2/5] Loss_D: 1.0640 Loss_G: 0.6527\n",
      "BATCH NUMBER 2359\n",
      "Epoch [2/5] Loss_D: 1.1432 Loss_G: 0.5838\n",
      "BATCH NUMBER 2360\n",
      "Epoch [2/5] Loss_D: 1.1747 Loss_G: 0.5686\n",
      "BATCH NUMBER 2361\n",
      "Epoch [2/5] Loss_D: 1.2264 Loss_G: 0.5409\n",
      "BATCH NUMBER 2362\n",
      "Epoch [2/5] Loss_D: 1.2916 Loss_G: 0.4998\n",
      "BATCH NUMBER 2363\n",
      "Epoch [2/5] Loss_D: 1.1172 Loss_G: 0.6055\n",
      "BATCH NUMBER 2364\n",
      "Epoch [2/5] Loss_D: 1.1972 Loss_G: 0.5606\n",
      "BATCH NUMBER 2365\n",
      "Epoch [2/5] Loss_D: 1.1793 Loss_G: 0.5899\n",
      "BATCH NUMBER 2366\n",
      "Epoch [2/5] Loss_D: 1.0536 Loss_G: 0.6528\n",
      "BATCH NUMBER 2367\n",
      "Epoch [2/5] Loss_D: 1.1747 Loss_G: 0.5924\n",
      "BATCH NUMBER 2368\n",
      "Epoch [2/5] Loss_D: 1.1305 Loss_G: 0.5995\n",
      "BATCH NUMBER 2369\n",
      "Epoch [2/5] Loss_D: 1.0634 Loss_G: 0.6468\n",
      "BATCH NUMBER 2370\n",
      "Epoch [2/5] Loss_D: 1.1008 Loss_G: 0.6129\n",
      "BATCH NUMBER 2371\n",
      "Epoch [2/5] Loss_D: 1.1613 Loss_G: 0.5759\n",
      "BATCH NUMBER 2372\n",
      "Epoch [2/5] Loss_D: 1.0744 Loss_G: 0.6477\n",
      "BATCH NUMBER 2373\n",
      "Epoch [2/5] Loss_D: 1.1839 Loss_G: 0.5935\n",
      "BATCH NUMBER 2374\n",
      "Epoch [2/5] Loss_D: 1.1830 Loss_G: 0.5648\n",
      "BATCH NUMBER 2375\n",
      "Epoch [2/5] Loss_D: 1.1271 Loss_G: 0.5928\n",
      "BATCH NUMBER 2376\n",
      "Epoch [2/5] Loss_D: 1.1792 Loss_G: 0.5753\n",
      "BATCH NUMBER 2377\n",
      "Epoch [2/5] Loss_D: 1.0890 Loss_G: 0.6380\n",
      "BATCH NUMBER 2378\n",
      "Epoch [2/5] Loss_D: 1.1984 Loss_G: 0.5696\n",
      "BATCH NUMBER 2379\n",
      "Epoch [2/5] Loss_D: 1.1568 Loss_G: 0.5773\n",
      "BATCH NUMBER 2380\n",
      "Epoch [2/5] Loss_D: 1.1118 Loss_G: 0.6113\n",
      "BATCH NUMBER 2381\n",
      "Epoch [2/5] Loss_D: 1.0563 Loss_G: 0.6522\n",
      "BATCH NUMBER 2382\n",
      "Epoch [2/5] Loss_D: 1.0275 Loss_G: 0.6740\n",
      "BATCH NUMBER 2383\n",
      "Epoch [2/5] Loss_D: 1.0907 Loss_G: 0.6434\n",
      "BATCH NUMBER 2384\n",
      "Epoch [2/5] Loss_D: 1.1252 Loss_G: 0.6029\n",
      "BATCH NUMBER 2385\n",
      "Epoch [2/5] Loss_D: 1.0454 Loss_G: 0.6588\n",
      "BATCH NUMBER 2386\n",
      "Epoch [2/5] Loss_D: 1.0831 Loss_G: 0.6311\n",
      "BATCH NUMBER 2387\n",
      "Epoch [2/5] Loss_D: 1.1198 Loss_G: 0.6053\n",
      "BATCH NUMBER 2388\n",
      "Epoch [2/5] Loss_D: 1.1678 Loss_G: 0.5825\n",
      "BATCH NUMBER 2389\n",
      "Epoch [2/5] Loss_D: 1.1905 Loss_G: 0.5586\n",
      "BATCH NUMBER 2390\n",
      "Epoch [2/5] Loss_D: 1.0832 Loss_G: 0.6307\n",
      "BATCH NUMBER 2391\n",
      "Epoch [2/5] Loss_D: 1.3054 Loss_G: 0.5388\n",
      "BATCH NUMBER 2392\n",
      "Epoch [2/5] Loss_D: 1.1788 Loss_G: 0.5645\n",
      "BATCH NUMBER 2393\n",
      "Epoch [2/5] Loss_D: 1.2423 Loss_G: 0.5297\n",
      "BATCH NUMBER 2394\n",
      "Epoch [2/5] Loss_D: 1.0952 Loss_G: 0.6180\n",
      "BATCH NUMBER 2395\n",
      "Epoch [2/5] Loss_D: 1.1413 Loss_G: 0.5976\n",
      "BATCH NUMBER 2396\n",
      "Epoch [2/5] Loss_D: 1.1650 Loss_G: 0.5761\n",
      "BATCH NUMBER 2397\n",
      "Epoch [2/5] Loss_D: 1.2511 Loss_G: 0.5312\n",
      "BATCH NUMBER 2398\n",
      "Epoch [2/5] Loss_D: 1.1254 Loss_G: 0.6046\n",
      "BATCH NUMBER 2399\n",
      "Epoch [2/5] Loss_D: 1.2951 Loss_G: 0.4965\n",
      "BATCH NUMBER 2400\n",
      "Epoch [2/5] Loss_D: 1.1241 Loss_G: 0.6019\n",
      "BATCH NUMBER 2401\n",
      "Epoch [2/5] Loss_D: 1.1676 Loss_G: 0.5825\n",
      "BATCH NUMBER 2402\n",
      "Epoch [2/5] Loss_D: 1.2744 Loss_G: 0.5154\n",
      "BATCH NUMBER 2403\n",
      "Epoch [2/5] Loss_D: 1.0748 Loss_G: 0.6338\n",
      "BATCH NUMBER 2404\n",
      "Epoch [2/5] Loss_D: 1.2308 Loss_G: 0.5563\n",
      "BATCH NUMBER 2405\n",
      "Epoch [2/5] Loss_D: 1.1431 Loss_G: 0.5903\n",
      "BATCH NUMBER 2406\n",
      "Epoch [2/5] Loss_D: 1.0966 Loss_G: 0.6254\n",
      "BATCH NUMBER 2407\n",
      "Epoch [2/5] Loss_D: 1.1292 Loss_G: 0.5925\n",
      "BATCH NUMBER 2408\n",
      "Epoch [2/5] Loss_D: 1.1288 Loss_G: 0.6002\n",
      "BATCH NUMBER 2409\n",
      "Epoch [2/5] Loss_D: 1.1431 Loss_G: 0.5912\n",
      "BATCH NUMBER 2410\n",
      "Epoch [2/5] Loss_D: 1.0884 Loss_G: 0.6263\n",
      "BATCH NUMBER 2411\n",
      "Epoch [2/5] Loss_D: 1.2501 Loss_G: 0.5170\n",
      "BATCH NUMBER 2412\n",
      "Epoch [2/5] Loss_D: 1.0877 Loss_G: 0.6298\n",
      "BATCH NUMBER 2413\n",
      "Epoch [2/5] Loss_D: 1.1737 Loss_G: 0.5727\n",
      "BATCH NUMBER 2414\n",
      "Epoch [2/5] Loss_D: 1.1382 Loss_G: 0.5870\n",
      "BATCH NUMBER 2415\n",
      "Epoch [2/5] Loss_D: 1.1961 Loss_G: 0.5685\n",
      "BATCH NUMBER 2416\n",
      "Epoch [2/5] Loss_D: 1.1302 Loss_G: 0.6057\n",
      "BATCH NUMBER 2417\n",
      "Epoch [2/5] Loss_D: 1.0685 Loss_G: 0.6426\n",
      "BATCH NUMBER 2418\n",
      "Epoch [2/5] Loss_D: 1.0697 Loss_G: 0.6479\n",
      "BATCH NUMBER 2419\n",
      "Epoch [2/5] Loss_D: 1.1119 Loss_G: 0.6097\n",
      "BATCH NUMBER 2420\n",
      "Epoch [2/5] Loss_D: 1.1573 Loss_G: 0.5867\n",
      "BATCH NUMBER 2421\n",
      "Epoch [2/5] Loss_D: 1.1768 Loss_G: 0.5709\n",
      "BATCH NUMBER 2422\n",
      "Epoch [2/5] Loss_D: 1.1349 Loss_G: 0.5894\n",
      "BATCH NUMBER 2423\n",
      "Epoch [2/5] Loss_D: 1.2200 Loss_G: 0.5441\n",
      "BATCH NUMBER 2424\n",
      "Epoch [2/5] Loss_D: 1.0978 Loss_G: 0.6187\n",
      "BATCH NUMBER 2425\n",
      "Epoch [2/5] Loss_D: 1.2208 Loss_G: 0.5496\n",
      "BATCH NUMBER 2426\n",
      "Epoch [2/5] Loss_D: 1.1077 Loss_G: 0.6079\n",
      "BATCH NUMBER 2427\n",
      "Epoch [2/5] Loss_D: 1.2042 Loss_G: 0.5587\n",
      "BATCH NUMBER 2428\n",
      "Epoch [2/5] Loss_D: 1.1108 Loss_G: 0.6171\n",
      "BATCH NUMBER 2429\n",
      "Epoch [2/5] Loss_D: 1.1524 Loss_G: 0.5979\n",
      "BATCH NUMBER 2430\n",
      "Epoch [2/5] Loss_D: 1.0704 Loss_G: 0.6400\n",
      "BATCH NUMBER 2431\n",
      "Epoch [2/5] Loss_D: 1.1804 Loss_G: 0.5791\n",
      "BATCH NUMBER 2432\n",
      "Epoch [2/5] Loss_D: 1.1266 Loss_G: 0.5991\n",
      "BATCH NUMBER 2433\n",
      "Epoch [2/5] Loss_D: 1.1776 Loss_G: 0.5864\n",
      "BATCH NUMBER 2434\n",
      "Epoch [2/5] Loss_D: 1.0646 Loss_G: 0.6458\n",
      "BATCH NUMBER 2435\n",
      "Epoch [2/5] Loss_D: 1.0869 Loss_G: 0.6258\n",
      "BATCH NUMBER 2436\n",
      "Epoch [2/5] Loss_D: 1.1319 Loss_G: 0.5962\n",
      "BATCH NUMBER 2437\n",
      "Epoch [2/5] Loss_D: 1.2083 Loss_G: 0.5509\n",
      "BATCH NUMBER 2438\n",
      "Epoch [2/5] Loss_D: 1.2150 Loss_G: 0.5468\n",
      "BATCH NUMBER 2439\n",
      "Epoch [2/5] Loss_D: 1.1343 Loss_G: 0.6020\n",
      "BATCH NUMBER 2440\n",
      "Epoch [2/5] Loss_D: 1.2550 Loss_G: 0.5274\n",
      "BATCH NUMBER 2441\n",
      "Epoch [2/5] Loss_D: 1.1365 Loss_G: 0.5855\n",
      "BATCH NUMBER 2442\n",
      "Epoch [2/5] Loss_D: 1.1332 Loss_G: 0.5913\n",
      "BATCH NUMBER 2443\n",
      "Epoch [2/5] Loss_D: 1.2100 Loss_G: 0.5398\n",
      "BATCH NUMBER 2444\n",
      "Epoch [2/5] Loss_D: 1.1711 Loss_G: 0.5991\n",
      "BATCH NUMBER 2445\n",
      "Epoch [2/5] Loss_D: 1.0743 Loss_G: 0.6340\n",
      "BATCH NUMBER 2446\n",
      "Epoch [2/5] Loss_D: 1.0837 Loss_G: 0.6300\n",
      "BATCH NUMBER 2447\n",
      "Epoch [2/5] Loss_D: 1.2439 Loss_G: 0.5801\n",
      "BATCH NUMBER 2448\n",
      "Epoch [2/5] Loss_D: 1.1551 Loss_G: 0.5784\n",
      "BATCH NUMBER 2449\n",
      "Epoch [2/5] Loss_D: 1.2263 Loss_G: 0.5439\n",
      "BATCH NUMBER 2450\n",
      "Epoch [2/5] Loss_D: 1.1959 Loss_G: 0.5647\n",
      "BATCH NUMBER 2451\n",
      "Epoch [2/5] Loss_D: 1.2039 Loss_G: 0.5562\n",
      "BATCH NUMBER 2452\n",
      "Epoch [2/5] Loss_D: 1.1751 Loss_G: 0.5755\n",
      "BATCH NUMBER 2453\n",
      "Epoch [2/5] Loss_D: 1.0667 Loss_G: 0.6418\n",
      "BATCH NUMBER 2454\n",
      "Epoch [2/5] Loss_D: 1.0641 Loss_G: 0.6442\n",
      "BATCH NUMBER 2455\n",
      "Epoch [2/5] Loss_D: 1.1073 Loss_G: 0.6124\n",
      "BATCH NUMBER 2456\n",
      "Epoch [2/5] Loss_D: 1.1716 Loss_G: 0.5753\n",
      "BATCH NUMBER 2457\n",
      "Epoch [2/5] Loss_D: 1.0547 Loss_G: 0.6517\n",
      "BATCH NUMBER 2458\n",
      "Epoch [2/5] Loss_D: 1.1179 Loss_G: 0.6098\n",
      "BATCH NUMBER 2459\n",
      "Epoch [2/5] Loss_D: 1.1139 Loss_G: 0.6076\n",
      "BATCH NUMBER 2460\n",
      "Epoch [2/5] Loss_D: 1.0427 Loss_G: 0.6625\n",
      "BATCH NUMBER 2461\n",
      "Epoch [2/5] Loss_D: 1.0713 Loss_G: 0.6395\n",
      "BATCH NUMBER 2462\n",
      "Epoch [2/5] Loss_D: 1.0525 Loss_G: 0.6568\n",
      "BATCH NUMBER 2463\n",
      "Epoch [2/5] Loss_D: 1.1825 Loss_G: 0.5583\n",
      "BATCH NUMBER 2464\n",
      "Epoch [2/5] Loss_D: 1.1764 Loss_G: 0.5979\n",
      "BATCH NUMBER 2465\n",
      "Epoch [2/5] Loss_D: 1.1002 Loss_G: 0.6158\n",
      "BATCH NUMBER 2466\n",
      "Epoch [2/5] Loss_D: 1.0586 Loss_G: 0.6494\n",
      "BATCH NUMBER 2467\n",
      "Epoch [2/5] Loss_D: 1.2557 Loss_G: 0.5203\n",
      "BATCH NUMBER 2468\n",
      "Epoch [2/5] Loss_D: 1.0970 Loss_G: 0.6170\n",
      "BATCH NUMBER 2469\n",
      "Epoch [2/5] Loss_D: 1.0791 Loss_G: 0.6321\n",
      "BATCH NUMBER 2470\n",
      "Epoch [2/5] Loss_D: 1.1717 Loss_G: 0.5644\n",
      "BATCH NUMBER 2471\n",
      "Epoch [2/5] Loss_D: 1.0775 Loss_G: 0.6317\n",
      "BATCH NUMBER 2472\n",
      "Epoch [2/5] Loss_D: 1.3250 Loss_G: 0.4856\n",
      "BATCH NUMBER 2473\n",
      "Epoch [2/5] Loss_D: 1.1637 Loss_G: 0.5787\n",
      "BATCH NUMBER 2474\n",
      "Epoch [2/5] Loss_D: 1.1412 Loss_G: 0.5954\n",
      "BATCH NUMBER 2475\n",
      "Epoch [2/5] Loss_D: 1.1126 Loss_G: 0.6123\n",
      "BATCH NUMBER 2476\n",
      "Epoch [2/5] Loss_D: 1.2450 Loss_G: 0.6410\n",
      "BATCH NUMBER 2477\n",
      "Epoch [2/5] Loss_D: 1.1056 Loss_G: 0.6109\n",
      "BATCH NUMBER 2478\n",
      "Epoch [2/5] Loss_D: 1.2101 Loss_G: 0.5568\n",
      "BATCH NUMBER 2479\n",
      "Epoch [2/5] Loss_D: 1.1115 Loss_G: 0.6144\n",
      "BATCH NUMBER 2480\n",
      "Epoch [2/5] Loss_D: 1.1278 Loss_G: 0.5970\n",
      "BATCH NUMBER 2481\n",
      "Epoch [2/5] Loss_D: 1.1354 Loss_G: 0.6032\n",
      "BATCH NUMBER 2482\n",
      "Epoch [2/5] Loss_D: 1.0605 Loss_G: 0.6453\n",
      "BATCH NUMBER 2483\n",
      "Epoch [2/5] Loss_D: 1.0700 Loss_G: 0.6371\n",
      "BATCH NUMBER 2484\n",
      "Epoch [2/5] Loss_D: 1.1904 Loss_G: 0.5577\n",
      "BATCH NUMBER 2485\n",
      "Epoch [2/5] Loss_D: 1.1653 Loss_G: 0.5795\n",
      "BATCH NUMBER 2486\n",
      "Epoch [2/5] Loss_D: 1.2104 Loss_G: 0.5397\n",
      "BATCH NUMBER 2487\n",
      "Epoch [2/5] Loss_D: 1.2535 Loss_G: 0.5389\n",
      "BATCH NUMBER 2488\n",
      "Epoch [2/5] Loss_D: 1.2486 Loss_G: 0.5334\n",
      "BATCH NUMBER 2489\n",
      "Epoch [2/5] Loss_D: 1.0402 Loss_G: 0.6637\n",
      "BATCH NUMBER 2490\n",
      "Epoch [2/5] Loss_D: 1.2504 Loss_G: 0.5255\n",
      "BATCH NUMBER 2491\n",
      "Epoch [2/5] Loss_D: 1.2707 Loss_G: 0.5091\n",
      "BATCH NUMBER 2492\n",
      "Epoch [2/5] Loss_D: 1.1598 Loss_G: 0.5789\n",
      "BATCH NUMBER 2493\n",
      "Epoch [2/5] Loss_D: 1.2922 Loss_G: 0.5066\n",
      "BATCH NUMBER 2494\n",
      "Epoch [2/5] Loss_D: 1.2174 Loss_G: 0.5462\n",
      "BATCH NUMBER 2495\n",
      "Epoch [2/5] Loss_D: 1.1291 Loss_G: 0.6092\n",
      "BATCH NUMBER 2496\n",
      "Epoch [2/5] Loss_D: 1.1298 Loss_G: 0.6060\n",
      "BATCH NUMBER 2497\n",
      "Epoch [2/5] Loss_D: 1.0974 Loss_G: 0.6240\n",
      "BATCH NUMBER 2498\n",
      "Epoch [2/5] Loss_D: 1.1703 Loss_G: 0.5727\n",
      "BATCH NUMBER 2499\n",
      "Epoch [2/5] Loss_D: 1.1658 Loss_G: 0.5708\n",
      "BATCH NUMBER 2500\n",
      "Epoch [2/5] Loss_D: 1.1330 Loss_G: 0.5961\n",
      "BATCH NUMBER 2501\n",
      "Epoch [2/5] Loss_D: 1.0596 Loss_G: 0.6462\n",
      "BATCH NUMBER 2502\n",
      "Epoch [2/5] Loss_D: 1.3872 Loss_G: 0.5552\n",
      "BATCH NUMBER 2503\n",
      "Epoch [2/5] Loss_D: 1.0563 Loss_G: 0.6522\n",
      "BATCH NUMBER 2504\n",
      "Epoch [2/5] Loss_D: 1.1513 Loss_G: 0.5744\n",
      "BATCH NUMBER 2505\n",
      "Epoch [2/5] Loss_D: 1.0888 Loss_G: 0.6252\n",
      "BATCH NUMBER 2506\n",
      "Epoch [2/5] Loss_D: 1.1185 Loss_G: 0.6038\n",
      "BATCH NUMBER 2507\n",
      "Epoch [2/5] Loss_D: 1.1297 Loss_G: 0.5928\n",
      "BATCH NUMBER 2508\n",
      "Epoch [2/5] Loss_D: 1.1728 Loss_G: 0.5597\n",
      "BATCH NUMBER 2509\n",
      "Epoch [2/5] Loss_D: 1.0861 Loss_G: 0.6262\n",
      "BATCH NUMBER 2510\n",
      "Epoch [2/5] Loss_D: 1.3987 Loss_G: 0.4661\n",
      "BATCH NUMBER 2511\n",
      "Epoch [2/5] Loss_D: 1.1455 Loss_G: 0.5929\n",
      "BATCH NUMBER 2512\n",
      "Epoch [2/5] Loss_D: 1.2183 Loss_G: 0.5309\n",
      "BATCH NUMBER 2513\n",
      "Epoch [2/5] Loss_D: 1.2452 Loss_G: 0.6063\n",
      "BATCH NUMBER 2514\n",
      "Epoch [2/5] Loss_D: 1.1077 Loss_G: 0.6098\n",
      "BATCH NUMBER 2515\n",
      "Epoch [2/5] Loss_D: 1.3934 Loss_G: 0.4448\n",
      "BATCH NUMBER 2516\n",
      "Epoch [2/5] Loss_D: 1.2323 Loss_G: 0.5320\n",
      "BATCH NUMBER 2517\n",
      "Epoch [2/5] Loss_D: 1.0889 Loss_G: 0.6335\n",
      "BATCH NUMBER 2518\n",
      "Epoch [2/5] Loss_D: 1.1245 Loss_G: 0.6050\n",
      "BATCH NUMBER 2519\n",
      "Epoch [2/5] Loss_D: 1.2237 Loss_G: 0.5322\n",
      "BATCH NUMBER 2520\n",
      "Epoch [2/5] Loss_D: 1.0586 Loss_G: 0.6571\n",
      "BATCH NUMBER 2521\n",
      "Epoch [2/5] Loss_D: 1.0762 Loss_G: 0.6436\n",
      "BATCH NUMBER 2522\n",
      "Epoch [2/5] Loss_D: 1.0621 Loss_G: 0.6467\n",
      "BATCH NUMBER 2523\n",
      "Epoch [2/5] Loss_D: 1.1993 Loss_G: 0.5569\n",
      "BATCH NUMBER 2524\n",
      "Epoch [2/5] Loss_D: 1.1970 Loss_G: 0.5529\n",
      "BATCH NUMBER 2525\n",
      "Epoch [2/5] Loss_D: 1.1297 Loss_G: 0.5881\n",
      "BATCH NUMBER 2526\n",
      "Epoch [2/5] Loss_D: 1.1770 Loss_G: 0.5850\n",
      "BATCH NUMBER 2527\n",
      "Epoch [2/5] Loss_D: 1.2809 Loss_G: 0.5747\n",
      "BATCH NUMBER 2528\n",
      "Epoch [2/5] Loss_D: 1.2386 Loss_G: 0.5324\n",
      "BATCH NUMBER 2529\n",
      "Epoch [2/5] Loss_D: 1.1901 Loss_G: 0.5635\n",
      "BATCH NUMBER 2530\n",
      "Epoch [2/5] Loss_D: 1.1237 Loss_G: 0.5957\n",
      "BATCH NUMBER 2531\n",
      "Epoch [2/5] Loss_D: 1.1924 Loss_G: 0.5572\n",
      "BATCH NUMBER 2532\n",
      "Epoch [2/5] Loss_D: 1.1577 Loss_G: 0.5724\n",
      "BATCH NUMBER 2533\n",
      "Epoch [2/5] Loss_D: 1.1403 Loss_G: 0.5920\n",
      "BATCH NUMBER 2534\n",
      "Epoch [2/5] Loss_D: 1.0771 Loss_G: 0.6353\n",
      "BATCH NUMBER 2535\n",
      "Epoch [2/5] Loss_D: 1.1533 Loss_G: 0.5703\n",
      "BATCH NUMBER 2536\n",
      "Epoch [2/5] Loss_D: 1.1734 Loss_G: 0.5686\n",
      "BATCH NUMBER 2537\n",
      "Epoch [2/5] Loss_D: 1.1420 Loss_G: 0.5911\n",
      "BATCH NUMBER 2538\n",
      "Epoch [2/5] Loss_D: 1.0652 Loss_G: 0.6412\n",
      "BATCH NUMBER 2539\n",
      "Epoch [2/5] Loss_D: 1.1610 Loss_G: 0.6017\n",
      "BATCH NUMBER 2540\n",
      "Epoch [2/5] Loss_D: 1.2602 Loss_G: 0.5173\n",
      "BATCH NUMBER 2541\n",
      "Epoch [2/5] Loss_D: 1.1248 Loss_G: 0.6038\n",
      "BATCH NUMBER 2542\n",
      "Epoch [2/5] Loss_D: 1.1775 Loss_G: 0.5690\n",
      "BATCH NUMBER 2543\n",
      "Epoch [2/5] Loss_D: 1.1244 Loss_G: 0.5958\n",
      "BATCH NUMBER 2544\n",
      "Epoch [2/5] Loss_D: 1.1342 Loss_G: 0.6001\n",
      "BATCH NUMBER 2545\n",
      "Epoch [2/5] Loss_D: 1.1675 Loss_G: 0.5687\n",
      "BATCH NUMBER 2546\n",
      "Epoch [2/5] Loss_D: 1.2992 Loss_G: 0.5072\n",
      "BATCH NUMBER 2547\n",
      "Epoch [2/5] Loss_D: 1.0495 Loss_G: 0.6557\n",
      "BATCH NUMBER 2548\n",
      "Epoch [2/5] Loss_D: 1.1675 Loss_G: 0.5652\n",
      "BATCH NUMBER 2549\n",
      "Epoch [2/5] Loss_D: 1.1310 Loss_G: 0.5966\n",
      "BATCH NUMBER 2550\n",
      "Epoch [2/5] Loss_D: 1.1388 Loss_G: 0.5917\n",
      "BATCH NUMBER 2551\n",
      "Epoch [2/5] Loss_D: 1.1011 Loss_G: 0.6217\n",
      "BATCH NUMBER 2552\n",
      "Epoch [2/5] Loss_D: 1.1253 Loss_G: 0.5942\n",
      "BATCH NUMBER 2553\n",
      "Epoch [2/5] Loss_D: 1.1653 Loss_G: 0.5774\n",
      "BATCH NUMBER 2554\n",
      "Epoch [2/5] Loss_D: 1.2930 Loss_G: 0.5048\n",
      "BATCH NUMBER 2555\n",
      "Epoch [2/5] Loss_D: 1.1702 Loss_G: 0.5637\n",
      "BATCH NUMBER 2556\n",
      "Epoch [2/5] Loss_D: 1.1103 Loss_G: 0.6141\n",
      "BATCH NUMBER 2557\n",
      "Epoch [2/5] Loss_D: 1.2400 Loss_G: 0.5262\n",
      "BATCH NUMBER 2558\n",
      "Epoch [2/5] Loss_D: 1.1430 Loss_G: 0.5836\n",
      "BATCH NUMBER 2559\n",
      "Epoch [2/5] Loss_D: 1.1088 Loss_G: 0.6162\n",
      "BATCH NUMBER 2560\n",
      "Epoch [2/5] Loss_D: 1.2086 Loss_G: 0.5585\n",
      "BATCH NUMBER 2561\n",
      "Epoch [2/5] Loss_D: 1.1642 Loss_G: 0.5865\n",
      "BATCH NUMBER 2562\n",
      "Epoch [2/5] Loss_D: 1.2957 Loss_G: 0.5042\n",
      "BATCH NUMBER 2563\n",
      "Epoch [2/5] Loss_D: 1.2074 Loss_G: 0.5561\n",
      "BATCH NUMBER 2564\n",
      "Epoch [2/5] Loss_D: 1.0889 Loss_G: 0.6239\n",
      "BATCH NUMBER 2565\n",
      "Epoch [2/5] Loss_D: 1.0703 Loss_G: 0.6384\n",
      "BATCH NUMBER 2566\n",
      "Epoch [2/5] Loss_D: 1.1874 Loss_G: 0.5675\n",
      "BATCH NUMBER 2567\n",
      "Epoch [2/5] Loss_D: 1.0863 Loss_G: 0.6294\n",
      "BATCH NUMBER 2568\n",
      "Epoch [2/5] Loss_D: 1.0526 Loss_G: 0.6519\n",
      "BATCH NUMBER 2569\n",
      "Epoch [2/5] Loss_D: 1.0672 Loss_G: 0.6412\n",
      "BATCH NUMBER 2570\n",
      "Epoch [2/5] Loss_D: 1.1030 Loss_G: 0.6141\n",
      "BATCH NUMBER 2571\n",
      "Epoch [2/5] Loss_D: 1.2364 Loss_G: 0.5722\n",
      "BATCH NUMBER 2572\n",
      "Epoch [2/5] Loss_D: 1.0990 Loss_G: 0.6177\n",
      "BATCH NUMBER 2573\n",
      "Epoch [2/5] Loss_D: 1.1017 Loss_G: 0.6234\n",
      "BATCH NUMBER 2574\n",
      "Epoch [2/5] Loss_D: 1.2141 Loss_G: 0.5414\n",
      "BATCH NUMBER 2575\n",
      "Epoch [2/5] Loss_D: 1.0927 Loss_G: 0.6279\n",
      "BATCH NUMBER 2576\n",
      "Epoch [2/5] Loss_D: 1.0992 Loss_G: 0.6174\n",
      "BATCH NUMBER 2577\n",
      "Epoch [2/5] Loss_D: 1.1237 Loss_G: 0.6138\n",
      "BATCH NUMBER 2578\n",
      "Epoch [2/5] Loss_D: 1.0560 Loss_G: 0.6502\n",
      "BATCH NUMBER 2579\n",
      "Epoch [2/5] Loss_D: 1.0547 Loss_G: 0.6525\n",
      "BATCH NUMBER 2580\n",
      "Epoch [2/5] Loss_D: 1.1510 Loss_G: 0.5813\n",
      "BATCH NUMBER 2581\n",
      "Epoch [2/5] Loss_D: 1.2105 Loss_G: 0.5484\n",
      "BATCH NUMBER 2582\n",
      "Epoch [2/5] Loss_D: 1.0941 Loss_G: 0.6193\n",
      "BATCH NUMBER 2583\n",
      "Epoch [2/5] Loss_D: 1.0880 Loss_G: 0.6259\n",
      "BATCH NUMBER 2584\n",
      "Epoch [2/5] Loss_D: 1.2753 Loss_G: 0.5211\n",
      "BATCH NUMBER 2585\n",
      "Epoch [2/5] Loss_D: 1.0662 Loss_G: 0.6433\n",
      "BATCH NUMBER 2586\n",
      "Epoch [2/5] Loss_D: 1.1216 Loss_G: 0.5989\n",
      "BATCH NUMBER 2587\n",
      "Epoch [2/5] Loss_D: 1.2775 Loss_G: 0.5114\n",
      "BATCH NUMBER 2588\n",
      "Epoch [2/5] Loss_D: 1.2185 Loss_G: 0.5620\n",
      "BATCH NUMBER 2589\n",
      "Epoch [2/5] Loss_D: 1.0963 Loss_G: 0.6270\n",
      "BATCH NUMBER 2590\n",
      "Epoch [2/5] Loss_D: 1.0995 Loss_G: 0.6318\n",
      "BATCH NUMBER 2591\n",
      "Epoch [2/5] Loss_D: 1.1067 Loss_G: 0.6099\n",
      "BATCH NUMBER 2592\n",
      "Epoch [2/5] Loss_D: 1.2813 Loss_G: 0.5080\n",
      "BATCH NUMBER 2593\n",
      "Epoch [2/5] Loss_D: 1.1827 Loss_G: 0.5710\n",
      "BATCH NUMBER 2594\n",
      "Epoch [2/5] Loss_D: 1.0815 Loss_G: 0.6347\n",
      "BATCH NUMBER 2595\n",
      "Epoch [2/5] Loss_D: 1.1343 Loss_G: 0.5948\n",
      "BATCH NUMBER 2596\n",
      "Epoch [2/5] Loss_D: 1.1096 Loss_G: 0.6164\n",
      "BATCH NUMBER 2597\n",
      "Epoch [2/5] Loss_D: 1.3234 Loss_G: 0.4793\n",
      "BATCH NUMBER 2598\n",
      "Epoch [2/5] Loss_D: 1.1048 Loss_G: 0.6110\n",
      "BATCH NUMBER 2599\n",
      "Epoch [2/5] Loss_D: 1.1326 Loss_G: 0.5998\n",
      "BATCH NUMBER 2600\n",
      "Epoch [2/5] Loss_D: 1.0741 Loss_G: 0.6359\n",
      "BATCH NUMBER 2601\n",
      "Epoch [2/5] Loss_D: 1.2591 Loss_G: 0.5275\n",
      "BATCH NUMBER 2602\n",
      "Epoch [2/5] Loss_D: 1.1419 Loss_G: 0.5921\n",
      "BATCH NUMBER 2603\n",
      "Epoch [2/5] Loss_D: 1.1213 Loss_G: 0.6046\n",
      "BATCH NUMBER 2604\n",
      "Epoch [2/5] Loss_D: 1.1041 Loss_G: 0.6096\n",
      "BATCH NUMBER 2605\n",
      "Epoch [2/5] Loss_D: 1.1261 Loss_G: 0.6028\n",
      "BATCH NUMBER 2606\n",
      "Epoch [2/5] Loss_D: 1.1752 Loss_G: 0.5695\n",
      "BATCH NUMBER 2607\n",
      "Epoch [2/5] Loss_D: 1.0891 Loss_G: 0.6242\n",
      "BATCH NUMBER 2608\n",
      "Epoch [2/5] Loss_D: 1.0828 Loss_G: 0.6306\n",
      "BATCH NUMBER 2609\n",
      "Epoch [2/5] Loss_D: 1.1095 Loss_G: 0.6144\n",
      "BATCH NUMBER 2610\n",
      "Epoch [2/5] Loss_D: 1.1605 Loss_G: 0.5804\n",
      "BATCH NUMBER 2611\n",
      "Epoch [2/5] Loss_D: 1.1149 Loss_G: 0.6175\n",
      "BATCH NUMBER 2612\n",
      "Epoch [2/5] Loss_D: 1.2760 Loss_G: 0.4935\n",
      "BATCH NUMBER 2613\n",
      "Epoch [2/5] Loss_D: 1.0786 Loss_G: 0.6344\n",
      "BATCH NUMBER 2614\n",
      "Epoch [2/5] Loss_D: 1.1684 Loss_G: 0.5677\n",
      "BATCH NUMBER 2615\n",
      "Epoch [2/5] Loss_D: 1.1200 Loss_G: 0.5985\n",
      "BATCH NUMBER 2616\n",
      "Epoch [2/5] Loss_D: 1.1383 Loss_G: 0.5888\n",
      "BATCH NUMBER 2617\n",
      "Epoch [2/5] Loss_D: 1.0977 Loss_G: 0.6263\n",
      "BATCH NUMBER 2618\n",
      "Epoch [2/5] Loss_D: 1.2004 Loss_G: 0.5716\n",
      "BATCH NUMBER 2619\n",
      "Epoch [2/5] Loss_D: 1.0812 Loss_G: 0.6382\n",
      "BATCH NUMBER 2620\n",
      "Epoch [2/5] Loss_D: 1.0644 Loss_G: 0.6448\n",
      "BATCH NUMBER 2621\n",
      "Epoch [2/5] Loss_D: 1.2032 Loss_G: 0.5535\n",
      "BATCH NUMBER 2622\n",
      "Epoch [2/5] Loss_D: 1.1130 Loss_G: 0.6024\n",
      "BATCH NUMBER 2623\n",
      "Epoch [2/5] Loss_D: 1.1253 Loss_G: 0.6020\n",
      "BATCH NUMBER 2624\n",
      "Epoch [2/5] Loss_D: 1.0684 Loss_G: 0.6410\n",
      "BATCH NUMBER 2625\n",
      "Epoch [2/5] Loss_D: 1.2899 Loss_G: 0.6013\n",
      "BATCH NUMBER 2626\n",
      "Epoch [2/5] Loss_D: 1.1048 Loss_G: 0.6199\n",
      "BATCH NUMBER 2627\n",
      "Epoch [2/5] Loss_D: 1.1659 Loss_G: 0.5773\n",
      "BATCH NUMBER 2628\n",
      "Epoch [2/5] Loss_D: 1.1316 Loss_G: 0.5974\n",
      "BATCH NUMBER 2629\n",
      "Epoch [2/5] Loss_D: 1.1909 Loss_G: 0.5583\n",
      "BATCH NUMBER 2630\n",
      "Epoch [2/5] Loss_D: 1.1146 Loss_G: 0.6241\n",
      "BATCH NUMBER 2631\n",
      "Epoch [2/5] Loss_D: 1.2328 Loss_G: 0.5382\n",
      "BATCH NUMBER 2632\n",
      "Epoch [2/5] Loss_D: 1.1165 Loss_G: 0.6103\n",
      "BATCH NUMBER 2633\n",
      "Epoch [2/5] Loss_D: 1.6266 Loss_G: 0.5133\n",
      "BATCH NUMBER 2634\n",
      "Epoch [2/5] Loss_D: 1.2464 Loss_G: 0.5354\n",
      "BATCH NUMBER 2635\n",
      "Epoch [2/5] Loss_D: 1.0941 Loss_G: 0.6280\n",
      "BATCH NUMBER 2636\n",
      "Epoch [2/5] Loss_D: 1.0938 Loss_G: 0.6197\n",
      "BATCH NUMBER 2637\n",
      "Epoch [2/5] Loss_D: 1.1466 Loss_G: 0.5925\n",
      "BATCH NUMBER 2638\n",
      "Epoch [2/5] Loss_D: 1.1788 Loss_G: 0.5577\n",
      "BATCH NUMBER 2639\n",
      "Epoch [2/5] Loss_D: 1.0919 Loss_G: 0.6212\n",
      "BATCH NUMBER 2640\n",
      "Epoch [2/5] Loss_D: 1.1025 Loss_G: 0.6296\n",
      "BATCH NUMBER 2641\n",
      "Epoch [2/5] Loss_D: 1.2257 Loss_G: 0.5372\n",
      "BATCH NUMBER 2642\n",
      "Epoch [2/5] Loss_D: 1.1222 Loss_G: 0.6045\n",
      "BATCH NUMBER 2643\n",
      "Epoch [2/5] Loss_D: 1.0431 Loss_G: 0.6599\n",
      "BATCH NUMBER 2644\n",
      "Epoch [2/5] Loss_D: 1.2011 Loss_G: 0.5463\n",
      "BATCH NUMBER 2645\n",
      "Epoch [2/5] Loss_D: 1.0885 Loss_G: 0.6242\n",
      "BATCH NUMBER 2646\n",
      "Epoch [2/5] Loss_D: 1.1432 Loss_G: 0.5819\n",
      "BATCH NUMBER 2647\n",
      "Epoch [2/5] Loss_D: 1.1248 Loss_G: 0.6041\n",
      "BATCH NUMBER 2648\n",
      "Epoch [2/5] Loss_D: 1.1149 Loss_G: 0.6066\n",
      "BATCH NUMBER 2649\n",
      "Epoch [2/5] Loss_D: 1.1387 Loss_G: 0.6033\n",
      "BATCH NUMBER 2650\n",
      "Epoch [2/5] Loss_D: 1.3952 Loss_G: 0.4376\n",
      "BATCH NUMBER 2651\n",
      "Epoch [2/5] Loss_D: 1.0990 Loss_G: 0.6180\n",
      "BATCH NUMBER 2652\n",
      "Epoch [2/5] Loss_D: 1.0861 Loss_G: 0.6341\n",
      "BATCH NUMBER 2653\n",
      "Epoch [2/5] Loss_D: 1.1016 Loss_G: 0.6231\n",
      "BATCH NUMBER 2654\n",
      "Epoch [2/5] Loss_D: 1.1020 Loss_G: 0.6122\n",
      "BATCH NUMBER 2655\n",
      "Epoch [2/5] Loss_D: 1.1126 Loss_G: 0.6106\n",
      "BATCH NUMBER 2656\n",
      "Epoch [2/5] Loss_D: 1.1311 Loss_G: 0.6029\n",
      "BATCH NUMBER 2657\n",
      "Epoch [2/5] Loss_D: 1.1556 Loss_G: 0.5776\n",
      "BATCH NUMBER 2658\n",
      "Epoch [2/5] Loss_D: 1.1393 Loss_G: 0.5924\n",
      "BATCH NUMBER 2659\n",
      "Epoch [2/5] Loss_D: 1.0861 Loss_G: 0.6345\n",
      "BATCH NUMBER 2660\n",
      "Epoch [2/5] Loss_D: 1.1074 Loss_G: 0.6160\n",
      "BATCH NUMBER 2661\n",
      "Epoch [2/5] Loss_D: 1.1031 Loss_G: 0.6146\n",
      "BATCH NUMBER 2662\n",
      "Epoch [2/5] Loss_D: 1.1244 Loss_G: 0.6008\n",
      "BATCH NUMBER 2663\n",
      "Epoch [2/5] Loss_D: 1.2462 Loss_G: 0.5314\n",
      "BATCH NUMBER 2664\n",
      "Epoch [2/5] Loss_D: 1.1313 Loss_G: 0.5962\n",
      "BATCH NUMBER 2665\n",
      "Epoch [2/5] Loss_D: 1.2403 Loss_G: 0.5258\n",
      "BATCH NUMBER 2666\n",
      "Epoch [2/5] Loss_D: 1.1092 Loss_G: 0.6158\n",
      "BATCH NUMBER 2667\n",
      "Epoch [2/5] Loss_D: 1.1142 Loss_G: 0.6131\n",
      "BATCH NUMBER 2668\n",
      "Epoch [2/5] Loss_D: 1.1946 Loss_G: 0.5620\n",
      "BATCH NUMBER 2669\n",
      "Epoch [2/5] Loss_D: 1.0985 Loss_G: 0.6149\n",
      "BATCH NUMBER 2670\n",
      "Epoch [2/5] Loss_D: 1.1350 Loss_G: 0.6033\n",
      "BATCH NUMBER 2671\n",
      "Epoch [2/5] Loss_D: 1.0855 Loss_G: 0.6333\n",
      "BATCH NUMBER 2672\n",
      "Epoch [2/5] Loss_D: 1.1582 Loss_G: 0.5813\n",
      "BATCH NUMBER 2673\n",
      "Epoch [2/5] Loss_D: 1.1836 Loss_G: 0.5612\n",
      "BATCH NUMBER 2674\n",
      "Epoch [2/5] Loss_D: 1.0943 Loss_G: 0.6280\n",
      "BATCH NUMBER 2675\n",
      "Epoch [2/5] Loss_D: 1.0778 Loss_G: 0.6405\n",
      "BATCH NUMBER 2676\n",
      "Epoch [2/5] Loss_D: 1.0568 Loss_G: 0.6496\n",
      "BATCH NUMBER 2677\n",
      "Epoch [2/5] Loss_D: 1.2323 Loss_G: 0.5301\n",
      "BATCH NUMBER 2678\n",
      "Epoch [2/5] Loss_D: 1.1637 Loss_G: 0.5806\n",
      "BATCH NUMBER 2679\n",
      "Epoch [2/5] Loss_D: 1.0921 Loss_G: 0.6202\n",
      "BATCH NUMBER 2680\n",
      "Epoch [2/5] Loss_D: 1.1702 Loss_G: 0.5800\n",
      "BATCH NUMBER 2681\n",
      "Epoch [2/5] Loss_D: 1.4712 Loss_G: 0.4339\n",
      "BATCH NUMBER 2682\n",
      "Epoch [2/5] Loss_D: 1.1265 Loss_G: 0.6008\n",
      "BATCH NUMBER 2683\n",
      "Epoch [2/5] Loss_D: 1.1563 Loss_G: 0.5793\n",
      "BATCH NUMBER 2684\n",
      "Epoch [2/5] Loss_D: 1.0872 Loss_G: 0.6218\n",
      "BATCH NUMBER 2685\n",
      "Epoch [2/5] Loss_D: 1.0765 Loss_G: 0.6333\n",
      "BATCH NUMBER 2686\n",
      "Epoch [2/5] Loss_D: 1.1229 Loss_G: 0.6036\n",
      "BATCH NUMBER 2687\n",
      "Epoch [2/5] Loss_D: 1.2626 Loss_G: 0.5139\n",
      "BATCH NUMBER 2688\n",
      "Epoch [2/5] Loss_D: 1.2655 Loss_G: 0.5198\n",
      "BATCH NUMBER 2689\n",
      "Epoch [2/5] Loss_D: 1.1318 Loss_G: 0.5952\n",
      "BATCH NUMBER 2690\n",
      "Epoch [2/5] Loss_D: 1.1326 Loss_G: 0.6044\n",
      "BATCH NUMBER 2691\n",
      "Epoch [2/5] Loss_D: 1.1410 Loss_G: 0.5888\n",
      "BATCH NUMBER 2692\n",
      "Epoch [2/5] Loss_D: 1.2042 Loss_G: 0.5477\n",
      "BATCH NUMBER 2693\n",
      "Epoch [2/5] Loss_D: 1.0972 Loss_G: 0.6236\n",
      "BATCH NUMBER 2694\n",
      "Epoch [2/5] Loss_D: 1.0726 Loss_G: 0.6443\n",
      "BATCH NUMBER 2695\n",
      "Epoch [2/5] Loss_D: 1.0833 Loss_G: 0.6299\n",
      "BATCH NUMBER 2696\n",
      "Epoch [2/5] Loss_D: 1.1312 Loss_G: 0.5975\n",
      "BATCH NUMBER 2697\n",
      "Epoch [2/5] Loss_D: 1.1215 Loss_G: 0.6254\n",
      "BATCH NUMBER 2698\n",
      "Epoch [2/5] Loss_D: 1.1545 Loss_G: 0.5847\n",
      "BATCH NUMBER 2699\n",
      "Epoch [2/5] Loss_D: 1.1392 Loss_G: 0.5909\n",
      "BATCH NUMBER 2700\n",
      "Epoch [2/5] Loss_D: 1.0749 Loss_G: 0.6352\n",
      "BATCH NUMBER 2701\n",
      "Epoch [2/5] Loss_D: 1.0682 Loss_G: 0.6401\n",
      "BATCH NUMBER 2702\n",
      "Epoch [2/5] Loss_D: 1.0500 Loss_G: 0.6548\n",
      "BATCH NUMBER 2703\n",
      "Epoch [2/5] Loss_D: 1.0993 Loss_G: 0.6139\n",
      "BATCH NUMBER 2704\n",
      "Epoch [2/5] Loss_D: 1.1409 Loss_G: 0.6206\n",
      "BATCH NUMBER 2705\n",
      "Epoch [2/5] Loss_D: 1.0617 Loss_G: 0.6465\n",
      "BATCH NUMBER 2706\n",
      "Epoch [2/5] Loss_D: 1.0601 Loss_G: 0.6462\n",
      "BATCH NUMBER 2707\n",
      "Epoch [2/5] Loss_D: 1.0922 Loss_G: 0.6191\n",
      "BATCH NUMBER 2708\n",
      "Epoch [2/5] Loss_D: 1.1349 Loss_G: 0.6020\n",
      "BATCH NUMBER 2709\n",
      "Epoch [2/5] Loss_D: 1.3372 Loss_G: 0.4772\n",
      "BATCH NUMBER 2710\n",
      "Epoch [2/5] Loss_D: 1.0979 Loss_G: 0.6238\n",
      "BATCH NUMBER 2711\n",
      "Epoch [2/5] Loss_D: 1.1637 Loss_G: 0.5783\n",
      "BATCH NUMBER 2712\n",
      "Epoch [2/5] Loss_D: 1.2246 Loss_G: 0.5447\n",
      "BATCH NUMBER 2713\n",
      "Epoch [2/5] Loss_D: 1.1615 Loss_G: 0.5722\n",
      "BATCH NUMBER 2714\n",
      "Epoch [2/5] Loss_D: 1.2709 Loss_G: 0.5223\n",
      "BATCH NUMBER 2715\n",
      "Epoch [2/5] Loss_D: 1.0420 Loss_G: 0.6625\n",
      "BATCH NUMBER 2716\n",
      "Epoch [2/5] Loss_D: 1.1477 Loss_G: 0.5860\n",
      "BATCH NUMBER 2717\n",
      "Epoch [2/5] Loss_D: 1.0806 Loss_G: 0.6302\n",
      "BATCH NUMBER 2718\n",
      "Epoch [2/5] Loss_D: 1.0686 Loss_G: 0.6413\n",
      "BATCH NUMBER 2719\n",
      "Epoch [2/5] Loss_D: 1.0849 Loss_G: 0.6358\n",
      "BATCH NUMBER 2720\n",
      "Epoch [2/5] Loss_D: 1.1474 Loss_G: 0.5948\n",
      "BATCH NUMBER 2721\n",
      "Epoch [2/5] Loss_D: 1.1287 Loss_G: 0.5997\n",
      "BATCH NUMBER 2722\n",
      "Epoch [2/5] Loss_D: 1.1806 Loss_G: 0.5722\n",
      "BATCH NUMBER 2723\n",
      "Epoch [2/5] Loss_D: 1.1402 Loss_G: 0.5926\n",
      "BATCH NUMBER 2724\n",
      "Epoch [2/5] Loss_D: 1.2485 Loss_G: 0.5256\n",
      "BATCH NUMBER 2725\n",
      "Epoch [2/5] Loss_D: 1.1996 Loss_G: 0.5745\n",
      "BATCH NUMBER 2726\n",
      "Epoch [2/5] Loss_D: 1.1835 Loss_G: 0.5509\n",
      "BATCH NUMBER 2727\n",
      "Epoch [2/5] Loss_D: 1.0565 Loss_G: 0.6509\n",
      "BATCH NUMBER 2728\n",
      "Epoch [2/5] Loss_D: 1.1292 Loss_G: 0.5984\n",
      "BATCH NUMBER 2729\n",
      "Epoch [2/5] Loss_D: 1.1686 Loss_G: 0.5731\n",
      "BATCH NUMBER 2730\n",
      "Epoch [2/5] Loss_D: 1.1161 Loss_G: 0.6184\n",
      "BATCH NUMBER 2731\n",
      "Epoch [2/5] Loss_D: 1.1031 Loss_G: 0.6264\n",
      "BATCH NUMBER 2732\n",
      "Epoch [2/5] Loss_D: 1.2665 Loss_G: 0.5199\n",
      "BATCH NUMBER 2733\n",
      "Epoch [2/5] Loss_D: 1.1573 Loss_G: 0.5975\n",
      "BATCH NUMBER 2734\n",
      "Epoch [2/5] Loss_D: 1.2020 Loss_G: 0.5635\n",
      "BATCH NUMBER 2735\n",
      "Epoch [2/5] Loss_D: 1.0964 Loss_G: 0.6261\n",
      "BATCH NUMBER 2736\n",
      "Epoch [2/5] Loss_D: 1.1691 Loss_G: 0.5713\n",
      "BATCH NUMBER 2737\n",
      "Epoch [2/5] Loss_D: 1.1155 Loss_G: 0.6022\n",
      "BATCH NUMBER 2738\n",
      "Epoch [2/5] Loss_D: 1.1249 Loss_G: 0.6000\n",
      "BATCH NUMBER 2739\n",
      "Epoch [2/5] Loss_D: 1.1664 Loss_G: 0.5773\n",
      "BATCH NUMBER 2740\n",
      "Epoch [2/5] Loss_D: 1.1782 Loss_G: 0.5653\n",
      "BATCH NUMBER 2741\n",
      "Epoch [2/5] Loss_D: 1.1212 Loss_G: 0.6056\n",
      "BATCH NUMBER 2742\n",
      "Epoch [2/5] Loss_D: 1.1730 Loss_G: 0.5622\n",
      "BATCH NUMBER 2743\n",
      "Epoch [2/5] Loss_D: 1.1112 Loss_G: 0.6123\n",
      "BATCH NUMBER 2744\n",
      "Epoch [2/5] Loss_D: 1.1364 Loss_G: 0.6011\n",
      "BATCH NUMBER 2745\n",
      "Epoch [2/5] Loss_D: 1.2045 Loss_G: 0.5539\n",
      "BATCH NUMBER 2746\n",
      "Epoch [2/5] Loss_D: 1.1651 Loss_G: 0.6083\n",
      "BATCH NUMBER 2747\n",
      "Epoch [2/5] Loss_D: 1.0690 Loss_G: 0.6412\n",
      "BATCH NUMBER 2748\n",
      "Epoch [2/5] Loss_D: 1.1434 Loss_G: 0.5935\n",
      "BATCH NUMBER 2749\n",
      "Epoch [2/5] Loss_D: 1.1028 Loss_G: 0.6142\n",
      "BATCH NUMBER 2750\n",
      "Epoch [2/5] Loss_D: 1.0382 Loss_G: 0.6660\n",
      "BATCH NUMBER 2751\n",
      "Epoch [2/5] Loss_D: 1.3163 Loss_G: 0.4853\n",
      "BATCH NUMBER 2752\n",
      "Epoch [2/5] Loss_D: 1.1259 Loss_G: 0.6002\n",
      "BATCH NUMBER 2753\n",
      "Epoch [2/5] Loss_D: 1.1313 Loss_G: 0.6066\n",
      "BATCH NUMBER 2754\n",
      "Epoch [2/5] Loss_D: 1.1390 Loss_G: 0.5916\n",
      "BATCH NUMBER 2755\n",
      "Epoch [2/5] Loss_D: 1.1214 Loss_G: 0.6121\n",
      "BATCH NUMBER 2756\n",
      "Epoch [2/5] Loss_D: 1.1164 Loss_G: 0.6080\n",
      "BATCH NUMBER 2757\n",
      "Epoch [2/5] Loss_D: 1.2339 Loss_G: 0.5457\n",
      "BATCH NUMBER 2758\n",
      "Epoch [2/5] Loss_D: 1.1497 Loss_G: 0.5981\n",
      "BATCH NUMBER 2759\n",
      "Epoch [2/5] Loss_D: 1.1731 Loss_G: 0.5797\n",
      "BATCH NUMBER 2760\n",
      "Epoch [2/5] Loss_D: 1.0790 Loss_G: 0.6318\n",
      "BATCH NUMBER 2761\n",
      "Epoch [2/5] Loss_D: 1.1105 Loss_G: 0.6076\n",
      "BATCH NUMBER 2762\n",
      "Epoch [2/5] Loss_D: 1.1654 Loss_G: 0.5770\n",
      "BATCH NUMBER 2763\n",
      "Epoch [2/5] Loss_D: 1.1974 Loss_G: 0.5599\n",
      "BATCH NUMBER 2764\n",
      "Epoch [2/5] Loss_D: 1.2379 Loss_G: 0.5417\n",
      "BATCH NUMBER 2765\n",
      "Epoch [2/5] Loss_D: 1.1985 Loss_G: 0.5595\n",
      "BATCH NUMBER 2766\n",
      "Epoch [2/5] Loss_D: 1.1311 Loss_G: 0.6062\n",
      "BATCH NUMBER 2767\n",
      "Epoch [2/5] Loss_D: 1.0811 Loss_G: 0.6299\n",
      "BATCH NUMBER 2768\n",
      "Epoch [2/5] Loss_D: 1.2262 Loss_G: 0.5533\n",
      "BATCH NUMBER 2769\n",
      "Epoch [2/5] Loss_D: 1.2124 Loss_G: 0.5497\n",
      "BATCH NUMBER 2770\n",
      "Epoch [2/5] Loss_D: 1.1955 Loss_G: 0.5688\n",
      "BATCH NUMBER 2771\n",
      "Epoch [2/5] Loss_D: 1.2159 Loss_G: 0.5510\n",
      "BATCH NUMBER 2772\n",
      "Epoch [2/5] Loss_D: 1.0631 Loss_G: 0.6532\n",
      "BATCH NUMBER 2773\n",
      "Epoch [2/5] Loss_D: 1.1084 Loss_G: 0.6163\n",
      "BATCH NUMBER 2774\n",
      "Epoch [2/5] Loss_D: 1.0787 Loss_G: 0.6318\n",
      "BATCH NUMBER 2775\n",
      "Epoch [2/5] Loss_D: 1.1212 Loss_G: 0.6022\n",
      "BATCH NUMBER 2776\n",
      "Epoch [2/5] Loss_D: 1.0358 Loss_G: 0.6659\n",
      "BATCH NUMBER 2777\n",
      "Epoch [2/5] Loss_D: 1.1801 Loss_G: 0.5786\n",
      "BATCH NUMBER 2778\n",
      "Epoch [2/5] Loss_D: 1.0383 Loss_G: 0.6645\n",
      "BATCH NUMBER 2779\n",
      "Epoch [2/5] Loss_D: 1.2965 Loss_G: 0.4885\n",
      "BATCH NUMBER 2780\n",
      "Epoch [2/5] Loss_D: 1.1167 Loss_G: 0.6180\n",
      "BATCH NUMBER 2781\n",
      "Epoch [2/5] Loss_D: 1.0858 Loss_G: 0.6266\n",
      "BATCH NUMBER 2782\n",
      "Epoch [2/5] Loss_D: 1.2192 Loss_G: 0.5495\n",
      "BATCH NUMBER 2783\n",
      "Epoch [2/5] Loss_D: 1.1940 Loss_G: 0.5550\n",
      "BATCH NUMBER 2784\n",
      "Epoch [2/5] Loss_D: 1.0522 Loss_G: 0.6553\n",
      "BATCH NUMBER 2785\n",
      "Epoch [2/5] Loss_D: 1.0625 Loss_G: 0.6447\n",
      "BATCH NUMBER 2786\n",
      "Epoch [2/5] Loss_D: 1.0832 Loss_G: 0.6274\n",
      "BATCH NUMBER 2787\n",
      "Epoch [2/5] Loss_D: 1.1437 Loss_G: 0.5952\n",
      "BATCH NUMBER 2788\n",
      "Epoch [2/5] Loss_D: 1.1201 Loss_G: 0.6080\n",
      "BATCH NUMBER 2789\n",
      "Epoch [2/5] Loss_D: 1.1328 Loss_G: 0.5954\n",
      "BATCH NUMBER 2790\n",
      "Epoch [2/5] Loss_D: 1.0570 Loss_G: 0.6496\n",
      "BATCH NUMBER 2791\n",
      "Epoch [2/5] Loss_D: 1.1205 Loss_G: 0.6003\n",
      "BATCH NUMBER 2792\n",
      "Epoch [2/5] Loss_D: 1.1291 Loss_G: 0.5975\n",
      "BATCH NUMBER 2793\n",
      "Epoch [2/5] Loss_D: 1.1024 Loss_G: 0.6210\n",
      "BATCH NUMBER 2794\n",
      "Epoch [2/5] Loss_D: 1.2663 Loss_G: 0.5183\n",
      "BATCH NUMBER 2795\n",
      "Epoch [2/5] Loss_D: 1.1671 Loss_G: 0.5747\n",
      "BATCH NUMBER 2796\n",
      "Epoch [2/5] Loss_D: 1.1176 Loss_G: 0.6079\n",
      "BATCH NUMBER 2797\n",
      "Epoch [2/5] Loss_D: 1.1089 Loss_G: 0.6145\n",
      "BATCH NUMBER 2798\n",
      "Epoch [2/5] Loss_D: 1.2618 Loss_G: 0.5225\n",
      "BATCH NUMBER 2799\n",
      "Epoch [2/5] Loss_D: 1.2347 Loss_G: 0.6205\n",
      "BATCH NUMBER 2800\n",
      "Epoch [2/5] Loss_D: 1.1627 Loss_G: 0.5811\n",
      "BATCH NUMBER 2801\n",
      "Epoch [2/5] Loss_D: 1.1495 Loss_G: 0.5832\n",
      "BATCH NUMBER 2802\n",
      "Epoch [2/5] Loss_D: 1.1051 Loss_G: 0.6131\n",
      "BATCH NUMBER 2803\n",
      "Epoch [2/5] Loss_D: 1.0328 Loss_G: 0.6690\n",
      "BATCH NUMBER 2804\n",
      "Epoch [2/5] Loss_D: 1.1500 Loss_G: 0.5921\n",
      "BATCH NUMBER 2805\n",
      "Epoch [2/5] Loss_D: 1.2035 Loss_G: 0.5548\n",
      "BATCH NUMBER 2806\n",
      "Epoch [2/5] Loss_D: 1.2998 Loss_G: 0.6468\n",
      "BATCH NUMBER 2807\n",
      "Epoch [2/5] Loss_D: 1.1364 Loss_G: 0.5909\n",
      "BATCH NUMBER 2808\n",
      "Epoch [2/5] Loss_D: 1.1499 Loss_G: 0.5851\n",
      "BATCH NUMBER 2809\n",
      "Epoch [2/5] Loss_D: 1.1981 Loss_G: 0.5828\n",
      "BATCH NUMBER 2810\n",
      "Epoch [2/5] Loss_D: 1.1366 Loss_G: 0.5932\n",
      "BATCH NUMBER 2811\n",
      "Epoch [2/5] Loss_D: 1.1831 Loss_G: 0.5627\n",
      "BATCH NUMBER 2812\n",
      "Epoch [2/5] Loss_D: 1.1202 Loss_G: 0.5988\n",
      "BATCH NUMBER 2813\n",
      "Epoch [2/5] Loss_D: 1.1130 Loss_G: 0.6036\n",
      "BATCH NUMBER 2814\n",
      "Epoch [2/5] Loss_D: 1.2328 Loss_G: 0.5453\n",
      "BATCH NUMBER 2815\n",
      "Epoch [2/5] Loss_D: 1.1017 Loss_G: 0.6098\n",
      "BATCH NUMBER 2816\n",
      "Epoch [2/5] Loss_D: 1.0575 Loss_G: 0.6495\n",
      "BATCH NUMBER 2817\n",
      "Epoch [2/5] Loss_D: 1.0534 Loss_G: 0.6506\n",
      "BATCH NUMBER 2818\n",
      "Epoch [2/5] Loss_D: 1.0824 Loss_G: 0.6312\n",
      "BATCH NUMBER 2819\n",
      "Epoch [2/5] Loss_D: 1.0667 Loss_G: 0.6419\n",
      "BATCH NUMBER 2820\n",
      "Epoch [2/5] Loss_D: 1.3416 Loss_G: 0.4740\n",
      "BATCH NUMBER 2821\n",
      "Epoch [2/5] Loss_D: 1.1344 Loss_G: 0.5933\n",
      "BATCH NUMBER 2822\n",
      "Epoch [2/5] Loss_D: 1.1509 Loss_G: 0.5905\n",
      "BATCH NUMBER 2823\n",
      "Epoch [2/5] Loss_D: 1.1236 Loss_G: 0.6115\n",
      "BATCH NUMBER 2824\n",
      "Epoch [2/5] Loss_D: 1.1900 Loss_G: 0.5653\n",
      "BATCH NUMBER 2825\n",
      "Epoch [2/5] Loss_D: 1.1377 Loss_G: 0.6002\n",
      "BATCH NUMBER 2826\n",
      "Epoch [2/5] Loss_D: 1.3593 Loss_G: 0.4590\n",
      "BATCH NUMBER 2827\n",
      "Epoch [2/5] Loss_D: 1.2768 Loss_G: 0.5145\n",
      "BATCH NUMBER 2828\n",
      "Epoch [2/5] Loss_D: 1.0735 Loss_G: 0.6445\n",
      "BATCH NUMBER 2829\n",
      "Epoch [2/5] Loss_D: 1.2105 Loss_G: 0.5579\n",
      "BATCH NUMBER 2830\n",
      "Epoch [2/5] Loss_D: 1.1230 Loss_G: 0.5947\n",
      "BATCH NUMBER 2831\n",
      "Epoch [2/5] Loss_D: 1.1284 Loss_G: 0.6108\n",
      "BATCH NUMBER 2832\n",
      "Epoch [2/5] Loss_D: 1.1221 Loss_G: 0.6043\n",
      "BATCH NUMBER 2833\n",
      "Epoch [2/5] Loss_D: 1.0681 Loss_G: 0.6405\n",
      "BATCH NUMBER 2834\n",
      "Epoch [2/5] Loss_D: 1.2734 Loss_G: 0.5140\n",
      "BATCH NUMBER 2835\n",
      "Epoch [2/5] Loss_D: 1.1029 Loss_G: 0.6123\n",
      "BATCH NUMBER 2836\n",
      "Epoch [2/5] Loss_D: 1.1438 Loss_G: 0.5888\n",
      "BATCH NUMBER 2837\n",
      "Epoch [2/5] Loss_D: 1.2093 Loss_G: 0.5522\n",
      "BATCH NUMBER 2838\n",
      "Epoch [2/5] Loss_D: 1.1218 Loss_G: 0.6046\n",
      "BATCH NUMBER 2839\n",
      "Epoch [2/5] Loss_D: 1.0765 Loss_G: 0.6315\n",
      "BATCH NUMBER 2840\n",
      "Epoch [2/5] Loss_D: 1.1666 Loss_G: 0.5685\n",
      "BATCH NUMBER 2841\n",
      "Epoch [2/5] Loss_D: 1.0460 Loss_G: 0.6584\n",
      "BATCH NUMBER 2842\n",
      "Epoch [2/5] Loss_D: 1.1058 Loss_G: 0.6126\n",
      "BATCH NUMBER 2843\n",
      "Epoch [2/5] Loss_D: 1.1163 Loss_G: 0.6081\n",
      "BATCH NUMBER 2844\n",
      "Epoch [2/5] Loss_D: 1.0573 Loss_G: 0.6494\n",
      "BATCH NUMBER 2845\n",
      "Epoch [2/5] Loss_D: 1.1321 Loss_G: 0.6057\n",
      "BATCH NUMBER 2846\n",
      "Epoch [2/5] Loss_D: 1.0991 Loss_G: 0.6151\n",
      "BATCH NUMBER 2847\n",
      "Epoch [2/5] Loss_D: 1.0510 Loss_G: 0.6570\n",
      "BATCH NUMBER 2848\n",
      "Epoch [2/5] Loss_D: 1.3169 Loss_G: 0.4931\n",
      "BATCH NUMBER 2849\n",
      "Epoch [2/5] Loss_D: 1.0558 Loss_G: 0.6651\n",
      "BATCH NUMBER 2850\n",
      "Epoch [2/5] Loss_D: 1.1879 Loss_G: 0.5582\n",
      "BATCH NUMBER 2851\n",
      "Epoch [2/5] Loss_D: 1.2265 Loss_G: 0.5396\n",
      "BATCH NUMBER 2852\n",
      "Epoch [2/5] Loss_D: 1.0929 Loss_G: 0.6279\n",
      "BATCH NUMBER 2853\n",
      "Epoch [2/5] Loss_D: 1.0648 Loss_G: 0.6431\n",
      "BATCH NUMBER 2854\n",
      "Epoch [2/5] Loss_D: 1.3065 Loss_G: 0.4931\n",
      "BATCH NUMBER 2855\n",
      "Epoch [2/5] Loss_D: 1.1504 Loss_G: 0.5893\n",
      "BATCH NUMBER 2856\n",
      "Epoch [2/5] Loss_D: 1.2211 Loss_G: 0.6464\n",
      "BATCH NUMBER 2857\n",
      "Epoch [2/5] Loss_D: 1.1252 Loss_G: 0.5996\n",
      "BATCH NUMBER 2858\n",
      "Epoch [2/5] Loss_D: 1.0756 Loss_G: 0.6421\n",
      "BATCH NUMBER 2859\n",
      "Epoch [2/5] Loss_D: 1.0349 Loss_G: 0.6681\n",
      "BATCH NUMBER 2860\n",
      "Epoch [2/5] Loss_D: 1.0969 Loss_G: 0.6147\n",
      "BATCH NUMBER 2861\n",
      "Epoch [2/5] Loss_D: 1.1836 Loss_G: 0.5633\n",
      "BATCH NUMBER 2862\n",
      "Epoch [2/5] Loss_D: 1.0774 Loss_G: 0.6328\n",
      "BATCH NUMBER 2863\n",
      "Epoch [2/5] Loss_D: 1.1728 Loss_G: 0.5788\n",
      "BATCH NUMBER 2864\n",
      "Epoch [2/5] Loss_D: 1.0462 Loss_G: 0.6574\n",
      "BATCH NUMBER 2865\n",
      "Epoch [2/5] Loss_D: 1.2874 Loss_G: 0.5051\n",
      "BATCH NUMBER 2866\n",
      "Epoch [2/5] Loss_D: 1.1363 Loss_G: 0.6002\n",
      "BATCH NUMBER 2867\n",
      "Epoch [2/5] Loss_D: 1.1287 Loss_G: 0.6068\n",
      "BATCH NUMBER 2868\n",
      "Epoch [2/5] Loss_D: 1.0938 Loss_G: 0.6255\n",
      "BATCH NUMBER 2869\n",
      "Epoch [2/5] Loss_D: 1.2554 Loss_G: 0.5173\n",
      "BATCH NUMBER 2870\n",
      "Epoch [2/5] Loss_D: 1.1663 Loss_G: 0.5676\n",
      "BATCH NUMBER 2871\n",
      "Epoch [2/5] Loss_D: 1.0978 Loss_G: 0.6153\n",
      "BATCH NUMBER 2872\n",
      "Epoch [2/5] Loss_D: 1.1487 Loss_G: 0.5903\n",
      "BATCH NUMBER 2873\n",
      "Epoch [2/5] Loss_D: 1.0396 Loss_G: 0.6642\n",
      "BATCH NUMBER 2874\n",
      "Epoch [2/5] Loss_D: 1.0676 Loss_G: 0.6418\n",
      "BATCH NUMBER 2875\n",
      "Epoch [2/5] Loss_D: 1.1206 Loss_G: 0.6127\n",
      "BATCH NUMBER 2876\n",
      "Epoch [2/5] Loss_D: 1.0787 Loss_G: 0.6327\n",
      "BATCH NUMBER 2877\n",
      "Epoch [2/5] Loss_D: 1.1636 Loss_G: 0.5788\n",
      "BATCH NUMBER 2878\n",
      "Epoch [2/5] Loss_D: 1.1639 Loss_G: 0.5859\n",
      "BATCH NUMBER 2879\n",
      "Epoch [2/5] Loss_D: 1.0317 Loss_G: 0.6722\n",
      "BATCH NUMBER 2880\n",
      "Epoch [2/5] Loss_D: 1.1935 Loss_G: 0.5606\n",
      "BATCH NUMBER 2881\n",
      "Epoch [2/5] Loss_D: 1.1183 Loss_G: 0.6043\n",
      "BATCH NUMBER 2882\n",
      "Epoch [2/5] Loss_D: 1.0516 Loss_G: 0.6542\n",
      "BATCH NUMBER 2883\n",
      "Epoch [2/5] Loss_D: 1.1320 Loss_G: 0.5978\n",
      "BATCH NUMBER 2884\n",
      "Epoch [2/5] Loss_D: 1.1309 Loss_G: 0.6048\n",
      "BATCH NUMBER 2885\n",
      "Epoch [2/5] Loss_D: 1.0708 Loss_G: 0.6380\n",
      "BATCH NUMBER 2886\n",
      "Epoch [2/5] Loss_D: 1.0977 Loss_G: 0.6177\n",
      "BATCH NUMBER 2887\n",
      "Epoch [2/5] Loss_D: 1.0598 Loss_G: 0.6462\n",
      "BATCH NUMBER 2888\n",
      "Epoch [2/5] Loss_D: 1.0985 Loss_G: 0.6137\n",
      "BATCH NUMBER 2889\n",
      "Epoch [2/5] Loss_D: 1.1301 Loss_G: 0.6058\n",
      "BATCH NUMBER 2890\n",
      "Epoch [2/5] Loss_D: 1.0985 Loss_G: 0.6222\n",
      "BATCH NUMBER 2891\n",
      "Epoch [2/5] Loss_D: 1.0611 Loss_G: 0.6467\n",
      "BATCH NUMBER 2892\n",
      "Epoch [2/5] Loss_D: 1.1568 Loss_G: 0.5822\n",
      "BATCH NUMBER 2893\n",
      "Epoch [2/5] Loss_D: 1.2557 Loss_G: 0.5229\n",
      "BATCH NUMBER 2894\n",
      "Epoch [2/5] Loss_D: 1.0786 Loss_G: 0.6290\n",
      "BATCH NUMBER 2895\n",
      "Epoch [2/5] Loss_D: 1.2330 Loss_G: 0.5462\n",
      "BATCH NUMBER 2896\n",
      "Epoch [2/5] Loss_D: 1.1510 Loss_G: 0.5820\n",
      "BATCH NUMBER 2897\n",
      "Epoch [2/5] Loss_D: 1.1482 Loss_G: 0.5854\n",
      "BATCH NUMBER 2898\n",
      "Epoch [2/5] Loss_D: 1.1309 Loss_G: 0.5889\n",
      "BATCH NUMBER 2899\n",
      "Epoch [2/5] Loss_D: 1.0817 Loss_G: 0.6302\n",
      "BATCH NUMBER 2900\n",
      "Epoch [2/5] Loss_D: 1.1518 Loss_G: 0.5912\n",
      "BATCH NUMBER 2901\n",
      "Epoch [2/5] Loss_D: 1.4064 Loss_G: 0.4327\n",
      "BATCH NUMBER 2902\n",
      "Epoch [2/5] Loss_D: 1.0561 Loss_G: 0.6514\n",
      "BATCH NUMBER 2903\n",
      "Epoch [2/5] Loss_D: 1.1418 Loss_G: 0.5879\n",
      "BATCH NUMBER 2904\n",
      "Epoch [2/5] Loss_D: 1.1199 Loss_G: 0.6066\n",
      "BATCH NUMBER 2905\n",
      "Epoch [2/5] Loss_D: 1.1312 Loss_G: 0.5957\n",
      "BATCH NUMBER 2906\n",
      "Epoch [2/5] Loss_D: 1.2214 Loss_G: 0.5390\n",
      "BATCH NUMBER 2907\n",
      "Epoch [2/5] Loss_D: 1.1845 Loss_G: 0.5706\n",
      "BATCH NUMBER 2908\n",
      "Epoch [2/5] Loss_D: 1.1077 Loss_G: 0.6163\n",
      "BATCH NUMBER 2909\n",
      "Epoch [2/5] Loss_D: 1.0769 Loss_G: 0.6343\n",
      "BATCH NUMBER 2910\n",
      "Epoch [2/5] Loss_D: 1.1541 Loss_G: 0.5855\n",
      "BATCH NUMBER 2911\n",
      "Epoch [2/5] Loss_D: 1.0717 Loss_G: 0.6373\n",
      "BATCH NUMBER 2912\n",
      "Epoch [2/5] Loss_D: 1.1107 Loss_G: 0.6041\n",
      "BATCH NUMBER 2913\n",
      "Epoch [2/5] Loss_D: 1.2083 Loss_G: 0.5415\n",
      "BATCH NUMBER 2914\n",
      "Epoch [2/5] Loss_D: 1.0969 Loss_G: 0.6153\n",
      "BATCH NUMBER 2915\n",
      "Epoch [2/5] Loss_D: 1.1701 Loss_G: 0.5662\n",
      "BATCH NUMBER 2916\n",
      "Epoch [2/5] Loss_D: 1.1894 Loss_G: 0.5572\n",
      "BATCH NUMBER 2917\n",
      "Epoch [2/5] Loss_D: 1.0745 Loss_G: 0.6364\n",
      "BATCH NUMBER 2918\n",
      "Epoch [2/5] Loss_D: 1.0625 Loss_G: 0.6456\n",
      "BATCH NUMBER 2919\n",
      "Epoch [2/5] Loss_D: 1.0969 Loss_G: 0.6183\n",
      "BATCH NUMBER 2920\n",
      "Epoch [2/5] Loss_D: 1.0570 Loss_G: 0.6581\n",
      "BATCH NUMBER 2921\n",
      "Epoch [2/5] Loss_D: 1.1524 Loss_G: 0.5854\n",
      "BATCH NUMBER 2922\n",
      "Epoch [2/5] Loss_D: 1.0532 Loss_G: 0.6536\n",
      "BATCH NUMBER 2923\n",
      "Epoch [2/5] Loss_D: 1.1432 Loss_G: 0.5961\n",
      "BATCH NUMBER 2924\n",
      "Epoch [2/5] Loss_D: 1.1251 Loss_G: 0.5964\n",
      "BATCH NUMBER 2925\n",
      "Epoch [2/5] Loss_D: 1.0774 Loss_G: 0.6408\n",
      "BATCH NUMBER 2926\n",
      "Epoch [2/5] Loss_D: 1.0196 Loss_G: 0.6811\n",
      "BATCH NUMBER 2927\n",
      "Epoch [2/5] Loss_D: 1.1655 Loss_G: 0.5768\n",
      "BATCH NUMBER 2928\n",
      "Epoch [2/5] Loss_D: 1.1241 Loss_G: 0.5939\n",
      "BATCH NUMBER 2929\n",
      "Epoch [2/5] Loss_D: 1.0506 Loss_G: 0.6536\n",
      "BATCH NUMBER 2930\n",
      "Epoch [2/5] Loss_D: 1.0493 Loss_G: 0.6562\n",
      "BATCH NUMBER 2931\n",
      "Epoch [2/5] Loss_D: 1.1617 Loss_G: 0.5798\n",
      "BATCH NUMBER 2932\n",
      "Epoch [2/5] Loss_D: 1.0869 Loss_G: 0.6227\n",
      "BATCH NUMBER 2933\n",
      "Epoch [2/5] Loss_D: 1.0774 Loss_G: 0.6402\n",
      "BATCH NUMBER 2934\n",
      "Epoch [2/5] Loss_D: 1.1408 Loss_G: 0.5864\n",
      "BATCH NUMBER 2935\n",
      "Epoch [2/5] Loss_D: 1.1471 Loss_G: 0.5827\n",
      "BATCH NUMBER 2936\n",
      "Epoch [2/5] Loss_D: 1.2217 Loss_G: 0.5467\n",
      "BATCH NUMBER 2937\n",
      "Epoch [2/5] Loss_D: 1.1688 Loss_G: 0.5829\n",
      "BATCH NUMBER 2938\n",
      "Epoch [2/5] Loss_D: 1.0691 Loss_G: 0.6427\n",
      "BATCH NUMBER 2939\n",
      "Epoch [2/5] Loss_D: 1.1297 Loss_G: 0.6068\n",
      "BATCH NUMBER 2940\n",
      "Epoch [2/5] Loss_D: 1.0682 Loss_G: 0.6402\n",
      "BATCH NUMBER 2941\n",
      "Epoch [2/5] Loss_D: 1.0345 Loss_G: 0.6682\n",
      "BATCH NUMBER 2942\n",
      "Epoch [2/5] Loss_D: 1.1000 Loss_G: 0.6146\n",
      "BATCH NUMBER 2943\n",
      "Epoch [2/5] Loss_D: 1.0284 Loss_G: 0.6736\n",
      "BATCH NUMBER 2944\n",
      "Epoch [2/5] Loss_D: 1.1546 Loss_G: 0.5798\n",
      "BATCH NUMBER 2945\n",
      "Epoch [2/5] Loss_D: 1.0211 Loss_G: 0.6798\n",
      "BATCH NUMBER 2946\n",
      "Epoch [2/5] Loss_D: 1.1136 Loss_G: 0.6041\n",
      "BATCH NUMBER 2947\n",
      "Epoch [2/5] Loss_D: 1.1342 Loss_G: 0.6357\n",
      "BATCH NUMBER 2948\n",
      "Epoch [2/5] Loss_D: 1.1281 Loss_G: 0.6017\n",
      "BATCH NUMBER 2949\n",
      "Epoch [2/5] Loss_D: 1.0769 Loss_G: 0.6354\n",
      "BATCH NUMBER 2950\n",
      "Epoch [2/5] Loss_D: 1.2386 Loss_G: 0.5268\n",
      "BATCH NUMBER 2951\n",
      "Epoch [2/5] Loss_D: 1.0632 Loss_G: 0.6492\n",
      "BATCH NUMBER 2952\n",
      "Epoch [2/5] Loss_D: 1.1104 Loss_G: 0.6068\n",
      "BATCH NUMBER 2953\n",
      "Epoch [2/5] Loss_D: 1.0769 Loss_G: 0.6336\n",
      "BATCH NUMBER 2954\n",
      "Epoch [2/5] Loss_D: 1.0936 Loss_G: 0.6193\n",
      "BATCH NUMBER 2955\n",
      "Epoch [2/5] Loss_D: 1.1513 Loss_G: 0.5892\n",
      "BATCH NUMBER 2956\n",
      "Epoch [2/5] Loss_D: 1.1455 Loss_G: 0.6023\n",
      "BATCH NUMBER 2957\n",
      "Epoch [2/5] Loss_D: 1.0683 Loss_G: 0.6405\n",
      "BATCH NUMBER 2958\n",
      "Epoch [2/5] Loss_D: 1.0953 Loss_G: 0.6266\n",
      "BATCH NUMBER 2959\n",
      "Epoch [2/5] Loss_D: 1.0904 Loss_G: 0.6232\n",
      "BATCH NUMBER 2960\n",
      "Epoch [2/5] Loss_D: 1.1722 Loss_G: 0.5799\n",
      "BATCH NUMBER 2961\n",
      "Epoch [2/5] Loss_D: 1.0746 Loss_G: 0.6346\n",
      "BATCH NUMBER 2962\n",
      "Epoch [2/5] Loss_D: 1.1404 Loss_G: 0.5967\n",
      "BATCH NUMBER 2963\n",
      "Epoch [2/5] Loss_D: 1.0623 Loss_G: 0.6446\n",
      "BATCH NUMBER 2964\n",
      "Epoch [2/5] Loss_D: 1.1010 Loss_G: 0.6230\n",
      "BATCH NUMBER 2965\n",
      "Epoch [2/5] Loss_D: 1.0424 Loss_G: 0.6614\n",
      "BATCH NUMBER 2966\n",
      "Epoch [2/5] Loss_D: 1.1784 Loss_G: 0.5751\n",
      "BATCH NUMBER 2967\n",
      "Epoch [2/5] Loss_D: 1.0995 Loss_G: 0.6166\n",
      "BATCH NUMBER 2968\n",
      "Epoch [2/5] Loss_D: 1.4041 Loss_G: 0.4386\n",
      "BATCH NUMBER 2969\n",
      "Epoch [2/5] Loss_D: 1.0755 Loss_G: 0.6343\n",
      "BATCH NUMBER 2970\n",
      "Epoch [2/5] Loss_D: 1.0764 Loss_G: 0.6341\n",
      "BATCH NUMBER 2971\n",
      "Epoch [2/5] Loss_D: 1.2653 Loss_G: 0.5155\n",
      "BATCH NUMBER 2972\n",
      "Epoch [2/5] Loss_D: 1.1017 Loss_G: 0.6203\n",
      "BATCH NUMBER 2973\n",
      "Epoch [2/5] Loss_D: 1.0645 Loss_G: 0.6436\n",
      "BATCH NUMBER 2974\n",
      "Epoch [2/5] Loss_D: 1.0920 Loss_G: 0.6281\n",
      "BATCH NUMBER 2975\n",
      "Epoch [2/5] Loss_D: 1.3145 Loss_G: 0.4957\n",
      "BATCH NUMBER 2976\n",
      "Epoch [2/5] Loss_D: 1.1692 Loss_G: 0.5735\n",
      "BATCH NUMBER 2977\n",
      "Epoch [2/5] Loss_D: 1.1855 Loss_G: 0.5604\n",
      "BATCH NUMBER 2978\n",
      "Epoch [2/5] Loss_D: 1.0792 Loss_G: 0.6323\n",
      "BATCH NUMBER 2979\n",
      "Epoch [2/5] Loss_D: 1.0666 Loss_G: 0.6424\n",
      "BATCH NUMBER 2980\n",
      "Epoch [2/5] Loss_D: 1.0730 Loss_G: 0.6376\n",
      "BATCH NUMBER 2981\n",
      "Epoch [2/5] Loss_D: 1.1091 Loss_G: 0.6161\n",
      "BATCH NUMBER 2982\n",
      "Epoch [2/5] Loss_D: 1.0814 Loss_G: 0.6335\n",
      "BATCH NUMBER 2983\n",
      "Epoch [2/5] Loss_D: 1.1129 Loss_G: 0.6136\n",
      "BATCH NUMBER 2984\n",
      "Epoch [2/5] Loss_D: 1.1887 Loss_G: 0.5597\n",
      "BATCH NUMBER 2985\n",
      "Epoch [2/5] Loss_D: 1.1164 Loss_G: 0.6170\n",
      "BATCH NUMBER 2986\n",
      "Epoch [2/5] Loss_D: 1.1641 Loss_G: 0.5858\n",
      "BATCH NUMBER 2987\n",
      "Epoch [2/5] Loss_D: 1.2192 Loss_G: 0.5478\n",
      "BATCH NUMBER 2988\n",
      "Epoch [2/5] Loss_D: 1.2246 Loss_G: 0.5446\n",
      "BATCH NUMBER 2989\n",
      "Epoch [2/5] Loss_D: 1.1192 Loss_G: 0.6075\n",
      "BATCH NUMBER 2990\n",
      "Epoch [2/5] Loss_D: 1.0931 Loss_G: 0.6277\n",
      "BATCH NUMBER 2991\n",
      "Epoch [2/5] Loss_D: 1.2249 Loss_G: 0.5457\n",
      "BATCH NUMBER 2992\n",
      "Epoch [2/5] Loss_D: 1.2729 Loss_G: 0.5053\n",
      "BATCH NUMBER 2993\n",
      "Epoch [2/5] Loss_D: 1.1006 Loss_G: 0.6133\n",
      "BATCH NUMBER 2994\n",
      "Epoch [2/5] Loss_D: 1.1264 Loss_G: 0.6096\n",
      "BATCH NUMBER 2995\n",
      "Epoch [2/5] Loss_D: 1.0455 Loss_G: 0.6595\n",
      "BATCH NUMBER 2996\n",
      "Epoch [2/5] Loss_D: 1.0923 Loss_G: 0.6210\n",
      "BATCH NUMBER 2997\n",
      "Epoch [2/5] Loss_D: 1.0800 Loss_G: 0.6303\n",
      "BATCH NUMBER 2998\n",
      "Epoch [2/5] Loss_D: 1.0871 Loss_G: 0.6241\n",
      "BATCH NUMBER 2999\n",
      "Epoch [2/5] Loss_D: 1.1465 Loss_G: 0.5918\n",
      "BATCH NUMBER 3000\n",
      "Epoch [3/5] Loss_D: 1.0478 Loss_G: 0.6571\n",
      "BATCH NUMBER 3001\n",
      "Epoch [3/5] Loss_D: 1.1204 Loss_G: 0.5976\n",
      "BATCH NUMBER 3002\n",
      "Epoch [3/5] Loss_D: 1.0971 Loss_G: 0.6185\n",
      "BATCH NUMBER 3003\n",
      "Epoch [3/5] Loss_D: 1.2001 Loss_G: 0.5513\n",
      "BATCH NUMBER 3004\n",
      "Epoch [3/5] Loss_D: 1.2737 Loss_G: 0.5132\n",
      "BATCH NUMBER 3005\n",
      "Epoch [3/5] Loss_D: 1.0926 Loss_G: 0.6208\n",
      "BATCH NUMBER 3006\n",
      "Epoch [3/5] Loss_D: 1.0755 Loss_G: 0.6337\n",
      "BATCH NUMBER 3007\n",
      "Epoch [3/5] Loss_D: 1.1221 Loss_G: 0.6043\n",
      "BATCH NUMBER 3008\n",
      "Epoch [3/5] Loss_D: 1.1382 Loss_G: 0.5936\n",
      "BATCH NUMBER 3009\n",
      "Epoch [3/5] Loss_D: 1.1038 Loss_G: 0.6148\n",
      "BATCH NUMBER 3010\n",
      "Epoch [3/5] Loss_D: 1.1520 Loss_G: 0.5888\n",
      "BATCH NUMBER 3011\n",
      "Epoch [3/5] Loss_D: 1.1203 Loss_G: 0.6151\n",
      "BATCH NUMBER 3012\n",
      "Epoch [3/5] Loss_D: 1.0437 Loss_G: 0.6606\n",
      "BATCH NUMBER 3013\n",
      "Epoch [3/5] Loss_D: 1.1328 Loss_G: 0.5955\n",
      "BATCH NUMBER 3014\n",
      "Epoch [3/5] Loss_D: 1.1507 Loss_G: 0.5828\n",
      "BATCH NUMBER 3015\n",
      "Epoch [3/5] Loss_D: 1.0658 Loss_G: 0.6416\n",
      "BATCH NUMBER 3016\n",
      "Epoch [3/5] Loss_D: 1.1906 Loss_G: 0.5568\n",
      "BATCH NUMBER 3017\n",
      "Epoch [3/5] Loss_D: 1.1177 Loss_G: 0.6077\n",
      "BATCH NUMBER 3018\n",
      "Epoch [3/5] Loss_D: 1.1264 Loss_G: 0.6016\n",
      "BATCH NUMBER 3019\n",
      "Epoch [3/5] Loss_D: 1.1192 Loss_G: 0.6079\n",
      "BATCH NUMBER 3020\n",
      "Epoch [3/5] Loss_D: 1.0601 Loss_G: 0.6467\n",
      "BATCH NUMBER 3021\n",
      "Epoch [3/5] Loss_D: 1.3163 Loss_G: 0.4923\n",
      "BATCH NUMBER 3022\n",
      "Epoch [3/5] Loss_D: 1.3008 Loss_G: 0.4997\n",
      "BATCH NUMBER 3023\n",
      "Epoch [3/5] Loss_D: 1.0915 Loss_G: 0.6231\n",
      "BATCH NUMBER 3024\n",
      "Epoch [3/5] Loss_D: 1.2208 Loss_G: 0.5472\n",
      "BATCH NUMBER 3025\n",
      "Epoch [3/5] Loss_D: 1.0483 Loss_G: 0.6555\n",
      "BATCH NUMBER 3026\n",
      "Epoch [3/5] Loss_D: 1.1478 Loss_G: 0.5884\n",
      "BATCH NUMBER 3027\n",
      "Epoch [3/5] Loss_D: 1.0781 Loss_G: 0.6342\n",
      "BATCH NUMBER 3028\n",
      "Epoch [3/5] Loss_D: 1.0852 Loss_G: 0.6350\n",
      "BATCH NUMBER 3029\n",
      "Epoch [3/5] Loss_D: 1.2497 Loss_G: 0.5236\n",
      "BATCH NUMBER 3030\n",
      "Epoch [3/5] Loss_D: 1.1019 Loss_G: 0.6206\n",
      "BATCH NUMBER 3031\n",
      "Epoch [3/5] Loss_D: 1.1910 Loss_G: 0.5638\n",
      "BATCH NUMBER 3032\n",
      "Epoch [3/5] Loss_D: 1.1114 Loss_G: 0.6161\n",
      "BATCH NUMBER 3033\n",
      "Epoch [3/5] Loss_D: 1.0915 Loss_G: 0.6206\n",
      "BATCH NUMBER 3034\n",
      "Epoch [3/5] Loss_D: 1.1468 Loss_G: 0.5945\n",
      "BATCH NUMBER 3035\n",
      "Epoch [3/5] Loss_D: 1.0637 Loss_G: 0.6437\n",
      "BATCH NUMBER 3036\n",
      "Epoch [3/5] Loss_D: 1.2147 Loss_G: 0.5434\n",
      "BATCH NUMBER 3037\n",
      "Epoch [3/5] Loss_D: 1.1280 Loss_G: 0.6011\n",
      "BATCH NUMBER 3038\n",
      "Epoch [3/5] Loss_D: 1.1942 Loss_G: 0.5690\n",
      "BATCH NUMBER 3039\n",
      "Epoch [3/5] Loss_D: 1.1850 Loss_G: 0.5763\n",
      "BATCH NUMBER 3040\n",
      "Epoch [3/5] Loss_D: 1.0373 Loss_G: 0.6669\n",
      "BATCH NUMBER 3041\n",
      "Epoch [3/5] Loss_D: 1.0592 Loss_G: 0.6474\n",
      "BATCH NUMBER 3042\n",
      "Epoch [3/5] Loss_D: 1.0525 Loss_G: 0.6527\n",
      "BATCH NUMBER 3043\n",
      "Epoch [3/5] Loss_D: 1.2317 Loss_G: 0.5402\n",
      "BATCH NUMBER 3044\n",
      "Epoch [3/5] Loss_D: 1.0549 Loss_G: 0.6498\n",
      "BATCH NUMBER 3045\n",
      "Epoch [3/5] Loss_D: 1.1291 Loss_G: 0.6062\n",
      "BATCH NUMBER 3046\n",
      "Epoch [3/5] Loss_D: 1.0710 Loss_G: 0.6384\n",
      "BATCH NUMBER 3047\n",
      "Epoch [3/5] Loss_D: 1.1542 Loss_G: 0.5963\n",
      "BATCH NUMBER 3048\n",
      "Epoch [3/5] Loss_D: 1.2191 Loss_G: 0.5538\n",
      "BATCH NUMBER 3049\n",
      "Epoch [3/5] Loss_D: 1.0588 Loss_G: 0.6487\n",
      "BATCH NUMBER 3050\n",
      "Epoch [3/5] Loss_D: 1.0825 Loss_G: 0.6282\n",
      "BATCH NUMBER 3051\n",
      "Epoch [3/5] Loss_D: 1.2464 Loss_G: 0.5344\n",
      "BATCH NUMBER 3052\n",
      "Epoch [3/5] Loss_D: 1.0916 Loss_G: 0.6229\n",
      "BATCH NUMBER 3053\n",
      "Epoch [3/5] Loss_D: 1.1036 Loss_G: 0.6110\n",
      "BATCH NUMBER 3054\n",
      "Epoch [3/5] Loss_D: 1.0670 Loss_G: 0.6434\n",
      "BATCH NUMBER 3055\n",
      "Epoch [3/5] Loss_D: 1.2194 Loss_G: 0.5484\n",
      "BATCH NUMBER 3056\n",
      "Epoch [3/5] Loss_D: 1.1819 Loss_G: 0.5651\n",
      "BATCH NUMBER 3057\n",
      "Epoch [3/5] Loss_D: 1.0787 Loss_G: 0.6331\n",
      "BATCH NUMBER 3058\n",
      "Epoch [3/5] Loss_D: 1.1570 Loss_G: 0.5906\n",
      "BATCH NUMBER 3059\n",
      "Epoch [3/5] Loss_D: 1.1099 Loss_G: 0.6144\n",
      "BATCH NUMBER 3060\n",
      "Epoch [3/5] Loss_D: 1.1166 Loss_G: 0.6075\n",
      "BATCH NUMBER 3061\n",
      "Epoch [3/5] Loss_D: 1.2399 Loss_G: 0.5397\n",
      "BATCH NUMBER 3062\n",
      "Epoch [3/5] Loss_D: 1.0862 Loss_G: 0.6259\n",
      "BATCH NUMBER 3063\n",
      "Epoch [3/5] Loss_D: 1.1944 Loss_G: 0.5541\n",
      "BATCH NUMBER 3064\n",
      "Epoch [3/5] Loss_D: 1.0556 Loss_G: 0.6513\n",
      "BATCH NUMBER 3065\n",
      "Epoch [3/5] Loss_D: 1.2347 Loss_G: 0.5360\n",
      "BATCH NUMBER 3066\n",
      "Epoch [3/5] Loss_D: 1.0340 Loss_G: 0.6691\n",
      "BATCH NUMBER 3067\n",
      "Epoch [3/5] Loss_D: 1.1146 Loss_G: 0.6114\n",
      "BATCH NUMBER 3068\n",
      "Epoch [3/5] Loss_D: 1.0414 Loss_G: 0.6613\n",
      "BATCH NUMBER 3069\n",
      "Epoch [3/5] Loss_D: 1.1271 Loss_G: 0.6004\n",
      "BATCH NUMBER 3070\n",
      "Epoch [3/5] Loss_D: 1.1929 Loss_G: 0.6085\n",
      "BATCH NUMBER 3071\n",
      "Epoch [3/5] Loss_D: 1.2266 Loss_G: 0.5414\n",
      "BATCH NUMBER 3072\n",
      "Epoch [3/5] Loss_D: 1.1110 Loss_G: 0.6151\n",
      "BATCH NUMBER 3073\n",
      "Epoch [3/5] Loss_D: 1.0688 Loss_G: 0.6486\n",
      "BATCH NUMBER 3074\n",
      "Epoch [3/5] Loss_D: 1.1706 Loss_G: 0.5825\n",
      "BATCH NUMBER 3075\n",
      "Epoch [3/5] Loss_D: 1.0656 Loss_G: 0.6439\n",
      "BATCH NUMBER 3076\n",
      "Epoch [3/5] Loss_D: 1.0808 Loss_G: 0.6283\n",
      "BATCH NUMBER 3077\n",
      "Epoch [3/5] Loss_D: 1.0985 Loss_G: 0.6180\n",
      "BATCH NUMBER 3078\n",
      "Epoch [3/5] Loss_D: 1.1215 Loss_G: 0.6056\n",
      "BATCH NUMBER 3079\n",
      "Epoch [3/5] Loss_D: 1.0672 Loss_G: 0.6409\n",
      "BATCH NUMBER 3080\n",
      "Epoch [3/5] Loss_D: 1.4332 Loss_G: 0.5947\n",
      "BATCH NUMBER 3081\n",
      "Epoch [3/5] Loss_D: 1.1060 Loss_G: 0.6098\n",
      "BATCH NUMBER 3082\n",
      "Epoch [3/5] Loss_D: 1.1344 Loss_G: 0.6021\n",
      "BATCH NUMBER 3083\n",
      "Epoch [3/5] Loss_D: 1.0647 Loss_G: 0.6421\n",
      "BATCH NUMBER 3084\n",
      "Epoch [3/5] Loss_D: 1.1588 Loss_G: 0.5814\n",
      "BATCH NUMBER 3085\n",
      "Epoch [3/5] Loss_D: 1.0772 Loss_G: 0.6343\n",
      "BATCH NUMBER 3086\n",
      "Epoch [3/5] Loss_D: 1.0949 Loss_G: 0.6188\n",
      "BATCH NUMBER 3087\n",
      "Epoch [3/5] Loss_D: 1.1484 Loss_G: 0.5936\n",
      "BATCH NUMBER 3088\n",
      "Epoch [3/5] Loss_D: 1.1714 Loss_G: 0.5887\n",
      "BATCH NUMBER 3089\n",
      "Epoch [3/5] Loss_D: 1.3255 Loss_G: 0.4862\n",
      "BATCH NUMBER 3090\n",
      "Epoch [3/5] Loss_D: 1.1078 Loss_G: 0.6194\n",
      "BATCH NUMBER 3091\n",
      "Epoch [3/5] Loss_D: 1.0554 Loss_G: 0.6519\n",
      "BATCH NUMBER 3092\n",
      "Epoch [3/5] Loss_D: 1.2068 Loss_G: 0.5567\n",
      "BATCH NUMBER 3093\n",
      "Epoch [3/5] Loss_D: 1.1808 Loss_G: 0.5728\n",
      "BATCH NUMBER 3094\n",
      "Epoch [3/5] Loss_D: 1.1642 Loss_G: 0.5780\n",
      "BATCH NUMBER 3095\n",
      "Epoch [3/5] Loss_D: 1.0700 Loss_G: 0.6403\n",
      "BATCH NUMBER 3096\n",
      "Epoch [3/5] Loss_D: 1.1492 Loss_G: 0.5900\n",
      "BATCH NUMBER 3097\n",
      "Epoch [3/5] Loss_D: 1.0608 Loss_G: 0.6469\n",
      "BATCH NUMBER 3098\n",
      "Epoch [3/5] Loss_D: 1.1879 Loss_G: 0.5668\n",
      "BATCH NUMBER 3099\n",
      "Epoch [3/5] Loss_D: 1.0916 Loss_G: 0.6305\n",
      "BATCH NUMBER 3100\n",
      "Epoch [3/5] Loss_D: 1.1457 Loss_G: 0.5938\n",
      "BATCH NUMBER 3101\n",
      "Epoch [3/5] Loss_D: 1.2500 Loss_G: 0.5322\n",
      "BATCH NUMBER 3102\n",
      "Epoch [3/5] Loss_D: 1.0798 Loss_G: 0.6315\n",
      "BATCH NUMBER 3103\n",
      "Epoch [3/5] Loss_D: 1.1866 Loss_G: 0.5662\n",
      "BATCH NUMBER 3104\n",
      "Epoch [3/5] Loss_D: 1.1468 Loss_G: 0.5935\n",
      "BATCH NUMBER 3105\n",
      "Epoch [3/5] Loss_D: 1.1506 Loss_G: 0.5909\n",
      "BATCH NUMBER 3106\n",
      "Epoch [3/5] Loss_D: 1.0980 Loss_G: 0.6251\n",
      "BATCH NUMBER 3107\n",
      "Epoch [3/5] Loss_D: 1.0399 Loss_G: 0.6635\n",
      "BATCH NUMBER 3108\n",
      "Epoch [3/5] Loss_D: 1.0521 Loss_G: 0.6558\n",
      "BATCH NUMBER 3109\n",
      "Epoch [3/5] Loss_D: 1.1601 Loss_G: 0.5799\n",
      "BATCH NUMBER 3110\n",
      "Epoch [3/5] Loss_D: 1.0601 Loss_G: 0.6471\n",
      "BATCH NUMBER 3111\n",
      "Epoch [3/5] Loss_D: 1.1262 Loss_G: 0.6024\n",
      "BATCH NUMBER 3112\n",
      "Epoch [3/5] Loss_D: 1.2228 Loss_G: 0.5452\n",
      "BATCH NUMBER 3113\n",
      "Epoch [3/5] Loss_D: 1.0739 Loss_G: 0.6363\n",
      "BATCH NUMBER 3114\n",
      "Epoch [3/5] Loss_D: 1.1288 Loss_G: 0.6364\n",
      "BATCH NUMBER 3115\n",
      "Epoch [3/5] Loss_D: 1.0406 Loss_G: 0.6623\n",
      "BATCH NUMBER 3116\n",
      "Epoch [3/5] Loss_D: 1.1158 Loss_G: 0.6095\n",
      "BATCH NUMBER 3117\n",
      "Epoch [3/5] Loss_D: 1.2488 Loss_G: 0.5250\n",
      "BATCH NUMBER 3118\n",
      "Epoch [3/5] Loss_D: 1.2624 Loss_G: 0.5214\n",
      "BATCH NUMBER 3119\n",
      "Epoch [3/5] Loss_D: 1.1422 Loss_G: 0.5964\n",
      "BATCH NUMBER 3120\n",
      "Epoch [3/5] Loss_D: 1.1269 Loss_G: 0.6034\n",
      "BATCH NUMBER 3121\n",
      "Epoch [3/5] Loss_D: 1.1071 Loss_G: 0.6257\n",
      "BATCH NUMBER 3122\n",
      "Epoch [3/5] Loss_D: 1.3224 Loss_G: 0.4809\n",
      "BATCH NUMBER 3123\n",
      "Epoch [3/5] Loss_D: 1.1980 Loss_G: 0.5579\n",
      "BATCH NUMBER 3124\n",
      "Epoch [3/5] Loss_D: 1.1643 Loss_G: 0.5867\n",
      "BATCH NUMBER 3125\n",
      "Epoch [3/5] Loss_D: 1.1790 Loss_G: 0.5728\n",
      "BATCH NUMBER 3126\n",
      "Epoch [3/5] Loss_D: 1.0636 Loss_G: 0.6444\n",
      "BATCH NUMBER 3127\n",
      "Epoch [3/5] Loss_D: 1.0463 Loss_G: 0.6587\n",
      "BATCH NUMBER 3128\n",
      "Epoch [3/5] Loss_D: 1.1833 Loss_G: 0.5690\n",
      "BATCH NUMBER 3129\n",
      "Epoch [3/5] Loss_D: 1.0725 Loss_G: 0.6355\n",
      "BATCH NUMBER 3130\n",
      "Epoch [3/5] Loss_D: 1.1317 Loss_G: 0.6044\n",
      "BATCH NUMBER 3131\n",
      "Epoch [3/5] Loss_D: 1.0363 Loss_G: 0.6655\n",
      "BATCH NUMBER 3132\n",
      "Epoch [3/5] Loss_D: 1.0581 Loss_G: 0.6504\n",
      "BATCH NUMBER 3133\n",
      "Epoch [3/5] Loss_D: 1.0823 Loss_G: 0.6310\n",
      "BATCH NUMBER 3134\n",
      "Epoch [3/5] Loss_D: 1.1064 Loss_G: 0.6183\n",
      "BATCH NUMBER 3135\n",
      "Epoch [3/5] Loss_D: 1.0966 Loss_G: 0.6241\n",
      "BATCH NUMBER 3136\n",
      "Epoch [3/5] Loss_D: 1.1483 Loss_G: 0.5975\n",
      "BATCH NUMBER 3137\n",
      "Epoch [3/5] Loss_D: 1.1803 Loss_G: 0.5584\n",
      "BATCH NUMBER 3138\n",
      "Epoch [3/5] Loss_D: 1.0787 Loss_G: 0.6326\n",
      "BATCH NUMBER 3139\n",
      "Epoch [3/5] Loss_D: 1.1622 Loss_G: 0.5805\n",
      "BATCH NUMBER 3140\n",
      "Epoch [3/5] Loss_D: 1.1420 Loss_G: 0.5912\n",
      "BATCH NUMBER 3141\n",
      "Epoch [3/5] Loss_D: 1.1454 Loss_G: 0.5841\n",
      "BATCH NUMBER 3142\n",
      "Epoch [3/5] Loss_D: 1.2031 Loss_G: 0.5590\n",
      "BATCH NUMBER 3143\n",
      "Epoch [3/5] Loss_D: 1.2998 Loss_G: 0.5004\n",
      "BATCH NUMBER 3144\n",
      "Epoch [3/5] Loss_D: 1.1273 Loss_G: 0.6025\n",
      "BATCH NUMBER 3145\n",
      "Epoch [3/5] Loss_D: 1.0631 Loss_G: 0.6441\n",
      "BATCH NUMBER 3146\n",
      "Epoch [3/5] Loss_D: 1.0828 Loss_G: 0.6284\n",
      "BATCH NUMBER 3147\n",
      "Epoch [3/5] Loss_D: 1.1316 Loss_G: 0.5975\n",
      "BATCH NUMBER 3148\n",
      "Epoch [3/5] Loss_D: 1.1517 Loss_G: 0.5821\n",
      "BATCH NUMBER 3149\n",
      "Epoch [3/5] Loss_D: 1.1110 Loss_G: 0.6071\n",
      "BATCH NUMBER 3150\n",
      "Epoch [3/5] Loss_D: 1.1051 Loss_G: 0.6087\n",
      "BATCH NUMBER 3151\n",
      "Epoch [3/5] Loss_D: 1.0384 Loss_G: 0.6653\n",
      "BATCH NUMBER 3152\n",
      "Epoch [3/5] Loss_D: 1.0500 Loss_G: 0.6561\n",
      "BATCH NUMBER 3153\n",
      "Epoch [3/5] Loss_D: 1.1808 Loss_G: 0.5810\n",
      "BATCH NUMBER 3154\n",
      "Epoch [3/5] Loss_D: 1.0540 Loss_G: 0.6533\n",
      "BATCH NUMBER 3155\n",
      "Epoch [3/5] Loss_D: 1.1763 Loss_G: 0.5761\n",
      "BATCH NUMBER 3156\n",
      "Epoch [3/5] Loss_D: 1.0896 Loss_G: 0.6297\n",
      "BATCH NUMBER 3157\n",
      "Epoch [3/5] Loss_D: 1.1859 Loss_G: 0.5611\n",
      "BATCH NUMBER 3158\n",
      "Epoch [3/5] Loss_D: 1.1543 Loss_G: 0.5942\n",
      "BATCH NUMBER 3159\n",
      "Epoch [3/5] Loss_D: 1.0820 Loss_G: 0.6370\n",
      "BATCH NUMBER 3160\n",
      "Epoch [3/5] Loss_D: 1.0483 Loss_G: 0.6568\n",
      "BATCH NUMBER 3161\n",
      "Epoch [3/5] Loss_D: 1.1004 Loss_G: 0.6230\n",
      "BATCH NUMBER 3162\n",
      "Epoch [3/5] Loss_D: 1.0716 Loss_G: 0.6383\n",
      "BATCH NUMBER 3163\n",
      "Epoch [3/5] Loss_D: 1.1666 Loss_G: 0.5839\n",
      "BATCH NUMBER 3164\n",
      "Epoch [3/5] Loss_D: 1.0569 Loss_G: 0.6501\n",
      "BATCH NUMBER 3165\n",
      "Epoch [3/5] Loss_D: 1.0839 Loss_G: 0.6362\n",
      "BATCH NUMBER 3166\n",
      "Epoch [3/5] Loss_D: 1.0673 Loss_G: 0.6409\n",
      "BATCH NUMBER 3167\n",
      "Epoch [3/5] Loss_D: 1.1988 Loss_G: 0.5586\n",
      "BATCH NUMBER 3168\n",
      "Epoch [3/5] Loss_D: 1.0739 Loss_G: 0.6437\n",
      "BATCH NUMBER 3169\n",
      "Epoch [3/5] Loss_D: 1.0423 Loss_G: 0.6618\n",
      "BATCH NUMBER 3170\n",
      "Epoch [3/5] Loss_D: 1.1333 Loss_G: 0.6028\n",
      "BATCH NUMBER 3171\n",
      "Epoch [3/5] Loss_D: 1.0525 Loss_G: 0.6531\n",
      "BATCH NUMBER 3172\n",
      "Epoch [3/5] Loss_D: 1.2763 Loss_G: 0.5172\n",
      "BATCH NUMBER 3173\n",
      "Epoch [3/5] Loss_D: 1.2914 Loss_G: 0.5064\n",
      "BATCH NUMBER 3174\n",
      "Epoch [3/5] Loss_D: 1.1209 Loss_G: 0.6052\n",
      "BATCH NUMBER 3175\n",
      "Epoch [3/5] Loss_D: 1.0487 Loss_G: 0.6573\n",
      "BATCH NUMBER 3176\n",
      "Epoch [3/5] Loss_D: 1.1684 Loss_G: 0.5655\n",
      "BATCH NUMBER 3177\n",
      "Epoch [3/5] Loss_D: 1.1506 Loss_G: 0.5797\n",
      "BATCH NUMBER 3178\n",
      "Epoch [3/5] Loss_D: 1.0543 Loss_G: 0.6516\n",
      "BATCH NUMBER 3179\n",
      "Epoch [3/5] Loss_D: 1.0805 Loss_G: 0.6296\n",
      "BATCH NUMBER 3180\n",
      "Epoch [3/5] Loss_D: 1.1158 Loss_G: 0.6184\n",
      "BATCH NUMBER 3181\n",
      "Epoch [3/5] Loss_D: 1.0675 Loss_G: 0.6420\n",
      "BATCH NUMBER 3182\n",
      "Epoch [3/5] Loss_D: 1.1859 Loss_G: 0.5677\n",
      "BATCH NUMBER 3183\n",
      "Epoch [3/5] Loss_D: 1.0569 Loss_G: 0.6507\n",
      "BATCH NUMBER 3184\n",
      "Epoch [3/5] Loss_D: 1.1883 Loss_G: 0.5735\n",
      "BATCH NUMBER 3185\n",
      "Epoch [3/5] Loss_D: 1.1217 Loss_G: 0.6028\n",
      "BATCH NUMBER 3186\n",
      "Epoch [3/5] Loss_D: 1.1262 Loss_G: 0.6025\n",
      "BATCH NUMBER 3187\n",
      "Epoch [3/5] Loss_D: 1.1004 Loss_G: 0.6239\n",
      "BATCH NUMBER 3188\n",
      "Epoch [3/5] Loss_D: 1.1737 Loss_G: 0.5784\n",
      "BATCH NUMBER 3189\n",
      "Epoch [3/5] Loss_D: 1.0756 Loss_G: 0.6413\n",
      "BATCH NUMBER 3190\n",
      "Epoch [3/5] Loss_D: 1.1342 Loss_G: 0.6215\n",
      "BATCH NUMBER 3191\n",
      "Epoch [3/5] Loss_D: 1.0481 Loss_G: 0.6592\n",
      "BATCH NUMBER 3192\n",
      "Epoch [3/5] Loss_D: 1.1741 Loss_G: 0.5688\n",
      "BATCH NUMBER 3193\n",
      "Epoch [3/5] Loss_D: 1.1756 Loss_G: 0.5679\n",
      "BATCH NUMBER 3194\n",
      "Epoch [3/5] Loss_D: 1.0436 Loss_G: 0.6615\n",
      "BATCH NUMBER 3195\n",
      "Epoch [3/5] Loss_D: 1.3221 Loss_G: 0.4884\n",
      "BATCH NUMBER 3196\n",
      "Epoch [3/5] Loss_D: 1.2086 Loss_G: 0.5582\n",
      "BATCH NUMBER 3197\n",
      "Epoch [3/5] Loss_D: 1.0729 Loss_G: 0.6357\n",
      "BATCH NUMBER 3198\n",
      "Epoch [3/5] Loss_D: 1.1193 Loss_G: 0.6158\n",
      "BATCH NUMBER 3199\n",
      "Epoch [3/5] Loss_D: 1.1566 Loss_G: 0.5928\n",
      "BATCH NUMBER 3200\n",
      "Epoch [3/5] Loss_D: 1.3011 Loss_G: 0.4975\n",
      "BATCH NUMBER 3201\n",
      "Epoch [3/5] Loss_D: 1.1327 Loss_G: 0.5933\n",
      "BATCH NUMBER 3202\n",
      "Epoch [3/5] Loss_D: 1.0523 Loss_G: 0.6539\n",
      "BATCH NUMBER 3203\n",
      "Epoch [3/5] Loss_D: 1.0870 Loss_G: 0.6311\n",
      "BATCH NUMBER 3204\n",
      "Epoch [3/5] Loss_D: 1.0899 Loss_G: 0.6237\n",
      "BATCH NUMBER 3205\n",
      "Epoch [3/5] Loss_D: 1.0538 Loss_G: 0.6506\n",
      "BATCH NUMBER 3206\n",
      "Epoch [3/5] Loss_D: 1.0561 Loss_G: 0.6606\n",
      "BATCH NUMBER 3207\n",
      "Epoch [3/5] Loss_D: 1.3365 Loss_G: 0.4862\n",
      "BATCH NUMBER 3208\n",
      "Epoch [3/5] Loss_D: 1.1057 Loss_G: 0.6162\n",
      "BATCH NUMBER 3209\n",
      "Epoch [3/5] Loss_D: 1.0408 Loss_G: 0.6639\n",
      "BATCH NUMBER 3210\n",
      "Epoch [3/5] Loss_D: 1.0917 Loss_G: 0.6224\n",
      "BATCH NUMBER 3211\n",
      "Epoch [3/5] Loss_D: 1.0639 Loss_G: 0.6436\n",
      "BATCH NUMBER 3212\n",
      "Epoch [3/5] Loss_D: 1.1806 Loss_G: 0.5741\n",
      "BATCH NUMBER 3213\n",
      "Epoch [3/5] Loss_D: 1.1465 Loss_G: 0.5865\n",
      "BATCH NUMBER 3214\n",
      "Epoch [3/5] Loss_D: 1.1817 Loss_G: 0.5632\n",
      "BATCH NUMBER 3215\n",
      "Epoch [3/5] Loss_D: 1.2559 Loss_G: 0.5199\n",
      "BATCH NUMBER 3216\n",
      "Epoch [3/5] Loss_D: 1.1187 Loss_G: 0.6304\n",
      "BATCH NUMBER 3217\n",
      "Epoch [3/5] Loss_D: 1.0471 Loss_G: 0.6576\n",
      "BATCH NUMBER 3218\n",
      "Epoch [3/5] Loss_D: 1.0951 Loss_G: 0.6268\n",
      "BATCH NUMBER 3219\n",
      "Epoch [3/5] Loss_D: 1.1004 Loss_G: 0.6222\n",
      "BATCH NUMBER 3220\n",
      "Epoch [3/5] Loss_D: 1.0580 Loss_G: 0.6481\n",
      "BATCH NUMBER 3221\n",
      "Epoch [3/5] Loss_D: 1.1469 Loss_G: 0.5922\n",
      "BATCH NUMBER 3222\n",
      "Epoch [3/5] Loss_D: 1.0887 Loss_G: 0.6306\n",
      "BATCH NUMBER 3223\n",
      "Epoch [3/5] Loss_D: 1.2581 Loss_G: 0.5258\n",
      "BATCH NUMBER 3224\n",
      "Epoch [3/5] Loss_D: 1.0682 Loss_G: 0.6411\n",
      "BATCH NUMBER 3225\n",
      "Epoch [3/5] Loss_D: 1.0846 Loss_G: 0.6293\n",
      "BATCH NUMBER 3226\n",
      "Epoch [3/5] Loss_D: 1.1765 Loss_G: 0.5848\n",
      "BATCH NUMBER 3227\n",
      "Epoch [3/5] Loss_D: 1.0598 Loss_G: 0.6501\n",
      "BATCH NUMBER 3228\n",
      "Epoch [3/5] Loss_D: 1.1226 Loss_G: 0.6043\n",
      "BATCH NUMBER 3229\n",
      "Epoch [3/5] Loss_D: 1.0755 Loss_G: 0.6348\n",
      "BATCH NUMBER 3230\n",
      "Epoch [3/5] Loss_D: 1.1040 Loss_G: 0.6189\n",
      "BATCH NUMBER 3231\n",
      "Epoch [3/5] Loss_D: 1.0552 Loss_G: 0.6522\n",
      "BATCH NUMBER 3232\n",
      "Epoch [3/5] Loss_D: 1.1048 Loss_G: 0.6272\n",
      "BATCH NUMBER 3233\n",
      "Epoch [3/5] Loss_D: 1.2284 Loss_G: 0.5416\n",
      "BATCH NUMBER 3234\n",
      "Epoch [3/5] Loss_D: 1.1211 Loss_G: 0.6061\n",
      "BATCH NUMBER 3235\n",
      "Epoch [3/5] Loss_D: 1.0763 Loss_G: 0.6434\n",
      "BATCH NUMBER 3236\n",
      "Epoch [3/5] Loss_D: 1.2244 Loss_G: 0.5456\n",
      "BATCH NUMBER 3237\n",
      "Epoch [3/5] Loss_D: 1.1518 Loss_G: 0.5868\n",
      "BATCH NUMBER 3238\n",
      "Epoch [3/5] Loss_D: 1.1628 Loss_G: 0.5892\n",
      "BATCH NUMBER 3239\n",
      "Epoch [3/5] Loss_D: 1.1332 Loss_G: 0.6016\n",
      "BATCH NUMBER 3240\n",
      "Epoch [3/5] Loss_D: 1.0581 Loss_G: 0.6498\n",
      "BATCH NUMBER 3241\n",
      "Epoch [3/5] Loss_D: 1.1084 Loss_G: 0.6107\n",
      "BATCH NUMBER 3242\n",
      "Epoch [3/5] Loss_D: 1.0763 Loss_G: 0.6330\n",
      "BATCH NUMBER 3243\n",
      "Epoch [3/5] Loss_D: 1.1603 Loss_G: 0.5883\n",
      "BATCH NUMBER 3244\n",
      "Epoch [3/5] Loss_D: 1.0712 Loss_G: 0.6365\n",
      "BATCH NUMBER 3245\n",
      "Epoch [3/5] Loss_D: 1.0363 Loss_G: 0.6668\n",
      "BATCH NUMBER 3246\n",
      "Epoch [3/5] Loss_D: 1.2903 Loss_G: 0.5058\n",
      "BATCH NUMBER 3247\n",
      "Epoch [3/5] Loss_D: 1.1034 Loss_G: 0.6216\n",
      "BATCH NUMBER 3248\n",
      "Epoch [3/5] Loss_D: 1.1067 Loss_G: 0.6077\n",
      "BATCH NUMBER 3249\n",
      "Epoch [3/5] Loss_D: 1.0489 Loss_G: 0.6568\n",
      "BATCH NUMBER 3250\n",
      "Epoch [3/5] Loss_D: 1.2541 Loss_G: 0.5285\n",
      "BATCH NUMBER 3251\n",
      "Epoch [3/5] Loss_D: 1.1764 Loss_G: 0.5681\n",
      "BATCH NUMBER 3252\n",
      "Epoch [3/5] Loss_D: 1.1081 Loss_G: 0.6162\n",
      "BATCH NUMBER 3253\n",
      "Epoch [3/5] Loss_D: 1.0735 Loss_G: 0.6336\n",
      "BATCH NUMBER 3254\n",
      "Epoch [3/5] Loss_D: 1.1452 Loss_G: 0.6016\n",
      "BATCH NUMBER 3255\n",
      "Epoch [3/5] Loss_D: 1.1963 Loss_G: 0.5595\n",
      "BATCH NUMBER 3256\n",
      "Epoch [3/5] Loss_D: 1.1133 Loss_G: 0.6048\n",
      "BATCH NUMBER 3257\n",
      "Epoch [3/5] Loss_D: 1.0397 Loss_G: 0.6645\n",
      "BATCH NUMBER 3258\n",
      "Epoch [3/5] Loss_D: 1.1295 Loss_G: 0.6002\n",
      "BATCH NUMBER 3259\n",
      "Epoch [3/5] Loss_D: 1.0738 Loss_G: 0.6341\n",
      "BATCH NUMBER 3260\n",
      "Epoch [3/5] Loss_D: 1.1724 Loss_G: 0.5782\n",
      "BATCH NUMBER 3261\n",
      "Epoch [3/5] Loss_D: 1.1256 Loss_G: 0.5941\n",
      "BATCH NUMBER 3262\n",
      "Epoch [3/5] Loss_D: 1.0363 Loss_G: 0.6662\n",
      "BATCH NUMBER 3263\n",
      "Epoch [3/5] Loss_D: 1.0890 Loss_G: 0.6245\n",
      "BATCH NUMBER 3264\n",
      "Epoch [3/5] Loss_D: 1.1606 Loss_G: 0.5684\n",
      "BATCH NUMBER 3265\n",
      "Epoch [3/5] Loss_D: 1.0815 Loss_G: 0.6286\n",
      "BATCH NUMBER 3266\n",
      "Epoch [3/5] Loss_D: 1.2064 Loss_G: 0.5602\n",
      "BATCH NUMBER 3267\n",
      "Epoch [3/5] Loss_D: 1.1443 Loss_G: 0.5951\n",
      "BATCH NUMBER 3268\n",
      "Epoch [3/5] Loss_D: 1.0608 Loss_G: 0.6458\n",
      "BATCH NUMBER 3269\n",
      "Epoch [3/5] Loss_D: 1.1759 Loss_G: 0.5669\n",
      "BATCH NUMBER 3270\n",
      "Epoch [3/5] Loss_D: 1.0967 Loss_G: 0.6244\n",
      "BATCH NUMBER 3271\n",
      "Epoch [3/5] Loss_D: 1.0944 Loss_G: 0.6190\n",
      "BATCH NUMBER 3272\n",
      "Epoch [3/5] Loss_D: 1.2459 Loss_G: 0.5349\n",
      "BATCH NUMBER 3273\n",
      "Epoch [3/5] Loss_D: 1.0712 Loss_G: 0.6451\n",
      "BATCH NUMBER 3274\n",
      "Epoch [3/5] Loss_D: 1.2897 Loss_G: 0.5356\n",
      "BATCH NUMBER 3275\n",
      "Epoch [3/5] Loss_D: 1.0485 Loss_G: 0.6551\n",
      "BATCH NUMBER 3276\n",
      "Epoch [3/5] Loss_D: 1.1743 Loss_G: 0.5689\n",
      "BATCH NUMBER 3277\n",
      "Epoch [3/5] Loss_D: 1.1474 Loss_G: 0.5841\n",
      "BATCH NUMBER 3278\n",
      "Epoch [3/5] Loss_D: 1.1453 Loss_G: 0.5765\n",
      "BATCH NUMBER 3279\n",
      "Epoch [3/5] Loss_D: 1.0941 Loss_G: 0.6270\n",
      "BATCH NUMBER 3280\n",
      "Epoch [3/5] Loss_D: 1.2055 Loss_G: 0.5432\n",
      "BATCH NUMBER 3281\n",
      "Epoch [3/5] Loss_D: 1.1148 Loss_G: 0.6178\n",
      "BATCH NUMBER 3282\n",
      "Epoch [3/5] Loss_D: 1.1578 Loss_G: 0.5822\n",
      "BATCH NUMBER 3283\n",
      "Epoch [3/5] Loss_D: 1.0787 Loss_G: 0.6387\n",
      "BATCH NUMBER 3284\n",
      "Epoch [3/5] Loss_D: 1.0554 Loss_G: 0.6511\n",
      "BATCH NUMBER 3285\n",
      "Epoch [3/5] Loss_D: 1.1036 Loss_G: 0.6180\n",
      "BATCH NUMBER 3286\n",
      "Epoch [3/5] Loss_D: 1.0854 Loss_G: 0.6245\n",
      "BATCH NUMBER 3287\n",
      "Epoch [3/5] Loss_D: 1.0488 Loss_G: 0.6561\n",
      "BATCH NUMBER 3288\n",
      "Epoch [3/5] Loss_D: 1.1309 Loss_G: 0.6069\n",
      "BATCH NUMBER 3289\n",
      "Epoch [3/5] Loss_D: 1.1227 Loss_G: 0.6023\n",
      "BATCH NUMBER 3290\n",
      "Epoch [3/5] Loss_D: 1.0768 Loss_G: 0.6331\n",
      "BATCH NUMBER 3291\n",
      "Epoch [3/5] Loss_D: 1.3054 Loss_G: 0.4866\n",
      "BATCH NUMBER 3292\n",
      "Epoch [3/5] Loss_D: 1.2131 Loss_G: 0.5556\n",
      "BATCH NUMBER 3293\n",
      "Epoch [3/5] Loss_D: 1.1947 Loss_G: 0.5457\n",
      "BATCH NUMBER 3294\n",
      "Epoch [3/5] Loss_D: 1.4307 Loss_G: 0.4241\n",
      "BATCH NUMBER 3295\n",
      "Epoch [3/5] Loss_D: 1.0505 Loss_G: 0.6549\n",
      "BATCH NUMBER 3296\n",
      "Epoch [3/5] Loss_D: 1.1300 Loss_G: 0.5977\n",
      "BATCH NUMBER 3297\n",
      "Epoch [3/5] Loss_D: 1.1603 Loss_G: 0.5896\n",
      "BATCH NUMBER 3298\n",
      "Epoch [3/5] Loss_D: 1.2403 Loss_G: 0.5400\n",
      "BATCH NUMBER 3299\n",
      "Epoch [3/5] Loss_D: 1.2291 Loss_G: 0.5406\n",
      "BATCH NUMBER 3300\n",
      "Epoch [3/5] Loss_D: 1.0801 Loss_G: 0.6383\n",
      "BATCH NUMBER 3301\n",
      "Epoch [3/5] Loss_D: 1.0426 Loss_G: 0.6624\n",
      "BATCH NUMBER 3302\n",
      "Epoch [3/5] Loss_D: 1.0746 Loss_G: 0.6342\n",
      "BATCH NUMBER 3303\n",
      "Epoch [3/5] Loss_D: 1.0772 Loss_G: 0.6327\n",
      "BATCH NUMBER 3304\n",
      "Epoch [3/5] Loss_D: 1.0620 Loss_G: 0.6460\n",
      "BATCH NUMBER 3305\n",
      "Epoch [3/5] Loss_D: 1.0573 Loss_G: 0.6519\n",
      "BATCH NUMBER 3306\n",
      "Epoch [3/5] Loss_D: 1.1360 Loss_G: 0.5983\n",
      "BATCH NUMBER 3307\n",
      "Epoch [3/5] Loss_D: 1.3379 Loss_G: 0.5126\n",
      "BATCH NUMBER 3308\n",
      "Epoch [3/5] Loss_D: 1.1154 Loss_G: 0.6103\n",
      "BATCH NUMBER 3309\n",
      "Epoch [3/5] Loss_D: 1.2297 Loss_G: 0.5411\n",
      "BATCH NUMBER 3310\n",
      "Epoch [3/5] Loss_D: 1.1568 Loss_G: 0.5851\n",
      "BATCH NUMBER 3311\n",
      "Epoch [3/5] Loss_D: 1.0781 Loss_G: 0.6409\n",
      "BATCH NUMBER 3312\n",
      "Epoch [3/5] Loss_D: 1.2455 Loss_G: 0.5358\n",
      "BATCH NUMBER 3313\n",
      "Epoch [3/5] Loss_D: 1.0860 Loss_G: 0.6272\n",
      "BATCH NUMBER 3314\n",
      "Epoch [3/5] Loss_D: 1.0306 Loss_G: 0.6711\n",
      "BATCH NUMBER 3315\n",
      "Epoch [3/5] Loss_D: 1.1106 Loss_G: 0.6139\n",
      "BATCH NUMBER 3316\n",
      "Epoch [3/5] Loss_D: 1.1125 Loss_G: 0.6119\n",
      "BATCH NUMBER 3317\n",
      "Epoch [3/5] Loss_D: 1.1760 Loss_G: 0.5767\n",
      "BATCH NUMBER 3318\n",
      "Epoch [3/5] Loss_D: 1.0651 Loss_G: 0.6439\n",
      "BATCH NUMBER 3319\n",
      "Epoch [3/5] Loss_D: 1.0370 Loss_G: 0.6656\n",
      "BATCH NUMBER 3320\n",
      "Epoch [3/5] Loss_D: 1.0845 Loss_G: 0.6345\n",
      "BATCH NUMBER 3321\n",
      "Epoch [3/5] Loss_D: 1.0883 Loss_G: 0.6320\n",
      "BATCH NUMBER 3322\n",
      "Epoch [3/5] Loss_D: 1.0859 Loss_G: 0.6348\n",
      "BATCH NUMBER 3323\n",
      "Epoch [3/5] Loss_D: 1.1213 Loss_G: 0.6064\n",
      "BATCH NUMBER 3324\n",
      "Epoch [3/5] Loss_D: 1.0376 Loss_G: 0.6666\n",
      "BATCH NUMBER 3325\n",
      "Epoch [3/5] Loss_D: 1.0796 Loss_G: 0.6289\n",
      "BATCH NUMBER 3326\n",
      "Epoch [3/5] Loss_D: 1.1047 Loss_G: 0.6167\n",
      "BATCH NUMBER 3327\n",
      "Epoch [3/5] Loss_D: 1.1240 Loss_G: 0.6040\n",
      "BATCH NUMBER 3328\n",
      "Epoch [3/5] Loss_D: 1.1358 Loss_G: 0.5927\n",
      "BATCH NUMBER 3329\n",
      "Epoch [3/5] Loss_D: 1.1066 Loss_G: 0.6156\n",
      "BATCH NUMBER 3330\n",
      "Epoch [3/5] Loss_D: 1.2133 Loss_G: 0.5515\n",
      "BATCH NUMBER 3331\n",
      "Epoch [3/5] Loss_D: 1.1060 Loss_G: 0.6160\n",
      "BATCH NUMBER 3332\n",
      "Epoch [3/5] Loss_D: 1.1708 Loss_G: 0.5819\n",
      "BATCH NUMBER 3333\n",
      "Epoch [3/5] Loss_D: 1.0709 Loss_G: 0.6456\n",
      "BATCH NUMBER 3334\n",
      "Epoch [3/5] Loss_D: 1.0592 Loss_G: 0.6473\n",
      "BATCH NUMBER 3335\n",
      "Epoch [3/5] Loss_D: 1.0809 Loss_G: 0.6376\n",
      "BATCH NUMBER 3336\n",
      "Epoch [3/5] Loss_D: 1.1845 Loss_G: 0.5593\n",
      "BATCH NUMBER 3337\n",
      "Epoch [3/5] Loss_D: 1.0834 Loss_G: 0.6273\n",
      "BATCH NUMBER 3338\n",
      "Epoch [3/5] Loss_D: 1.1003 Loss_G: 0.6152\n",
      "BATCH NUMBER 3339\n",
      "Epoch [3/5] Loss_D: 1.0866 Loss_G: 0.6243\n",
      "BATCH NUMBER 3340\n",
      "Epoch [3/5] Loss_D: 1.0719 Loss_G: 0.6370\n",
      "BATCH NUMBER 3341\n",
      "Epoch [3/5] Loss_D: 1.2104 Loss_G: 0.5585\n",
      "BATCH NUMBER 3342\n",
      "Epoch [3/5] Loss_D: 1.2275 Loss_G: 0.5423\n",
      "BATCH NUMBER 3343\n",
      "Epoch [3/5] Loss_D: 1.1714 Loss_G: 0.5802\n",
      "BATCH NUMBER 3344\n",
      "Epoch [3/5] Loss_D: 1.1218 Loss_G: 0.5963\n",
      "BATCH NUMBER 3345\n",
      "Epoch [3/5] Loss_D: 1.2827 Loss_G: 0.5068\n",
      "BATCH NUMBER 3346\n",
      "Epoch [3/5] Loss_D: 1.0789 Loss_G: 0.6333\n",
      "BATCH NUMBER 3347\n",
      "Epoch [3/5] Loss_D: 1.0510 Loss_G: 0.6553\n",
      "BATCH NUMBER 3348\n",
      "Epoch [3/5] Loss_D: 1.1127 Loss_G: 0.6114\n",
      "BATCH NUMBER 3349\n",
      "Epoch [3/5] Loss_D: 1.2714 Loss_G: 0.5157\n",
      "BATCH NUMBER 3350\n",
      "Epoch [3/5] Loss_D: 1.0709 Loss_G: 0.6392\n",
      "BATCH NUMBER 3351\n",
      "Epoch [3/5] Loss_D: 1.1543 Loss_G: 0.5939\n",
      "BATCH NUMBER 3352\n",
      "Epoch [3/5] Loss_D: 1.2183 Loss_G: 0.5584\n",
      "BATCH NUMBER 3353\n",
      "Epoch [3/5] Loss_D: 1.1973 Loss_G: 0.5588\n",
      "BATCH NUMBER 3354\n",
      "Epoch [3/5] Loss_D: 1.0533 Loss_G: 0.6531\n",
      "BATCH NUMBER 3355\n",
      "Epoch [3/5] Loss_D: 1.0701 Loss_G: 0.6473\n",
      "BATCH NUMBER 3356\n",
      "Epoch [3/5] Loss_D: 1.1709 Loss_G: 0.5788\n",
      "BATCH NUMBER 3357\n",
      "Epoch [3/5] Loss_D: 1.0867 Loss_G: 0.6246\n",
      "BATCH NUMBER 3358\n",
      "Epoch [3/5] Loss_D: 1.0700 Loss_G: 0.6395\n",
      "BATCH NUMBER 3359\n",
      "Epoch [3/5] Loss_D: 1.0945 Loss_G: 0.6266\n",
      "BATCH NUMBER 3360\n",
      "Epoch [3/5] Loss_D: 1.1142 Loss_G: 0.6186\n",
      "BATCH NUMBER 3361\n",
      "Epoch [3/5] Loss_D: 1.0724 Loss_G: 0.6457\n",
      "BATCH NUMBER 3362\n",
      "Epoch [3/5] Loss_D: 1.1527 Loss_G: 0.5881\n",
      "BATCH NUMBER 3363\n",
      "Epoch [3/5] Loss_D: 1.1047 Loss_G: 0.6185\n",
      "BATCH NUMBER 3364\n",
      "Epoch [3/5] Loss_D: 1.0718 Loss_G: 0.6390\n",
      "BATCH NUMBER 3365\n",
      "Epoch [3/5] Loss_D: 1.1133 Loss_G: 0.6199\n",
      "BATCH NUMBER 3366\n",
      "Epoch [3/5] Loss_D: 1.1613 Loss_G: 0.5879\n",
      "BATCH NUMBER 3367\n",
      "Epoch [3/5] Loss_D: 1.1315 Loss_G: 0.5961\n",
      "BATCH NUMBER 3368\n",
      "Epoch [3/5] Loss_D: 1.0502 Loss_G: 0.6546\n",
      "BATCH NUMBER 3369\n",
      "Epoch [3/5] Loss_D: 1.1314 Loss_G: 0.5927\n",
      "BATCH NUMBER 3370\n",
      "Epoch [3/5] Loss_D: 1.0844 Loss_G: 0.6346\n",
      "BATCH NUMBER 3371\n",
      "Epoch [3/5] Loss_D: 1.1194 Loss_G: 0.6059\n",
      "BATCH NUMBER 3372\n",
      "Epoch [3/5] Loss_D: 1.1107 Loss_G: 0.6124\n",
      "BATCH NUMBER 3373\n",
      "Epoch [3/5] Loss_D: 1.1452 Loss_G: 0.6039\n",
      "BATCH NUMBER 3374\n",
      "Epoch [3/5] Loss_D: 1.2616 Loss_G: 0.5143\n",
      "BATCH NUMBER 3375\n",
      "Epoch [3/5] Loss_D: 1.0372 Loss_G: 0.6670\n",
      "BATCH NUMBER 3376\n",
      "Epoch [3/5] Loss_D: 1.1366 Loss_G: 0.6011\n",
      "BATCH NUMBER 3377\n",
      "Epoch [3/5] Loss_D: 1.1419 Loss_G: 0.5885\n",
      "BATCH NUMBER 3378\n",
      "Epoch [3/5] Loss_D: 1.0900 Loss_G: 0.6312\n",
      "BATCH NUMBER 3379\n",
      "Epoch [3/5] Loss_D: 1.1117 Loss_G: 0.6125\n",
      "BATCH NUMBER 3380\n",
      "Epoch [3/5] Loss_D: 1.1104 Loss_G: 0.6142\n",
      "BATCH NUMBER 3381\n",
      "Epoch [3/5] Loss_D: 1.1844 Loss_G: 0.5612\n",
      "BATCH NUMBER 3382\n",
      "Epoch [3/5] Loss_D: 1.0786 Loss_G: 0.6341\n",
      "BATCH NUMBER 3383\n",
      "Epoch [3/5] Loss_D: 1.2274 Loss_G: 0.5415\n",
      "BATCH NUMBER 3384\n",
      "Epoch [3/5] Loss_D: 1.2929 Loss_G: 0.5037\n",
      "BATCH NUMBER 3385\n",
      "Epoch [3/5] Loss_D: 1.1799 Loss_G: 0.5639\n",
      "BATCH NUMBER 3386\n",
      "Epoch [3/5] Loss_D: 1.1059 Loss_G: 0.6111\n",
      "BATCH NUMBER 3387\n",
      "Epoch [3/5] Loss_D: 1.0337 Loss_G: 0.6724\n",
      "BATCH NUMBER 3388\n",
      "Epoch [3/5] Loss_D: 1.0648 Loss_G: 0.6430\n",
      "BATCH NUMBER 3389\n",
      "Epoch [3/5] Loss_D: 1.0965 Loss_G: 0.6254\n",
      "BATCH NUMBER 3390\n",
      "Epoch [3/5] Loss_D: 1.0460 Loss_G: 0.6594\n",
      "BATCH NUMBER 3391\n",
      "Epoch [3/5] Loss_D: 1.0975 Loss_G: 0.6147\n",
      "BATCH NUMBER 3392\n",
      "Epoch [3/5] Loss_D: 1.0823 Loss_G: 0.6382\n",
      "BATCH NUMBER 3393\n",
      "Epoch [3/5] Loss_D: 1.2594 Loss_G: 0.5247\n",
      "BATCH NUMBER 3394\n",
      "Epoch [3/5] Loss_D: 1.0481 Loss_G: 0.6570\n",
      "BATCH NUMBER 3395\n",
      "Epoch [3/5] Loss_D: 1.0728 Loss_G: 0.6352\n",
      "BATCH NUMBER 3396\n",
      "Epoch [3/5] Loss_D: 1.0466 Loss_G: 0.6570\n",
      "BATCH NUMBER 3397\n",
      "Epoch [3/5] Loss_D: 1.1945 Loss_G: 0.5690\n",
      "BATCH NUMBER 3398\n",
      "Epoch [3/5] Loss_D: 1.0443 Loss_G: 0.6587\n",
      "BATCH NUMBER 3399\n",
      "Epoch [3/5] Loss_D: 1.1777 Loss_G: 0.5731\n",
      "BATCH NUMBER 3400\n",
      "Epoch [3/5] Loss_D: 1.2461 Loss_G: 0.5280\n",
      "BATCH NUMBER 3401\n",
      "Epoch [3/5] Loss_D: 1.0916 Loss_G: 0.6206\n",
      "BATCH NUMBER 3402\n",
      "Epoch [3/5] Loss_D: 1.0495 Loss_G: 0.6592\n",
      "BATCH NUMBER 3403\n",
      "Epoch [3/5] Loss_D: 1.1443 Loss_G: 0.5944\n",
      "BATCH NUMBER 3404\n",
      "Epoch [3/5] Loss_D: 1.1596 Loss_G: 0.5766\n",
      "BATCH NUMBER 3405\n",
      "Epoch [3/5] Loss_D: 1.0795 Loss_G: 0.6407\n",
      "BATCH NUMBER 3406\n",
      "Epoch [3/5] Loss_D: 1.3012 Loss_G: 0.5051\n",
      "BATCH NUMBER 3407\n",
      "Epoch [3/5] Loss_D: 1.0872 Loss_G: 0.6271\n",
      "BATCH NUMBER 3408\n",
      "Epoch [3/5] Loss_D: 1.0381 Loss_G: 0.6658\n",
      "BATCH NUMBER 3409\n",
      "Epoch [3/5] Loss_D: 1.1581 Loss_G: 0.5760\n",
      "BATCH NUMBER 3410\n",
      "Epoch [3/5] Loss_D: 1.1857 Loss_G: 0.5759\n",
      "BATCH NUMBER 3411\n",
      "Epoch [3/5] Loss_D: 1.0831 Loss_G: 0.6288\n",
      "BATCH NUMBER 3412\n",
      "Epoch [3/5] Loss_D: 1.0538 Loss_G: 0.6521\n",
      "BATCH NUMBER 3413\n",
      "Epoch [3/5] Loss_D: 1.0552 Loss_G: 0.6495\n",
      "BATCH NUMBER 3414\n",
      "Epoch [3/5] Loss_D: 1.0579 Loss_G: 0.6472\n",
      "BATCH NUMBER 3415\n",
      "Epoch [3/5] Loss_D: 1.1904 Loss_G: 0.5641\n",
      "BATCH NUMBER 3416\n",
      "Epoch [3/5] Loss_D: 1.1094 Loss_G: 0.6068\n",
      "BATCH NUMBER 3417\n",
      "Epoch [3/5] Loss_D: 1.0633 Loss_G: 0.6442\n",
      "BATCH NUMBER 3418\n",
      "Epoch [3/5] Loss_D: 1.1518 Loss_G: 0.5959\n",
      "BATCH NUMBER 3419\n",
      "Epoch [3/5] Loss_D: 1.0542 Loss_G: 0.6522\n",
      "BATCH NUMBER 3420\n",
      "Epoch [3/5] Loss_D: 1.1070 Loss_G: 0.6163\n",
      "BATCH NUMBER 3421\n",
      "Epoch [3/5] Loss_D: 1.0682 Loss_G: 0.6488\n",
      "BATCH NUMBER 3422\n",
      "Epoch [3/5] Loss_D: 1.0947 Loss_G: 0.6282\n",
      "BATCH NUMBER 3423\n",
      "Epoch [3/5] Loss_D: 1.0527 Loss_G: 0.6539\n",
      "BATCH NUMBER 3424\n",
      "Epoch [3/5] Loss_D: 1.0854 Loss_G: 0.6334\n",
      "BATCH NUMBER 3425\n",
      "Epoch [3/5] Loss_D: 1.0797 Loss_G: 0.6311\n",
      "BATCH NUMBER 3426\n",
      "Epoch [3/5] Loss_D: 1.2101 Loss_G: 0.5491\n",
      "BATCH NUMBER 3427\n",
      "Epoch [3/5] Loss_D: 1.0894 Loss_G: 0.6316\n",
      "BATCH NUMBER 3428\n",
      "Epoch [3/5] Loss_D: 1.1041 Loss_G: 0.6187\n",
      "BATCH NUMBER 3429\n",
      "Epoch [3/5] Loss_D: 1.0592 Loss_G: 0.6461\n",
      "BATCH NUMBER 3430\n",
      "Epoch [3/5] Loss_D: 1.0908 Loss_G: 0.6287\n",
      "BATCH NUMBER 3431\n",
      "Epoch [3/5] Loss_D: 1.0956 Loss_G: 0.6166\n",
      "BATCH NUMBER 3432\n",
      "Epoch [3/5] Loss_D: 1.1178 Loss_G: 0.6082\n",
      "BATCH NUMBER 3433\n",
      "Epoch [3/5] Loss_D: 1.2413 Loss_G: 0.5386\n",
      "BATCH NUMBER 3434\n",
      "Epoch [3/5] Loss_D: 1.0852 Loss_G: 0.6346\n",
      "BATCH NUMBER 3435\n",
      "Epoch [3/5] Loss_D: 1.1811 Loss_G: 0.5712\n",
      "BATCH NUMBER 3436\n",
      "Epoch [3/5] Loss_D: 1.1414 Loss_G: 0.5892\n",
      "BATCH NUMBER 3437\n",
      "Epoch [3/5] Loss_D: 1.1171 Loss_G: 0.6166\n",
      "BATCH NUMBER 3438\n",
      "Epoch [3/5] Loss_D: 1.1484 Loss_G: 0.5817\n",
      "BATCH NUMBER 3439\n",
      "Epoch [3/5] Loss_D: 1.1607 Loss_G: 0.5888\n",
      "BATCH NUMBER 3440\n",
      "Epoch [3/5] Loss_D: 1.1212 Loss_G: 0.6136\n",
      "BATCH NUMBER 3441\n",
      "Epoch [3/5] Loss_D: 1.2684 Loss_G: 0.5170\n",
      "BATCH NUMBER 3442\n",
      "Epoch [3/5] Loss_D: 1.1282 Loss_G: 0.5981\n",
      "BATCH NUMBER 3443\n",
      "Epoch [3/5] Loss_D: 1.0450 Loss_G: 0.6580\n",
      "BATCH NUMBER 3444\n",
      "Epoch [3/5] Loss_D: 1.0499 Loss_G: 0.6554\n",
      "BATCH NUMBER 3445\n",
      "Epoch [3/5] Loss_D: 1.0530 Loss_G: 0.6519\n",
      "BATCH NUMBER 3446\n",
      "Epoch [3/5] Loss_D: 1.1892 Loss_G: 0.5720\n",
      "BATCH NUMBER 3447\n",
      "Epoch [3/5] Loss_D: 1.0659 Loss_G: 0.6427\n",
      "BATCH NUMBER 3448\n",
      "Epoch [3/5] Loss_D: 1.0365 Loss_G: 0.6674\n",
      "BATCH NUMBER 3449\n",
      "Epoch [3/5] Loss_D: 1.0432 Loss_G: 0.6606\n",
      "BATCH NUMBER 3450\n",
      "Epoch [3/5] Loss_D: 1.1188 Loss_G: 0.6144\n",
      "BATCH NUMBER 3451\n",
      "Epoch [3/5] Loss_D: 1.1142 Loss_G: 0.6099\n",
      "BATCH NUMBER 3452\n",
      "Epoch [3/5] Loss_D: 1.0507 Loss_G: 0.6552\n",
      "BATCH NUMBER 3453\n",
      "Epoch [3/5] Loss_D: 1.0958 Loss_G: 0.6258\n",
      "BATCH NUMBER 3454\n",
      "Epoch [3/5] Loss_D: 1.2161 Loss_G: 0.5506\n",
      "BATCH NUMBER 3455\n",
      "Epoch [3/5] Loss_D: 1.0833 Loss_G: 0.6364\n",
      "BATCH NUMBER 3456\n",
      "Epoch [3/5] Loss_D: 1.2706 Loss_G: 0.5155\n",
      "BATCH NUMBER 3457\n",
      "Epoch [3/5] Loss_D: 1.0316 Loss_G: 0.6727\n",
      "BATCH NUMBER 3458\n",
      "Epoch [3/5] Loss_D: 1.0389 Loss_G: 0.6639\n",
      "BATCH NUMBER 3459\n",
      "Epoch [3/5] Loss_D: 1.0481 Loss_G: 0.6590\n",
      "BATCH NUMBER 3460\n",
      "Epoch [3/5] Loss_D: 1.2037 Loss_G: 0.5603\n",
      "BATCH NUMBER 3461\n",
      "Epoch [3/5] Loss_D: 1.1575 Loss_G: 0.5918\n",
      "BATCH NUMBER 3462\n",
      "Epoch [3/5] Loss_D: 1.0147 Loss_G: 0.6853\n",
      "BATCH NUMBER 3463\n",
      "Epoch [3/5] Loss_D: 1.1635 Loss_G: 0.5863\n",
      "BATCH NUMBER 3464\n",
      "Epoch [3/5] Loss_D: 1.1868 Loss_G: 0.5685\n",
      "BATCH NUMBER 3465\n",
      "Epoch [3/5] Loss_D: 1.1437 Loss_G: 0.5772\n",
      "BATCH NUMBER 3466\n",
      "Epoch [3/5] Loss_D: 1.0528 Loss_G: 0.6521\n",
      "BATCH NUMBER 3467\n",
      "Epoch [3/5] Loss_D: 1.2042 Loss_G: 0.5611\n",
      "BATCH NUMBER 3468\n",
      "Epoch [3/5] Loss_D: 1.1891 Loss_G: 0.5637\n",
      "BATCH NUMBER 3469\n",
      "Epoch [3/5] Loss_D: 1.1043 Loss_G: 0.6123\n",
      "BATCH NUMBER 3470\n",
      "Epoch [3/5] Loss_D: 1.1330 Loss_G: 0.6041\n",
      "BATCH NUMBER 3471\n",
      "Epoch [3/5] Loss_D: 1.1765 Loss_G: 0.5763\n",
      "BATCH NUMBER 3472\n",
      "Epoch [3/5] Loss_D: 1.1535 Loss_G: 0.5863\n",
      "BATCH NUMBER 3473\n",
      "Epoch [3/5] Loss_D: 1.0549 Loss_G: 0.6525\n",
      "BATCH NUMBER 3474\n",
      "Epoch [3/5] Loss_D: 1.0421 Loss_G: 0.6632\n",
      "BATCH NUMBER 3475\n",
      "Epoch [3/5] Loss_D: 1.0564 Loss_G: 0.6501\n",
      "BATCH NUMBER 3476\n",
      "Epoch [3/5] Loss_D: 1.1654 Loss_G: 0.5841\n",
      "BATCH NUMBER 3477\n",
      "Epoch [3/5] Loss_D: 1.0336 Loss_G: 0.6691\n",
      "BATCH NUMBER 3478\n",
      "Epoch [3/5] Loss_D: 1.0746 Loss_G: 0.6352\n",
      "BATCH NUMBER 3479\n",
      "Epoch [3/5] Loss_D: 1.1281 Loss_G: 0.6076\n",
      "BATCH NUMBER 3480\n",
      "Epoch [3/5] Loss_D: 1.1629 Loss_G: 0.5869\n",
      "BATCH NUMBER 3481\n",
      "Epoch [3/5] Loss_D: 1.0314 Loss_G: 0.6713\n",
      "BATCH NUMBER 3482\n",
      "Epoch [3/5] Loss_D: 1.0868 Loss_G: 0.6350\n",
      "BATCH NUMBER 3483\n",
      "Epoch [3/5] Loss_D: 1.0346 Loss_G: 0.6683\n",
      "BATCH NUMBER 3484\n",
      "Epoch [3/5] Loss_D: 1.0482 Loss_G: 0.6558\n",
      "BATCH NUMBER 3485\n",
      "Epoch [3/5] Loss_D: 1.0454 Loss_G: 0.6591\n",
      "BATCH NUMBER 3486\n",
      "Epoch [3/5] Loss_D: 1.0365 Loss_G: 0.6656\n",
      "BATCH NUMBER 3487\n",
      "Epoch [3/5] Loss_D: 1.0537 Loss_G: 0.6522\n",
      "BATCH NUMBER 3488\n",
      "Epoch [3/5] Loss_D: 1.1919 Loss_G: 0.5632\n",
      "BATCH NUMBER 3489\n",
      "Epoch [3/5] Loss_D: 1.0381 Loss_G: 0.6665\n",
      "BATCH NUMBER 3490\n",
      "Epoch [3/5] Loss_D: 1.0598 Loss_G: 0.6493\n",
      "BATCH NUMBER 3491\n",
      "Epoch [3/5] Loss_D: 1.1551 Loss_G: 0.5920\n",
      "BATCH NUMBER 3492\n",
      "Epoch [3/5] Loss_D: 1.1643 Loss_G: 0.5840\n",
      "BATCH NUMBER 3493\n",
      "Epoch [3/5] Loss_D: 1.0859 Loss_G: 0.6333\n",
      "BATCH NUMBER 3494\n",
      "Epoch [3/5] Loss_D: 1.0374 Loss_G: 0.6657\n",
      "BATCH NUMBER 3495\n",
      "Epoch [3/5] Loss_D: 1.2000 Loss_G: 0.5646\n",
      "BATCH NUMBER 3496\n",
      "Epoch [3/5] Loss_D: 1.0575 Loss_G: 0.6484\n",
      "BATCH NUMBER 3497\n",
      "Epoch [3/5] Loss_D: 1.0719 Loss_G: 0.6451\n",
      "BATCH NUMBER 3498\n",
      "Epoch [3/5] Loss_D: 1.1239 Loss_G: 0.6033\n",
      "BATCH NUMBER 3499\n",
      "Epoch [3/5] Loss_D: 1.1181 Loss_G: 0.6060\n",
      "BATCH NUMBER 3500\n",
      "Epoch [3/5] Loss_D: 1.1107 Loss_G: 0.6222\n",
      "BATCH NUMBER 3501\n",
      "Epoch [3/5] Loss_D: 1.0539 Loss_G: 0.6521\n",
      "BATCH NUMBER 3502\n",
      "Epoch [3/5] Loss_D: 1.1147 Loss_G: 0.6108\n",
      "BATCH NUMBER 3503\n",
      "Epoch [3/5] Loss_D: 1.0589 Loss_G: 0.6491\n",
      "BATCH NUMBER 3504\n",
      "Epoch [3/5] Loss_D: 1.0769 Loss_G: 0.6438\n",
      "BATCH NUMBER 3505\n",
      "Epoch [3/5] Loss_D: 1.0991 Loss_G: 0.6135\n",
      "BATCH NUMBER 3506\n",
      "Epoch [3/5] Loss_D: 1.0549 Loss_G: 0.6511\n",
      "BATCH NUMBER 3507\n",
      "Epoch [3/5] Loss_D: 1.1069 Loss_G: 0.6158\n",
      "BATCH NUMBER 3508\n",
      "Epoch [3/5] Loss_D: 1.2018 Loss_G: 0.5558\n",
      "BATCH NUMBER 3509\n",
      "Epoch [3/5] Loss_D: 1.0718 Loss_G: 0.6385\n",
      "BATCH NUMBER 3510\n",
      "Epoch [3/5] Loss_D: 1.1811 Loss_G: 0.5728\n",
      "BATCH NUMBER 3511\n",
      "Epoch [3/5] Loss_D: 1.1663 Loss_G: 0.5827\n",
      "BATCH NUMBER 3512\n",
      "Epoch [3/5] Loss_D: 1.2672 Loss_G: 0.5188\n",
      "BATCH NUMBER 3513\n",
      "Epoch [3/5] Loss_D: 1.1209 Loss_G: 0.6048\n",
      "BATCH NUMBER 3514\n",
      "Epoch [3/5] Loss_D: 1.0582 Loss_G: 0.6577\n",
      "BATCH NUMBER 3515\n",
      "Epoch [3/5] Loss_D: 1.1439 Loss_G: 0.5939\n",
      "BATCH NUMBER 3516\n",
      "Epoch [3/5] Loss_D: 1.1281 Loss_G: 0.6079\n",
      "BATCH NUMBER 3517\n",
      "Epoch [3/5] Loss_D: 1.0778 Loss_G: 0.6413\n",
      "BATCH NUMBER 3518\n",
      "Epoch [3/5] Loss_D: 1.0709 Loss_G: 0.6352\n",
      "BATCH NUMBER 3519\n",
      "Epoch [3/5] Loss_D: 1.1047 Loss_G: 0.6270\n",
      "BATCH NUMBER 3520\n",
      "Epoch [3/5] Loss_D: 1.1878 Loss_G: 0.5649\n",
      "BATCH NUMBER 3521\n",
      "Epoch [3/5] Loss_D: 1.1756 Loss_G: 0.5746\n",
      "BATCH NUMBER 3522\n",
      "Epoch [3/5] Loss_D: 1.1657 Loss_G: 0.5877\n",
      "BATCH NUMBER 3523\n",
      "Epoch [3/5] Loss_D: 1.1459 Loss_G: 0.5920\n",
      "BATCH NUMBER 3524\n",
      "Epoch [3/5] Loss_D: 1.1168 Loss_G: 0.6172\n",
      "BATCH NUMBER 3525\n",
      "Epoch [3/5] Loss_D: 1.2550 Loss_G: 0.5278\n",
      "BATCH NUMBER 3526\n",
      "Epoch [3/5] Loss_D: 1.1248 Loss_G: 0.6093\n",
      "BATCH NUMBER 3527\n",
      "Epoch [3/5] Loss_D: 1.3403 Loss_G: 0.4740\n",
      "BATCH NUMBER 3528\n",
      "Epoch [3/5] Loss_D: 1.0272 Loss_G: 0.6741\n",
      "BATCH NUMBER 3529\n",
      "Epoch [3/5] Loss_D: 1.1171 Loss_G: 0.6169\n",
      "BATCH NUMBER 3530\n",
      "Epoch [3/5] Loss_D: 1.2174 Loss_G: 0.5483\n",
      "BATCH NUMBER 3531\n",
      "Epoch [3/5] Loss_D: 1.1626 Loss_G: 0.5788\n",
      "BATCH NUMBER 3532\n",
      "Epoch [3/5] Loss_D: 1.0969 Loss_G: 0.6244\n",
      "BATCH NUMBER 3533\n",
      "Epoch [3/5] Loss_D: 1.0403 Loss_G: 0.6625\n",
      "BATCH NUMBER 3534\n",
      "Epoch [3/5] Loss_D: 1.0453 Loss_G: 0.6602\n",
      "BATCH NUMBER 3535\n",
      "Epoch [3/5] Loss_D: 1.0652 Loss_G: 0.6448\n",
      "BATCH NUMBER 3536\n",
      "Epoch [3/5] Loss_D: 1.0732 Loss_G: 0.6348\n",
      "BATCH NUMBER 3537\n",
      "Epoch [3/5] Loss_D: 1.0492 Loss_G: 0.6561\n",
      "BATCH NUMBER 3538\n",
      "Epoch [3/5] Loss_D: 1.2352 Loss_G: 0.5453\n",
      "BATCH NUMBER 3539\n",
      "Epoch [3/5] Loss_D: 1.0939 Loss_G: 0.6275\n",
      "BATCH NUMBER 3540\n",
      "Epoch [3/5] Loss_D: 1.2417 Loss_G: 0.5379\n",
      "BATCH NUMBER 3541\n",
      "Epoch [3/5] Loss_D: 1.1300 Loss_G: 0.6043\n",
      "BATCH NUMBER 3542\n",
      "Epoch [3/5] Loss_D: 1.0940 Loss_G: 0.6284\n",
      "BATCH NUMBER 3543\n",
      "Epoch [3/5] Loss_D: 1.0399 Loss_G: 0.6632\n",
      "BATCH NUMBER 3544\n",
      "Epoch [3/5] Loss_D: 1.0616 Loss_G: 0.6474\n",
      "BATCH NUMBER 3545\n",
      "Epoch [3/5] Loss_D: 1.0655 Loss_G: 0.6448\n",
      "BATCH NUMBER 3546\n",
      "Epoch [3/5] Loss_D: 1.0531 Loss_G: 0.6522\n",
      "BATCH NUMBER 3547\n",
      "Epoch [3/5] Loss_D: 1.1782 Loss_G: 0.5644\n",
      "BATCH NUMBER 3548\n",
      "Epoch [3/5] Loss_D: 1.1472 Loss_G: 0.5967\n",
      "BATCH NUMBER 3549\n",
      "Epoch [3/5] Loss_D: 1.1051 Loss_G: 0.6184\n",
      "BATCH NUMBER 3550\n",
      "Epoch [3/5] Loss_D: 1.2213 Loss_G: 0.5464\n",
      "BATCH NUMBER 3551\n",
      "Epoch [3/5] Loss_D: 1.0310 Loss_G: 0.6708\n",
      "BATCH NUMBER 3552\n",
      "Epoch [3/5] Loss_D: 1.0906 Loss_G: 0.6319\n",
      "BATCH NUMBER 3553\n",
      "Epoch [3/5] Loss_D: 1.1040 Loss_G: 0.6165\n",
      "BATCH NUMBER 3554\n",
      "Epoch [3/5] Loss_D: 1.0846 Loss_G: 0.6275\n",
      "BATCH NUMBER 3555\n",
      "Epoch [3/5] Loss_D: 1.0590 Loss_G: 0.6470\n",
      "BATCH NUMBER 3556\n",
      "Epoch [3/5] Loss_D: 1.2175 Loss_G: 0.5521\n",
      "BATCH NUMBER 3557\n",
      "Epoch [3/5] Loss_D: 1.3411 Loss_G: 0.4725\n",
      "BATCH NUMBER 3558\n",
      "Epoch [3/5] Loss_D: 1.0964 Loss_G: 0.6179\n",
      "BATCH NUMBER 3559\n",
      "Epoch [3/5] Loss_D: 1.0480 Loss_G: 0.6576\n",
      "BATCH NUMBER 3560\n",
      "Epoch [3/5] Loss_D: 1.0912 Loss_G: 0.6285\n",
      "BATCH NUMBER 3561\n",
      "Epoch [3/5] Loss_D: 1.1841 Loss_G: 0.5771\n",
      "BATCH NUMBER 3562\n",
      "Epoch [3/5] Loss_D: 1.2834 Loss_G: 0.5054\n",
      "BATCH NUMBER 3563\n",
      "Epoch [3/5] Loss_D: 1.0727 Loss_G: 0.6385\n",
      "BATCH NUMBER 3564\n",
      "Epoch [3/5] Loss_D: 1.2277 Loss_G: 0.5403\n",
      "BATCH NUMBER 3565\n",
      "Epoch [3/5] Loss_D: 1.0928 Loss_G: 0.6271\n",
      "BATCH NUMBER 3566\n",
      "Epoch [3/5] Loss_D: 1.1758 Loss_G: 0.5763\n",
      "BATCH NUMBER 3567\n",
      "Epoch [3/5] Loss_D: 1.0298 Loss_G: 0.6729\n",
      "BATCH NUMBER 3568\n",
      "Epoch [3/5] Loss_D: 1.0659 Loss_G: 0.6423\n",
      "BATCH NUMBER 3569\n",
      "Epoch [3/5] Loss_D: 1.1451 Loss_G: 0.5925\n",
      "BATCH NUMBER 3570\n",
      "Epoch [3/5] Loss_D: 1.2033 Loss_G: 0.5530\n",
      "BATCH NUMBER 3571\n",
      "Epoch [3/5] Loss_D: 1.1504 Loss_G: 0.5885\n",
      "BATCH NUMBER 3572\n",
      "Epoch [3/5] Loss_D: 1.1588 Loss_G: 0.5827\n",
      "BATCH NUMBER 3573\n",
      "Epoch [3/5] Loss_D: 1.0327 Loss_G: 0.6716\n",
      "BATCH NUMBER 3574\n",
      "Epoch [3/5] Loss_D: 1.3296 Loss_G: 0.4829\n",
      "BATCH NUMBER 3575\n",
      "Epoch [3/5] Loss_D: 1.0996 Loss_G: 0.6125\n",
      "BATCH NUMBER 3576\n",
      "Epoch [3/5] Loss_D: 1.0812 Loss_G: 0.6377\n",
      "BATCH NUMBER 3577\n",
      "Epoch [3/5] Loss_D: 1.1461 Loss_G: 0.5914\n",
      "BATCH NUMBER 3578\n",
      "Epoch [3/5] Loss_D: 1.0902 Loss_G: 0.6253\n",
      "BATCH NUMBER 3579\n",
      "Epoch [3/5] Loss_D: 1.1474 Loss_G: 0.5811\n",
      "BATCH NUMBER 3580\n",
      "Epoch [3/5] Loss_D: 1.0347 Loss_G: 0.6685\n",
      "BATCH NUMBER 3581\n",
      "Epoch [3/5] Loss_D: 1.2364 Loss_G: 0.5425\n",
      "BATCH NUMBER 3582\n",
      "Epoch [3/5] Loss_D: 1.1188 Loss_G: 0.6081\n",
      "BATCH NUMBER 3583\n",
      "Epoch [3/5] Loss_D: 1.0749 Loss_G: 0.6427\n",
      "BATCH NUMBER 3584\n",
      "Epoch [3/5] Loss_D: 1.0608 Loss_G: 0.6465\n",
      "BATCH NUMBER 3585\n",
      "Epoch [3/5] Loss_D: 1.1113 Loss_G: 0.6128\n",
      "BATCH NUMBER 3586\n",
      "Epoch [3/5] Loss_D: 1.0690 Loss_G: 0.6479\n",
      "BATCH NUMBER 3587\n",
      "Epoch [3/5] Loss_D: 1.1597 Loss_G: 0.5889\n",
      "BATCH NUMBER 3588\n",
      "Epoch [3/5] Loss_D: 1.0761 Loss_G: 0.6445\n",
      "BATCH NUMBER 3589\n",
      "Epoch [3/5] Loss_D: 1.1567 Loss_G: 0.5929\n",
      "BATCH NUMBER 3590\n",
      "Epoch [3/5] Loss_D: 1.1178 Loss_G: 0.6092\n",
      "BATCH NUMBER 3591\n",
      "Epoch [3/5] Loss_D: 1.1567 Loss_G: 0.5828\n",
      "BATCH NUMBER 3592\n",
      "Epoch [3/5] Loss_D: 1.1541 Loss_G: 0.5846\n",
      "BATCH NUMBER 3593\n",
      "Epoch [3/5] Loss_D: 1.0793 Loss_G: 0.6397\n",
      "BATCH NUMBER 3594\n",
      "Epoch [3/5] Loss_D: 1.1970 Loss_G: 0.5589\n",
      "BATCH NUMBER 3595\n",
      "Epoch [3/5] Loss_D: 1.2185 Loss_G: 0.5575\n",
      "BATCH NUMBER 3596\n",
      "Epoch [3/5] Loss_D: 1.0801 Loss_G: 0.6289\n",
      "BATCH NUMBER 3597\n",
      "Epoch [3/5] Loss_D: 1.0404 Loss_G: 0.6649\n",
      "BATCH NUMBER 3598\n",
      "Epoch [3/5] Loss_D: 1.0863 Loss_G: 0.6331\n",
      "BATCH NUMBER 3599\n",
      "Epoch [3/5] Loss_D: 1.1649 Loss_G: 0.5767\n",
      "BATCH NUMBER 3600\n",
      "Epoch [3/5] Loss_D: 1.1262 Loss_G: 0.5989\n",
      "BATCH NUMBER 3601\n",
      "Epoch [3/5] Loss_D: 1.3309 Loss_G: 0.4803\n",
      "BATCH NUMBER 3602\n",
      "Epoch [3/5] Loss_D: 1.1648 Loss_G: 0.5772\n",
      "BATCH NUMBER 3603\n",
      "Epoch [3/5] Loss_D: 1.2115 Loss_G: 0.5548\n",
      "BATCH NUMBER 3604\n",
      "Epoch [3/5] Loss_D: 1.0530 Loss_G: 0.6529\n",
      "BATCH NUMBER 3605\n",
      "Epoch [3/5] Loss_D: 1.3158 Loss_G: 0.4932\n",
      "BATCH NUMBER 3606\n",
      "Epoch [3/5] Loss_D: 1.0891 Loss_G: 0.6314\n",
      "BATCH NUMBER 3607\n",
      "Epoch [3/5] Loss_D: 1.1105 Loss_G: 0.6063\n",
      "BATCH NUMBER 3608\n",
      "Epoch [3/5] Loss_D: 1.2739 Loss_G: 0.5193\n",
      "BATCH NUMBER 3609\n",
      "Epoch [3/5] Loss_D: 1.1111 Loss_G: 0.6146\n",
      "BATCH NUMBER 3610\n",
      "Epoch [3/5] Loss_D: 1.0742 Loss_G: 0.6368\n",
      "BATCH NUMBER 3611\n",
      "Epoch [3/5] Loss_D: 1.1088 Loss_G: 0.6236\n",
      "BATCH NUMBER 3612\n",
      "Epoch [3/5] Loss_D: 1.1475 Loss_G: 0.5994\n",
      "BATCH NUMBER 3613\n",
      "Epoch [3/5] Loss_D: 1.0886 Loss_G: 0.6309\n",
      "BATCH NUMBER 3614\n",
      "Epoch [3/5] Loss_D: 1.0425 Loss_G: 0.6612\n",
      "BATCH NUMBER 3615\n",
      "Epoch [3/5] Loss_D: 1.0699 Loss_G: 0.6472\n",
      "BATCH NUMBER 3616\n",
      "Epoch [3/5] Loss_D: 1.0446 Loss_G: 0.6614\n",
      "BATCH NUMBER 3617\n",
      "Epoch [3/5] Loss_D: 1.0215 Loss_G: 0.6789\n",
      "BATCH NUMBER 3618\n",
      "Epoch [3/5] Loss_D: 1.1507 Loss_G: 0.5967\n",
      "BATCH NUMBER 3619\n",
      "Epoch [3/5] Loss_D: 1.0773 Loss_G: 0.6328\n",
      "BATCH NUMBER 3620\n",
      "Epoch [3/5] Loss_D: 1.2620 Loss_G: 0.5298\n",
      "BATCH NUMBER 3621\n",
      "Epoch [3/5] Loss_D: 1.0790 Loss_G: 0.6315\n",
      "BATCH NUMBER 3622\n",
      "Epoch [3/5] Loss_D: 1.1537 Loss_G: 0.5854\n",
      "BATCH NUMBER 3623\n",
      "Epoch [3/5] Loss_D: 1.0969 Loss_G: 0.6236\n",
      "BATCH NUMBER 3624\n",
      "Epoch [3/5] Loss_D: 1.1771 Loss_G: 0.5760\n",
      "BATCH NUMBER 3625\n",
      "Epoch [3/5] Loss_D: 1.1066 Loss_G: 0.6192\n",
      "BATCH NUMBER 3626\n",
      "Epoch [3/5] Loss_D: 1.0861 Loss_G: 0.6237\n",
      "BATCH NUMBER 3627\n",
      "Epoch [3/5] Loss_D: 1.2724 Loss_G: 0.5139\n",
      "BATCH NUMBER 3628\n",
      "Epoch [3/5] Loss_D: 1.1179 Loss_G: 0.6083\n",
      "BATCH NUMBER 3629\n",
      "Epoch [3/5] Loss_D: 1.0146 Loss_G: 0.6853\n",
      "BATCH NUMBER 3630\n",
      "Epoch [3/5] Loss_D: 1.0939 Loss_G: 0.6278\n",
      "BATCH NUMBER 3631\n",
      "Epoch [3/5] Loss_D: 1.0590 Loss_G: 0.6462\n",
      "BATCH NUMBER 3632\n",
      "Epoch [3/5] Loss_D: 1.0760 Loss_G: 0.6328\n",
      "BATCH NUMBER 3633\n",
      "Epoch [3/5] Loss_D: 1.3239 Loss_G: 0.4964\n",
      "BATCH NUMBER 3634\n",
      "Epoch [3/5] Loss_D: 1.2001 Loss_G: 0.5642\n",
      "BATCH NUMBER 3635\n",
      "Epoch [3/5] Loss_D: 1.0222 Loss_G: 0.6790\n",
      "BATCH NUMBER 3636\n",
      "Epoch [3/5] Loss_D: 1.2506 Loss_G: 0.5316\n",
      "BATCH NUMBER 3637\n",
      "Epoch [3/5] Loss_D: 1.0478 Loss_G: 0.6582\n",
      "BATCH NUMBER 3638\n",
      "Epoch [3/5] Loss_D: 1.2364 Loss_G: 0.5349\n",
      "BATCH NUMBER 3639\n",
      "Epoch [3/5] Loss_D: 1.0757 Loss_G: 0.6445\n",
      "BATCH NUMBER 3640\n",
      "Epoch [3/5] Loss_D: 1.2371 Loss_G: 0.5428\n",
      "BATCH NUMBER 3641\n",
      "Epoch [3/5] Loss_D: 1.0766 Loss_G: 0.6334\n",
      "BATCH NUMBER 3642\n",
      "Epoch [3/5] Loss_D: 1.0864 Loss_G: 0.6339\n",
      "BATCH NUMBER 3643\n",
      "Epoch [3/5] Loss_D: 1.1409 Loss_G: 0.5953\n",
      "BATCH NUMBER 3644\n",
      "Epoch [3/5] Loss_D: 1.1094 Loss_G: 0.6227\n",
      "BATCH NUMBER 3645\n",
      "Epoch [3/5] Loss_D: 1.0413 Loss_G: 0.6623\n",
      "BATCH NUMBER 3646\n",
      "Epoch [3/5] Loss_D: 1.1753 Loss_G: 0.5769\n",
      "BATCH NUMBER 3647\n",
      "Epoch [3/5] Loss_D: 1.1116 Loss_G: 0.6112\n",
      "BATCH NUMBER 3648\n",
      "Epoch [3/5] Loss_D: 1.1869 Loss_G: 0.5585\n",
      "BATCH NUMBER 3649\n",
      "Epoch [3/5] Loss_D: 1.2224 Loss_G: 0.5441\n",
      "BATCH NUMBER 3650\n",
      "Epoch [3/5] Loss_D: 1.0803 Loss_G: 0.6405\n",
      "BATCH NUMBER 3651\n",
      "Epoch [3/5] Loss_D: 1.0525 Loss_G: 0.6529\n",
      "BATCH NUMBER 3652\n",
      "Epoch [3/5] Loss_D: 1.2980 Loss_G: 0.5083\n",
      "BATCH NUMBER 3653\n",
      "Epoch [3/5] Loss_D: 1.1830 Loss_G: 0.5697\n",
      "BATCH NUMBER 3654\n",
      "Epoch [3/5] Loss_D: 1.1099 Loss_G: 0.6126\n",
      "BATCH NUMBER 3655\n",
      "Epoch [3/5] Loss_D: 1.1251 Loss_G: 0.5996\n",
      "BATCH NUMBER 3656\n",
      "Epoch [3/5] Loss_D: 1.1227 Loss_G: 0.6113\n",
      "BATCH NUMBER 3657\n",
      "Epoch [3/5] Loss_D: 1.1082 Loss_G: 0.6070\n",
      "BATCH NUMBER 3658\n",
      "Epoch [3/5] Loss_D: 1.0785 Loss_G: 0.6388\n",
      "BATCH NUMBER 3659\n",
      "Epoch [3/5] Loss_D: 1.0510 Loss_G: 0.6543\n",
      "BATCH NUMBER 3660\n",
      "Epoch [3/5] Loss_D: 1.3173 Loss_G: 0.4832\n",
      "BATCH NUMBER 3661\n",
      "Epoch [3/5] Loss_D: 1.2140 Loss_G: 0.5577\n",
      "BATCH NUMBER 3662\n",
      "Epoch [3/5] Loss_D: 1.1589 Loss_G: 0.5899\n",
      "BATCH NUMBER 3663\n",
      "Epoch [3/5] Loss_D: 1.1234 Loss_G: 0.6114\n",
      "BATCH NUMBER 3664\n",
      "Epoch [3/5] Loss_D: 1.0369 Loss_G: 0.6655\n",
      "BATCH NUMBER 3665\n",
      "Epoch [3/5] Loss_D: 1.2246 Loss_G: 0.5446\n",
      "BATCH NUMBER 3666\n",
      "Epoch [3/5] Loss_D: 1.0516 Loss_G: 0.6549\n",
      "BATCH NUMBER 3667\n",
      "Epoch [3/5] Loss_D: 1.1065 Loss_G: 0.6165\n",
      "BATCH NUMBER 3668\n",
      "Epoch [3/5] Loss_D: 1.0348 Loss_G: 0.6681\n",
      "BATCH NUMBER 3669\n",
      "Epoch [3/5] Loss_D: 1.2724 Loss_G: 0.5204\n",
      "BATCH NUMBER 3670\n",
      "Epoch [3/5] Loss_D: 1.0916 Loss_G: 0.6279\n",
      "BATCH NUMBER 3671\n",
      "Epoch [3/5] Loss_D: 1.1149 Loss_G: 0.6109\n",
      "BATCH NUMBER 3672\n",
      "Epoch [3/5] Loss_D: 1.1300 Loss_G: 0.6054\n",
      "BATCH NUMBER 3673\n",
      "Epoch [3/5] Loss_D: 1.0399 Loss_G: 0.6638\n",
      "BATCH NUMBER 3674\n",
      "Epoch [3/5] Loss_D: 1.0828 Loss_G: 0.6359\n",
      "BATCH NUMBER 3675\n",
      "Epoch [3/5] Loss_D: 1.1351 Loss_G: 0.5930\n",
      "BATCH NUMBER 3676\n",
      "Epoch [3/5] Loss_D: 1.0735 Loss_G: 0.6440\n",
      "BATCH NUMBER 3677\n",
      "Epoch [3/5] Loss_D: 1.0955 Loss_G: 0.6155\n",
      "BATCH NUMBER 3678\n",
      "Epoch [3/5] Loss_D: 1.1108 Loss_G: 0.6210\n",
      "BATCH NUMBER 3679\n",
      "Epoch [3/5] Loss_D: 1.1313 Loss_G: 0.5958\n",
      "BATCH NUMBER 3680\n",
      "Epoch [3/5] Loss_D: 1.0722 Loss_G: 0.6458\n",
      "BATCH NUMBER 3681\n",
      "Epoch [3/5] Loss_D: 1.0451 Loss_G: 0.6580\n",
      "BATCH NUMBER 3682\n",
      "Epoch [3/5] Loss_D: 1.0941 Loss_G: 0.6266\n",
      "BATCH NUMBER 3683\n",
      "Epoch [3/5] Loss_D: 1.0970 Loss_G: 0.6253\n",
      "BATCH NUMBER 3684\n",
      "Epoch [3/5] Loss_D: 1.1575 Loss_G: 0.5902\n",
      "BATCH NUMBER 3685\n",
      "Epoch [3/5] Loss_D: 1.3173 Loss_G: 0.4932\n",
      "BATCH NUMBER 3686\n",
      "Epoch [3/5] Loss_D: 1.0690 Loss_G: 0.6489\n",
      "BATCH NUMBER 3687\n",
      "Epoch [3/5] Loss_D: 1.1048 Loss_G: 0.6186\n",
      "BATCH NUMBER 3688\n",
      "Epoch [3/5] Loss_D: 1.1648 Loss_G: 0.5850\n",
      "BATCH NUMBER 3689\n",
      "Epoch [3/5] Loss_D: 1.2208 Loss_G: 0.5487\n",
      "BATCH NUMBER 3690\n",
      "Epoch [3/5] Loss_D: 1.0979 Loss_G: 0.6147\n",
      "BATCH NUMBER 3691\n",
      "Epoch [3/5] Loss_D: 1.3787 Loss_G: 0.4509\n",
      "BATCH NUMBER 3692\n",
      "Epoch [3/5] Loss_D: 1.1636 Loss_G: 0.5873\n",
      "BATCH NUMBER 3693\n",
      "Epoch [3/5] Loss_D: 1.0238 Loss_G: 0.6769\n",
      "BATCH NUMBER 3694\n",
      "Epoch [3/5] Loss_D: 1.1114 Loss_G: 0.6130\n",
      "BATCH NUMBER 3695\n",
      "Epoch [3/5] Loss_D: 1.1016 Loss_G: 0.6215\n",
      "BATCH NUMBER 3696\n",
      "Epoch [3/5] Loss_D: 1.1459 Loss_G: 0.6017\n",
      "BATCH NUMBER 3697\n",
      "Epoch [3/5] Loss_D: 1.1422 Loss_G: 0.5957\n",
      "BATCH NUMBER 3698\n",
      "Epoch [3/5] Loss_D: 1.1064 Loss_G: 0.6161\n",
      "BATCH NUMBER 3699\n",
      "Epoch [3/5] Loss_D: 1.0550 Loss_G: 0.6499\n",
      "BATCH NUMBER 3700\n",
      "Epoch [3/5] Loss_D: 1.1786 Loss_G: 0.5645\n",
      "BATCH NUMBER 3701\n",
      "Epoch [3/5] Loss_D: 1.0334 Loss_G: 0.6695\n",
      "BATCH NUMBER 3702\n",
      "Epoch [3/5] Loss_D: 1.1274 Loss_G: 0.6078\n",
      "BATCH NUMBER 3703\n",
      "Epoch [3/5] Loss_D: 1.0331 Loss_G: 0.6698\n",
      "BATCH NUMBER 3704\n",
      "Epoch [3/5] Loss_D: 1.1866 Loss_G: 0.5674\n",
      "BATCH NUMBER 3705\n",
      "Epoch [3/5] Loss_D: 1.1463 Loss_G: 0.5915\n",
      "BATCH NUMBER 3706\n",
      "Epoch [3/5] Loss_D: 1.0885 Loss_G: 0.6310\n",
      "BATCH NUMBER 3707\n",
      "Epoch [3/5] Loss_D: 1.0740 Loss_G: 0.6446\n",
      "BATCH NUMBER 3708\n",
      "Epoch [3/5] Loss_D: 1.0647 Loss_G: 0.6507\n",
      "BATCH NUMBER 3709\n",
      "Epoch [3/5] Loss_D: 1.1245 Loss_G: 0.6039\n",
      "BATCH NUMBER 3710\n",
      "Epoch [3/5] Loss_D: 1.1265 Loss_G: 0.6081\n",
      "BATCH NUMBER 3711\n",
      "Epoch [3/5] Loss_D: 1.1382 Loss_G: 0.5975\n",
      "BATCH NUMBER 3712\n",
      "Epoch [3/5] Loss_D: 1.1018 Loss_G: 0.6217\n",
      "BATCH NUMBER 3713\n",
      "Epoch [3/5] Loss_D: 1.0388 Loss_G: 0.6652\n",
      "BATCH NUMBER 3714\n",
      "Epoch [3/5] Loss_D: 1.0404 Loss_G: 0.6634\n",
      "BATCH NUMBER 3715\n",
      "Epoch [3/5] Loss_D: 1.2703 Loss_G: 0.5224\n",
      "BATCH NUMBER 3716\n",
      "Epoch [3/5] Loss_D: 1.0815 Loss_G: 0.6294\n",
      "BATCH NUMBER 3717\n",
      "Epoch [3/5] Loss_D: 1.1232 Loss_G: 0.6012\n",
      "BATCH NUMBER 3718\n",
      "Epoch [3/5] Loss_D: 1.1756 Loss_G: 0.5777\n",
      "BATCH NUMBER 3719\n",
      "Epoch [3/5] Loss_D: 1.0890 Loss_G: 0.6239\n",
      "BATCH NUMBER 3720\n",
      "Epoch [3/5] Loss_D: 1.0788 Loss_G: 0.6394\n",
      "BATCH NUMBER 3721\n",
      "Epoch [3/5] Loss_D: 1.0832 Loss_G: 0.6355\n",
      "BATCH NUMBER 3722\n",
      "Epoch [3/5] Loss_D: 1.0715 Loss_G: 0.6459\n",
      "BATCH NUMBER 3723\n",
      "Epoch [3/5] Loss_D: 1.0652 Loss_G: 0.6432\n",
      "BATCH NUMBER 3724\n",
      "Epoch [3/5] Loss_D: 1.2988 Loss_G: 0.4982\n",
      "BATCH NUMBER 3725\n",
      "Epoch [3/5] Loss_D: 1.0565 Loss_G: 0.6495\n",
      "BATCH NUMBER 3726\n",
      "Epoch [3/5] Loss_D: 1.1854 Loss_G: 0.5676\n",
      "BATCH NUMBER 3727\n",
      "Epoch [3/5] Loss_D: 1.1116 Loss_G: 0.6125\n",
      "BATCH NUMBER 3728\n",
      "Epoch [3/5] Loss_D: 1.1940 Loss_G: 0.5605\n",
      "BATCH NUMBER 3729\n",
      "Epoch [3/5] Loss_D: 1.0311 Loss_G: 0.6716\n",
      "BATCH NUMBER 3730\n",
      "Epoch [3/5] Loss_D: 1.0618 Loss_G: 0.6453\n",
      "BATCH NUMBER 3731\n",
      "Epoch [3/5] Loss_D: 1.0639 Loss_G: 0.6523\n",
      "BATCH NUMBER 3732\n",
      "Epoch [3/5] Loss_D: 1.1008 Loss_G: 0.6210\n",
      "BATCH NUMBER 3733\n",
      "Epoch [3/5] Loss_D: 1.1455 Loss_G: 0.5921\n",
      "BATCH NUMBER 3734\n",
      "Epoch [3/5] Loss_D: 1.0691 Loss_G: 0.6394\n",
      "BATCH NUMBER 3735\n",
      "Epoch [3/5] Loss_D: 1.2538 Loss_G: 0.5277\n",
      "BATCH NUMBER 3736\n",
      "Epoch [3/5] Loss_D: 1.0604 Loss_G: 0.6458\n",
      "BATCH NUMBER 3737\n",
      "Epoch [3/5] Loss_D: 1.1314 Loss_G: 0.6050\n",
      "BATCH NUMBER 3738\n",
      "Epoch [3/5] Loss_D: 1.0498 Loss_G: 0.6553\n",
      "BATCH NUMBER 3739\n",
      "Epoch [3/5] Loss_D: 1.2657 Loss_G: 0.5188\n",
      "BATCH NUMBER 3740\n",
      "Epoch [3/5] Loss_D: 1.1081 Loss_G: 0.6237\n",
      "BATCH NUMBER 3741\n",
      "Epoch [3/5] Loss_D: 1.1184 Loss_G: 0.6163\n",
      "BATCH NUMBER 3742\n",
      "Epoch [3/5] Loss_D: 1.1041 Loss_G: 0.6212\n",
      "BATCH NUMBER 3743\n",
      "Epoch [3/5] Loss_D: 1.0791 Loss_G: 0.6320\n",
      "BATCH NUMBER 3744\n",
      "Epoch [3/5] Loss_D: 1.0305 Loss_G: 0.6717\n",
      "BATCH NUMBER 3745\n",
      "Epoch [3/5] Loss_D: 1.1524 Loss_G: 0.5881\n",
      "BATCH NUMBER 3746\n",
      "Epoch [3/5] Loss_D: 1.1996 Loss_G: 0.5552\n",
      "BATCH NUMBER 3747\n",
      "Epoch [3/5] Loss_D: 1.1162 Loss_G: 0.6162\n",
      "BATCH NUMBER 3748\n",
      "Epoch [3/5] Loss_D: 1.3834 Loss_G: 0.4548\n",
      "BATCH NUMBER 3749\n",
      "Epoch [3/5] Loss_D: 1.0613 Loss_G: 0.6465\n",
      "BATCH NUMBER 3750\n",
      "Epoch [3/5] Loss_D: 1.0492 Loss_G: 0.6573\n",
      "BATCH NUMBER 3751\n",
      "Epoch [3/5] Loss_D: 1.2678 Loss_G: 0.5401\n",
      "BATCH NUMBER 3752\n",
      "Epoch [3/5] Loss_D: 1.1640 Loss_G: 0.5860\n",
      "BATCH NUMBER 3753\n",
      "Epoch [3/5] Loss_D: 1.0300 Loss_G: 0.6723\n",
      "BATCH NUMBER 3754\n",
      "Epoch [3/5] Loss_D: 1.1112 Loss_G: 0.6146\n",
      "BATCH NUMBER 3755\n",
      "Epoch [3/5] Loss_D: 1.0660 Loss_G: 0.6417\n",
      "BATCH NUMBER 3756\n",
      "Epoch [3/5] Loss_D: 1.0487 Loss_G: 0.6569\n",
      "BATCH NUMBER 3757\n",
      "Epoch [3/5] Loss_D: 1.0745 Loss_G: 0.6441\n",
      "BATCH NUMBER 3758\n",
      "Epoch [3/5] Loss_D: 1.1406 Loss_G: 0.5958\n",
      "BATCH NUMBER 3759\n",
      "Epoch [3/5] Loss_D: 1.1174 Loss_G: 0.6164\n",
      "BATCH NUMBER 3760\n",
      "Epoch [3/5] Loss_D: 1.1175 Loss_G: 0.6156\n",
      "BATCH NUMBER 3761\n",
      "Epoch [3/5] Loss_D: 1.0615 Loss_G: 0.6464\n",
      "BATCH NUMBER 3762\n",
      "Epoch [3/5] Loss_D: 1.0887 Loss_G: 0.6232\n",
      "BATCH NUMBER 3763\n",
      "Epoch [3/5] Loss_D: 1.1480 Loss_G: 0.5901\n",
      "BATCH NUMBER 3764\n",
      "Epoch [3/5] Loss_D: 1.2801 Loss_G: 0.5147\n",
      "BATCH NUMBER 3765\n",
      "Epoch [3/5] Loss_D: 1.0406 Loss_G: 0.6636\n",
      "BATCH NUMBER 3766\n",
      "Epoch [3/5] Loss_D: 1.1590 Loss_G: 0.5898\n",
      "BATCH NUMBER 3767\n",
      "Epoch [3/5] Loss_D: 1.0923 Loss_G: 0.6304\n",
      "BATCH NUMBER 3768\n",
      "Epoch [3/5] Loss_D: 1.1921 Loss_G: 0.5640\n",
      "BATCH NUMBER 3769\n",
      "Epoch [3/5] Loss_D: 1.1804 Loss_G: 0.5740\n",
      "BATCH NUMBER 3770\n",
      "Epoch [3/5] Loss_D: 1.1083 Loss_G: 0.6134\n",
      "BATCH NUMBER 3771\n",
      "Epoch [3/5] Loss_D: 1.0912 Loss_G: 0.6288\n",
      "BATCH NUMBER 3772\n",
      "Epoch [3/5] Loss_D: 1.0941 Loss_G: 0.6262\n",
      "BATCH NUMBER 3773\n",
      "Epoch [3/5] Loss_D: 1.1075 Loss_G: 0.6210\n",
      "BATCH NUMBER 3774\n",
      "Epoch [3/5] Loss_D: 1.1962 Loss_G: 0.5671\n",
      "BATCH NUMBER 3775\n",
      "Epoch [3/5] Loss_D: 1.0541 Loss_G: 0.6514\n",
      "BATCH NUMBER 3776\n",
      "Epoch [3/5] Loss_D: 1.0491 Loss_G: 0.6555\n",
      "BATCH NUMBER 3777\n",
      "Epoch [3/5] Loss_D: 1.0346 Loss_G: 0.6682\n",
      "BATCH NUMBER 3778\n",
      "Epoch [3/5] Loss_D: 1.0742 Loss_G: 0.6434\n",
      "BATCH NUMBER 3779\n",
      "Epoch [3/5] Loss_D: 1.0452 Loss_G: 0.6585\n",
      "BATCH NUMBER 3780\n",
      "Epoch [3/5] Loss_D: 1.0445 Loss_G: 0.6598\n",
      "BATCH NUMBER 3781\n",
      "Epoch [3/5] Loss_D: 1.1805 Loss_G: 0.5705\n",
      "BATCH NUMBER 3782\n",
      "Epoch [3/5] Loss_D: 1.2129 Loss_G: 0.5526\n",
      "BATCH NUMBER 3783\n",
      "Epoch [3/5] Loss_D: 1.0752 Loss_G: 0.6336\n",
      "BATCH NUMBER 3784\n",
      "Epoch [3/5] Loss_D: 1.1797 Loss_G: 0.5723\n",
      "BATCH NUMBER 3785\n",
      "Epoch [3/5] Loss_D: 1.1141 Loss_G: 0.6184\n",
      "BATCH NUMBER 3786\n",
      "Epoch [3/5] Loss_D: 1.1977 Loss_G: 0.5585\n",
      "BATCH NUMBER 3787\n",
      "Epoch [3/5] Loss_D: 1.0437 Loss_G: 0.6602\n",
      "BATCH NUMBER 3788\n",
      "Epoch [3/5] Loss_D: 1.0943 Loss_G: 0.6273\n",
      "BATCH NUMBER 3789\n",
      "Epoch [3/5] Loss_D: 1.0546 Loss_G: 0.6509\n",
      "BATCH NUMBER 3790\n",
      "Epoch [3/5] Loss_D: 1.0869 Loss_G: 0.6334\n",
      "BATCH NUMBER 3791\n",
      "Epoch [3/5] Loss_D: 1.3323 Loss_G: 0.4800\n",
      "BATCH NUMBER 3792\n",
      "Epoch [3/5] Loss_D: 1.0672 Loss_G: 0.6396\n",
      "BATCH NUMBER 3793\n",
      "Epoch [3/5] Loss_D: 1.2209 Loss_G: 0.5566\n",
      "BATCH NUMBER 3794\n",
      "Epoch [3/5] Loss_D: 1.1328 Loss_G: 0.6048\n",
      "BATCH NUMBER 3795\n",
      "Epoch [3/5] Loss_D: 1.0487 Loss_G: 0.6556\n",
      "BATCH NUMBER 3796\n",
      "Epoch [3/5] Loss_D: 1.0542 Loss_G: 0.6515\n",
      "BATCH NUMBER 3797\n",
      "Epoch [3/5] Loss_D: 1.1761 Loss_G: 0.5736\n",
      "BATCH NUMBER 3798\n",
      "Epoch [3/5] Loss_D: 1.1096 Loss_G: 0.6223\n",
      "BATCH NUMBER 3799\n",
      "Epoch [3/5] Loss_D: 1.2595 Loss_G: 0.5235\n",
      "BATCH NUMBER 3800\n",
      "Epoch [3/5] Loss_D: 1.2034 Loss_G: 0.5619\n",
      "BATCH NUMBER 3801\n",
      "Epoch [3/5] Loss_D: 1.1675 Loss_G: 0.5727\n",
      "BATCH NUMBER 3802\n",
      "Epoch [3/5] Loss_D: 1.1572 Loss_G: 0.5911\n",
      "BATCH NUMBER 3803\n",
      "Epoch [3/5] Loss_D: 1.1141 Loss_G: 0.6181\n",
      "BATCH NUMBER 3804\n",
      "Epoch [3/5] Loss_D: 1.0578 Loss_G: 0.6489\n",
      "BATCH NUMBER 3805\n",
      "Epoch [3/5] Loss_D: 1.0402 Loss_G: 0.6629\n",
      "BATCH NUMBER 3806\n",
      "Epoch [3/5] Loss_D: 1.0440 Loss_G: 0.6603\n",
      "BATCH NUMBER 3807\n",
      "Epoch [3/5] Loss_D: 1.0603 Loss_G: 0.6475\n",
      "BATCH NUMBER 3808\n",
      "Epoch [3/5] Loss_D: 1.0376 Loss_G: 0.6651\n",
      "BATCH NUMBER 3809\n",
      "Epoch [3/5] Loss_D: 1.0260 Loss_G: 0.6755\n",
      "BATCH NUMBER 3810\n",
      "Epoch [3/5] Loss_D: 1.0829 Loss_G: 0.6359\n",
      "BATCH NUMBER 3811\n",
      "Epoch [3/5] Loss_D: 1.1106 Loss_G: 0.6215\n",
      "BATCH NUMBER 3812\n",
      "Epoch [3/5] Loss_D: 1.1064 Loss_G: 0.6072\n",
      "BATCH NUMBER 3813\n",
      "Epoch [3/5] Loss_D: 1.1251 Loss_G: 0.6139\n",
      "BATCH NUMBER 3814\n",
      "Epoch [3/5] Loss_D: 1.1244 Loss_G: 0.6011\n",
      "BATCH NUMBER 3815\n",
      "Epoch [3/5] Loss_D: 1.0566 Loss_G: 0.6499\n",
      "BATCH NUMBER 3816\n",
      "Epoch [3/5] Loss_D: 1.2167 Loss_G: 0.5592\n",
      "BATCH NUMBER 3817\n",
      "Epoch [3/5] Loss_D: 1.0530 Loss_G: 0.6536\n",
      "BATCH NUMBER 3818\n",
      "Epoch [3/5] Loss_D: 1.0477 Loss_G: 0.6559\n",
      "BATCH NUMBER 3819\n",
      "Epoch [3/5] Loss_D: 1.0818 Loss_G: 0.6293\n",
      "BATCH NUMBER 3820\n",
      "Epoch [3/5] Loss_D: 1.0668 Loss_G: 0.6402\n",
      "BATCH NUMBER 3821\n",
      "Epoch [3/5] Loss_D: 1.1042 Loss_G: 0.6087\n",
      "BATCH NUMBER 3822\n",
      "Epoch [3/5] Loss_D: 1.1754 Loss_G: 0.5864\n",
      "BATCH NUMBER 3823\n",
      "Epoch [3/5] Loss_D: 1.0688 Loss_G: 0.6389\n",
      "BATCH NUMBER 3824\n",
      "Epoch [3/5] Loss_D: 1.1746 Loss_G: 0.5791\n",
      "BATCH NUMBER 3825\n",
      "Epoch [3/5] Loss_D: 1.1402 Loss_G: 0.5971\n",
      "BATCH NUMBER 3826\n",
      "Epoch [3/5] Loss_D: 1.0472 Loss_G: 0.6569\n",
      "BATCH NUMBER 3827\n",
      "Epoch [3/5] Loss_D: 1.1069 Loss_G: 0.6091\n",
      "BATCH NUMBER 3828\n",
      "Epoch [3/5] Loss_D: 1.1050 Loss_G: 0.6180\n",
      "BATCH NUMBER 3829\n",
      "Epoch [3/5] Loss_D: 1.1217 Loss_G: 0.6034\n",
      "BATCH NUMBER 3830\n",
      "Epoch [3/5] Loss_D: 1.0421 Loss_G: 0.6612\n",
      "BATCH NUMBER 3831\n",
      "Epoch [3/5] Loss_D: 1.0345 Loss_G: 0.6678\n",
      "BATCH NUMBER 3832\n",
      "Epoch [3/5] Loss_D: 1.2006 Loss_G: 0.5643\n",
      "BATCH NUMBER 3833\n",
      "Epoch [3/5] Loss_D: 1.1241 Loss_G: 0.6105\n",
      "BATCH NUMBER 3834\n",
      "Epoch [3/5] Loss_D: 1.1520 Loss_G: 0.5883\n",
      "BATCH NUMBER 3835\n",
      "Epoch [3/5] Loss_D: 1.0629 Loss_G: 0.6448\n",
      "BATCH NUMBER 3836\n",
      "Epoch [3/5] Loss_D: 1.0637 Loss_G: 0.6526\n",
      "BATCH NUMBER 3837\n",
      "Epoch [3/5] Loss_D: 1.0284 Loss_G: 0.6743\n",
      "BATCH NUMBER 3838\n",
      "Epoch [3/5] Loss_D: 1.1673 Loss_G: 0.5837\n",
      "BATCH NUMBER 3839\n",
      "Epoch [3/5] Loss_D: 1.1016 Loss_G: 0.6200\n",
      "BATCH NUMBER 3840\n",
      "Epoch [3/5] Loss_D: 1.0773 Loss_G: 0.6410\n",
      "BATCH NUMBER 3841\n",
      "Epoch [3/5] Loss_D: 1.0332 Loss_G: 0.6687\n",
      "BATCH NUMBER 3842\n",
      "Epoch [3/5] Loss_D: 1.2491 Loss_G: 0.5332\n",
      "BATCH NUMBER 3843\n",
      "Epoch [3/5] Loss_D: 1.1167 Loss_G: 0.6165\n",
      "BATCH NUMBER 3844\n",
      "Epoch [3/5] Loss_D: 1.3736 Loss_G: 0.4635\n",
      "BATCH NUMBER 3845\n",
      "Epoch [3/5] Loss_D: 1.1739 Loss_G: 0.5768\n",
      "BATCH NUMBER 3846\n",
      "Epoch [3/5] Loss_D: 1.0591 Loss_G: 0.6480\n",
      "BATCH NUMBER 3847\n",
      "Epoch [3/5] Loss_D: 1.0264 Loss_G: 0.6745\n",
      "BATCH NUMBER 3848\n",
      "Epoch [3/5] Loss_D: 1.0262 Loss_G: 0.6758\n",
      "BATCH NUMBER 3849\n",
      "Epoch [3/5] Loss_D: 1.0460 Loss_G: 0.6573\n",
      "BATCH NUMBER 3850\n",
      "Epoch [3/5] Loss_D: 1.1551 Loss_G: 0.5937\n",
      "BATCH NUMBER 3851\n",
      "Epoch [3/5] Loss_D: 1.0872 Loss_G: 0.6221\n",
      "BATCH NUMBER 3852\n",
      "Epoch [3/5] Loss_D: 1.1125 Loss_G: 0.6191\n",
      "BATCH NUMBER 3853\n",
      "Epoch [3/5] Loss_D: 1.1100 Loss_G: 0.6159\n",
      "BATCH NUMBER 3854\n",
      "Epoch [3/5] Loss_D: 1.0660 Loss_G: 0.6418\n",
      "BATCH NUMBER 3855\n",
      "Epoch [3/5] Loss_D: 1.0945 Loss_G: 0.6177\n",
      "BATCH NUMBER 3856\n",
      "Epoch [3/5] Loss_D: 1.1325 Loss_G: 0.6018\n",
      "BATCH NUMBER 3857\n",
      "Epoch [3/5] Loss_D: 1.0492 Loss_G: 0.6554\n",
      "BATCH NUMBER 3858\n",
      "Epoch [3/5] Loss_D: 1.1712 Loss_G: 0.5804\n",
      "BATCH NUMBER 3859\n",
      "Epoch [3/5] Loss_D: 1.1586 Loss_G: 0.5957\n",
      "BATCH NUMBER 3860\n",
      "Epoch [3/5] Loss_D: 1.1172 Loss_G: 0.6069\n",
      "BATCH NUMBER 3861\n",
      "Epoch [3/5] Loss_D: 1.1838 Loss_G: 0.5778\n",
      "BATCH NUMBER 3862\n",
      "Epoch [3/5] Loss_D: 1.2760 Loss_G: 0.5179\n",
      "BATCH NUMBER 3863\n",
      "Epoch [3/5] Loss_D: 1.0303 Loss_G: 0.6715\n",
      "BATCH NUMBER 3864\n",
      "Epoch [3/5] Loss_D: 1.1392 Loss_G: 0.5968\n",
      "BATCH NUMBER 3865\n",
      "Epoch [3/5] Loss_D: 1.1105 Loss_G: 0.6131\n",
      "BATCH NUMBER 3866\n",
      "Epoch [3/5] Loss_D: 1.0410 Loss_G: 0.6621\n",
      "BATCH NUMBER 3867\n",
      "Epoch [3/5] Loss_D: 1.0467 Loss_G: 0.6572\n",
      "BATCH NUMBER 3868\n",
      "Epoch [3/5] Loss_D: 1.3565 Loss_G: 0.4684\n",
      "BATCH NUMBER 3869\n",
      "Epoch [3/5] Loss_D: 1.1684 Loss_G: 0.5828\n",
      "BATCH NUMBER 3870\n",
      "Epoch [3/5] Loss_D: 1.0580 Loss_G: 0.6490\n",
      "BATCH NUMBER 3871\n",
      "Epoch [3/5] Loss_D: 1.0408 Loss_G: 0.6620\n",
      "BATCH NUMBER 3872\n",
      "Epoch [3/5] Loss_D: 1.2099 Loss_G: 0.5558\n",
      "BATCH NUMBER 3873\n",
      "Epoch [3/5] Loss_D: 1.0991 Loss_G: 0.6312\n",
      "BATCH NUMBER 3874\n",
      "Epoch [3/5] Loss_D: 1.1158 Loss_G: 0.6087\n",
      "BATCH NUMBER 3875\n",
      "Epoch [3/5] Loss_D: 1.1623 Loss_G: 0.5796\n",
      "BATCH NUMBER 3876\n",
      "Epoch [3/5] Loss_D: 1.1197 Loss_G: 0.6131\n",
      "BATCH NUMBER 3877\n",
      "Epoch [3/5] Loss_D: 1.1287 Loss_G: 0.6086\n",
      "BATCH NUMBER 3878\n",
      "Epoch [3/5] Loss_D: 1.2061 Loss_G: 0.5592\n",
      "BATCH NUMBER 3879\n",
      "Epoch [3/5] Loss_D: 1.1679 Loss_G: 0.5823\n",
      "BATCH NUMBER 3880\n",
      "Epoch [3/5] Loss_D: 1.0651 Loss_G: 0.6510\n",
      "BATCH NUMBER 3881\n",
      "Epoch [3/5] Loss_D: 1.1389 Loss_G: 0.5975\n",
      "BATCH NUMBER 3882\n",
      "Epoch [3/5] Loss_D: 1.1050 Loss_G: 0.6198\n",
      "BATCH NUMBER 3883\n",
      "Epoch [3/5] Loss_D: 1.1105 Loss_G: 0.6214\n",
      "BATCH NUMBER 3884\n",
      "Epoch [3/5] Loss_D: 1.0836 Loss_G: 0.6365\n",
      "BATCH NUMBER 3885\n",
      "Epoch [3/5] Loss_D: 1.0506 Loss_G: 0.6548\n",
      "BATCH NUMBER 3886\n",
      "Epoch [3/5] Loss_D: 1.0480 Loss_G: 0.6566\n",
      "BATCH NUMBER 3887\n",
      "Epoch [3/5] Loss_D: 1.1987 Loss_G: 0.5660\n",
      "BATCH NUMBER 3888\n",
      "Epoch [3/5] Loss_D: 1.0677 Loss_G: 0.6490\n",
      "BATCH NUMBER 3889\n",
      "Epoch [3/5] Loss_D: 1.2421 Loss_G: 0.5340\n",
      "BATCH NUMBER 3890\n",
      "Epoch [3/5] Loss_D: 1.0681 Loss_G: 0.6393\n",
      "BATCH NUMBER 3891\n",
      "Epoch [3/5] Loss_D: 1.0909 Loss_G: 0.6216\n",
      "BATCH NUMBER 3892\n",
      "Epoch [3/5] Loss_D: 1.0193 Loss_G: 0.6809\n",
      "BATCH NUMBER 3893\n",
      "Epoch [3/5] Loss_D: 1.0773 Loss_G: 0.6323\n",
      "BATCH NUMBER 3894\n",
      "Epoch [3/5] Loss_D: 1.0308 Loss_G: 0.6710\n",
      "BATCH NUMBER 3895\n",
      "Epoch [3/5] Loss_D: 1.2126 Loss_G: 0.5724\n",
      "BATCH NUMBER 3896\n",
      "Epoch [3/5] Loss_D: 1.2008 Loss_G: 0.5641\n",
      "BATCH NUMBER 3897\n",
      "Epoch [3/5] Loss_D: 1.0390 Loss_G: 0.6639\n",
      "BATCH NUMBER 3898\n",
      "Epoch [3/5] Loss_D: 1.0941 Loss_G: 0.6262\n",
      "BATCH NUMBER 3899\n",
      "Epoch [3/5] Loss_D: 1.1130 Loss_G: 0.6201\n",
      "BATCH NUMBER 3900\n",
      "Epoch [3/5] Loss_D: 1.2189 Loss_G: 0.5486\n",
      "BATCH NUMBER 3901\n",
      "Epoch [3/5] Loss_D: 1.1633 Loss_G: 0.5859\n",
      "BATCH NUMBER 3902\n",
      "Epoch [3/5] Loss_D: 1.0697 Loss_G: 0.6437\n",
      "BATCH NUMBER 3903\n",
      "Epoch [3/5] Loss_D: 1.0416 Loss_G: 0.6623\n",
      "BATCH NUMBER 3904\n",
      "Epoch [3/5] Loss_D: 1.1903 Loss_G: 0.5632\n",
      "BATCH NUMBER 3905\n",
      "Epoch [3/5] Loss_D: 1.0550 Loss_G: 0.6500\n",
      "BATCH NUMBER 3906\n",
      "Epoch [3/5] Loss_D: 1.0561 Loss_G: 0.6518\n",
      "BATCH NUMBER 3907\n",
      "Epoch [3/5] Loss_D: 1.0607 Loss_G: 0.6469\n",
      "BATCH NUMBER 3908\n",
      "Epoch [3/5] Loss_D: 1.0339 Loss_G: 0.6681\n",
      "BATCH NUMBER 3909\n",
      "Epoch [3/5] Loss_D: 1.0169 Loss_G: 0.6832\n",
      "BATCH NUMBER 3910\n",
      "Epoch [3/5] Loss_D: 1.0591 Loss_G: 0.6475\n",
      "BATCH NUMBER 3911\n",
      "Epoch [3/5] Loss_D: 1.1239 Loss_G: 0.6119\n",
      "BATCH NUMBER 3912\n",
      "Epoch [3/5] Loss_D: 1.0573 Loss_G: 0.6505\n",
      "BATCH NUMBER 3913\n",
      "Epoch [3/5] Loss_D: 1.0232 Loss_G: 0.6781\n",
      "BATCH NUMBER 3914\n",
      "Epoch [3/5] Loss_D: 1.1869 Loss_G: 0.5751\n",
      "BATCH NUMBER 3915\n",
      "Epoch [3/5] Loss_D: 1.2214 Loss_G: 0.5558\n",
      "BATCH NUMBER 3916\n",
      "Epoch [3/5] Loss_D: 1.0527 Loss_G: 0.6537\n",
      "BATCH NUMBER 3917\n",
      "Epoch [3/5] Loss_D: 1.0525 Loss_G: 0.6563\n",
      "BATCH NUMBER 3918\n",
      "Epoch [3/5] Loss_D: 1.1683 Loss_G: 0.5817\n",
      "BATCH NUMBER 3919\n",
      "Epoch [3/5] Loss_D: 1.0802 Loss_G: 0.6306\n",
      "BATCH NUMBER 3920\n",
      "Epoch [3/5] Loss_D: 1.1025 Loss_G: 0.6211\n",
      "BATCH NUMBER 3921\n",
      "Epoch [3/5] Loss_D: 1.0717 Loss_G: 0.6349\n",
      "BATCH NUMBER 3922\n",
      "Epoch [3/5] Loss_D: 1.0844 Loss_G: 0.6350\n",
      "BATCH NUMBER 3923\n",
      "Epoch [3/5] Loss_D: 1.0503 Loss_G: 0.6544\n",
      "BATCH NUMBER 3924\n",
      "Epoch [3/5] Loss_D: 1.1467 Loss_G: 0.6007\n",
      "BATCH NUMBER 3925\n",
      "Epoch [3/5] Loss_D: 1.0420 Loss_G: 0.6608\n",
      "BATCH NUMBER 3926\n",
      "Epoch [3/5] Loss_D: 1.1179 Loss_G: 0.6078\n",
      "BATCH NUMBER 3927\n",
      "Epoch [3/5] Loss_D: 1.0722 Loss_G: 0.6357\n",
      "BATCH NUMBER 3928\n",
      "Epoch [3/5] Loss_D: 1.1560 Loss_G: 0.5931\n",
      "BATCH NUMBER 3929\n",
      "Epoch [3/5] Loss_D: 1.0823 Loss_G: 0.6369\n",
      "BATCH NUMBER 3930\n",
      "Epoch [3/5] Loss_D: 1.1447 Loss_G: 0.5925\n",
      "BATCH NUMBER 3931\n",
      "Epoch [3/5] Loss_D: 1.0177 Loss_G: 0.6828\n",
      "BATCH NUMBER 3932\n",
      "Epoch [3/5] Loss_D: 1.0652 Loss_G: 0.6429\n",
      "BATCH NUMBER 3933\n",
      "Epoch [3/5] Loss_D: 1.2142 Loss_G: 0.5612\n",
      "BATCH NUMBER 3934\n",
      "Epoch [3/5] Loss_D: 1.0326 Loss_G: 0.6694\n",
      "BATCH NUMBER 3935\n",
      "Epoch [3/5] Loss_D: 1.0804 Loss_G: 0.6378\n",
      "BATCH NUMBER 3936\n",
      "Epoch [3/5] Loss_D: 1.0668 Loss_G: 0.6503\n",
      "BATCH NUMBER 3937\n",
      "Epoch [3/5] Loss_D: 1.0507 Loss_G: 0.6543\n",
      "BATCH NUMBER 3938\n",
      "Epoch [3/5] Loss_D: 1.1525 Loss_G: 0.5956\n",
      "BATCH NUMBER 3939\n",
      "Epoch [3/5] Loss_D: 1.0256 Loss_G: 0.6750\n",
      "BATCH NUMBER 3940\n",
      "Epoch [3/5] Loss_D: 1.0973 Loss_G: 0.6246\n",
      "BATCH NUMBER 3941\n",
      "Epoch [3/5] Loss_D: 1.2102 Loss_G: 0.5551\n",
      "BATCH NUMBER 3942\n",
      "Epoch [3/5] Loss_D: 1.0457 Loss_G: 0.6595\n",
      "BATCH NUMBER 3943\n",
      "Epoch [3/5] Loss_D: 1.1965 Loss_G: 0.5676\n",
      "BATCH NUMBER 3944\n",
      "Epoch [3/5] Loss_D: 1.0758 Loss_G: 0.6360\n",
      "BATCH NUMBER 3945\n",
      "Epoch [3/5] Loss_D: 1.2440 Loss_G: 0.5360\n",
      "BATCH NUMBER 3946\n",
      "Epoch [3/5] Loss_D: 1.1410 Loss_G: 0.5960\n",
      "BATCH NUMBER 3947\n",
      "Epoch [3/5] Loss_D: 1.0630 Loss_G: 0.6436\n",
      "BATCH NUMBER 3948\n",
      "Epoch [3/5] Loss_D: 1.0605 Loss_G: 0.6461\n",
      "BATCH NUMBER 3949\n",
      "Epoch [3/5] Loss_D: 1.0273 Loss_G: 0.6750\n",
      "BATCH NUMBER 3950\n",
      "Epoch [3/5] Loss_D: 1.1159 Loss_G: 0.6180\n",
      "BATCH NUMBER 3951\n",
      "Epoch [3/5] Loss_D: 1.0258 Loss_G: 0.6757\n",
      "BATCH NUMBER 3952\n",
      "Epoch [3/5] Loss_D: 1.1780 Loss_G: 0.5737\n",
      "BATCH NUMBER 3953\n",
      "Epoch [3/5] Loss_D: 1.0617 Loss_G: 0.6452\n",
      "BATCH NUMBER 3954\n",
      "Epoch [3/5] Loss_D: 1.1977 Loss_G: 0.5676\n",
      "BATCH NUMBER 3955\n",
      "Epoch [3/5] Loss_D: 1.2203 Loss_G: 0.5469\n",
      "BATCH NUMBER 3956\n",
      "Epoch [3/5] Loss_D: 1.0870 Loss_G: 0.6324\n",
      "BATCH NUMBER 3957\n",
      "Epoch [3/5] Loss_D: 1.0295 Loss_G: 0.6724\n",
      "BATCH NUMBER 3958\n",
      "Epoch [3/5] Loss_D: 1.1458 Loss_G: 0.6013\n",
      "BATCH NUMBER 3959\n",
      "Epoch [3/5] Loss_D: 1.1367 Loss_G: 0.6007\n",
      "BATCH NUMBER 3960\n",
      "Epoch [3/5] Loss_D: 1.1775 Loss_G: 0.5736\n",
      "BATCH NUMBER 3961\n",
      "Epoch [3/5] Loss_D: 1.1599 Loss_G: 0.5886\n",
      "BATCH NUMBER 3962\n",
      "Epoch [3/5] Loss_D: 1.1964 Loss_G: 0.5733\n",
      "BATCH NUMBER 3963\n",
      "Epoch [3/5] Loss_D: 1.0841 Loss_G: 0.6356\n",
      "BATCH NUMBER 3964\n",
      "Epoch [3/5] Loss_D: 1.1674 Loss_G: 0.5832\n",
      "BATCH NUMBER 3965\n",
      "Epoch [3/5] Loss_D: 1.0715 Loss_G: 0.6455\n",
      "BATCH NUMBER 3966\n",
      "Epoch [3/5] Loss_D: 1.0733 Loss_G: 0.6446\n",
      "BATCH NUMBER 3967\n",
      "Epoch [3/5] Loss_D: 1.0871 Loss_G: 0.6327\n",
      "BATCH NUMBER 3968\n",
      "Epoch [3/5] Loss_D: 1.1447 Loss_G: 0.5930\n",
      "BATCH NUMBER 3969\n",
      "Epoch [3/5] Loss_D: 1.0193 Loss_G: 0.6815\n",
      "BATCH NUMBER 3970\n",
      "Epoch [3/5] Loss_D: 1.1290 Loss_G: 0.6070\n",
      "BATCH NUMBER 3971\n",
      "Epoch [3/5] Loss_D: 1.0626 Loss_G: 0.6449\n",
      "BATCH NUMBER 3972\n",
      "Epoch [3/5] Loss_D: 1.1847 Loss_G: 0.5599\n",
      "BATCH NUMBER 3973\n",
      "Epoch [3/5] Loss_D: 1.1165 Loss_G: 0.6066\n",
      "BATCH NUMBER 3974\n",
      "Epoch [3/5] Loss_D: 1.0536 Loss_G: 0.6515\n",
      "BATCH NUMBER 3975\n",
      "Epoch [3/5] Loss_D: 1.1342 Loss_G: 0.6013\n",
      "BATCH NUMBER 3976\n",
      "Epoch [3/5] Loss_D: 1.0333 Loss_G: 0.6695\n",
      "BATCH NUMBER 3977\n",
      "Epoch [3/5] Loss_D: 1.0976 Loss_G: 0.6235\n",
      "BATCH NUMBER 3978\n",
      "Epoch [3/5] Loss_D: 1.0425 Loss_G: 0.6616\n",
      "BATCH NUMBER 3979\n",
      "Epoch [3/5] Loss_D: 1.0698 Loss_G: 0.6488\n",
      "BATCH NUMBER 3980\n",
      "Epoch [3/5] Loss_D: 1.0309 Loss_G: 0.6709\n",
      "BATCH NUMBER 3981\n",
      "Epoch [3/5] Loss_D: 1.1082 Loss_G: 0.6142\n",
      "BATCH NUMBER 3982\n",
      "Epoch [3/5] Loss_D: 1.1000 Loss_G: 0.6161\n",
      "BATCH NUMBER 3983\n",
      "Epoch [3/5] Loss_D: 1.1530 Loss_G: 0.5810\n",
      "BATCH NUMBER 3984\n",
      "Epoch [3/5] Loss_D: 1.2746 Loss_G: 0.5196\n",
      "BATCH NUMBER 3985\n",
      "Epoch [3/5] Loss_D: 1.1175 Loss_G: 0.6157\n",
      "BATCH NUMBER 3986\n",
      "Epoch [3/5] Loss_D: 1.0636 Loss_G: 0.6446\n",
      "BATCH NUMBER 3987\n",
      "Epoch [3/5] Loss_D: 1.2893 Loss_G: 0.5338\n",
      "BATCH NUMBER 3988\n",
      "Epoch [3/5] Loss_D: 1.1859 Loss_G: 0.5664\n",
      "BATCH NUMBER 3989\n",
      "Epoch [3/5] Loss_D: 1.1444 Loss_G: 0.6042\n",
      "BATCH NUMBER 3990\n",
      "Epoch [3/5] Loss_D: 1.0761 Loss_G: 0.6425\n",
      "BATCH NUMBER 3991\n",
      "Epoch [3/5] Loss_D: 1.1609 Loss_G: 0.5870\n",
      "BATCH NUMBER 3992\n",
      "Epoch [3/5] Loss_D: 1.0481 Loss_G: 0.6558\n",
      "BATCH NUMBER 3993\n",
      "Epoch [3/5] Loss_D: 1.0553 Loss_G: 0.6506\n",
      "BATCH NUMBER 3994\n",
      "Epoch [3/5] Loss_D: 1.0443 Loss_G: 0.6596\n",
      "BATCH NUMBER 3995\n",
      "Epoch [3/5] Loss_D: 1.1151 Loss_G: 0.6185\n",
      "BATCH NUMBER 3996\n",
      "Epoch [3/5] Loss_D: 1.0492 Loss_G: 0.6539\n",
      "BATCH NUMBER 3997\n",
      "Epoch [3/5] Loss_D: 1.0933 Loss_G: 0.6189\n",
      "BATCH NUMBER 3998\n",
      "Epoch [3/5] Loss_D: 1.0768 Loss_G: 0.6308\n",
      "BATCH NUMBER 3999\n",
      "Epoch [3/5] Loss_D: 1.1705 Loss_G: 0.5797\n",
      "BATCH NUMBER 4000\n",
      "Epoch [4/5] Loss_D: 1.2198 Loss_G: 0.5476\n",
      "BATCH NUMBER 4001\n",
      "Epoch [4/5] Loss_D: 1.0714 Loss_G: 0.6458\n",
      "BATCH NUMBER 4002\n",
      "Epoch [4/5] Loss_D: 1.0743 Loss_G: 0.6333\n",
      "BATCH NUMBER 4003\n",
      "Epoch [4/5] Loss_D: 1.0455 Loss_G: 0.6583\n",
      "BATCH NUMBER 4004\n",
      "Epoch [4/5] Loss_D: 1.2765 Loss_G: 0.5175\n",
      "BATCH NUMBER 4005\n",
      "Epoch [4/5] Loss_D: 1.0597 Loss_G: 0.6483\n",
      "BATCH NUMBER 4006\n",
      "Epoch [4/5] Loss_D: 1.1733 Loss_G: 0.5786\n",
      "BATCH NUMBER 4007\n",
      "Epoch [4/5] Loss_D: 1.1094 Loss_G: 0.6235\n",
      "BATCH NUMBER 4008\n",
      "Epoch [4/5] Loss_D: 1.0284 Loss_G: 0.6746\n",
      "BATCH NUMBER 4009\n",
      "Epoch [4/5] Loss_D: 1.1166 Loss_G: 0.5988\n",
      "BATCH NUMBER 4010\n",
      "Epoch [4/5] Loss_D: 1.1449 Loss_G: 0.5934\n",
      "BATCH NUMBER 4011\n",
      "Epoch [4/5] Loss_D: 1.0305 Loss_G: 0.6718\n",
      "BATCH NUMBER 4012\n",
      "Epoch [4/5] Loss_D: 1.1505 Loss_G: 0.5904\n",
      "BATCH NUMBER 4013\n",
      "Epoch [4/5] Loss_D: 1.1716 Loss_G: 0.5824\n",
      "BATCH NUMBER 4014\n",
      "Epoch [4/5] Loss_D: 1.1696 Loss_G: 0.5803\n",
      "BATCH NUMBER 4015\n",
      "Epoch [4/5] Loss_D: 1.1763 Loss_G: 0.5743\n",
      "BATCH NUMBER 4016\n",
      "Epoch [4/5] Loss_D: 1.0801 Loss_G: 0.6382\n",
      "BATCH NUMBER 4017\n",
      "Epoch [4/5] Loss_D: 1.1382 Loss_G: 0.5982\n",
      "BATCH NUMBER 4018\n",
      "Epoch [4/5] Loss_D: 1.0399 Loss_G: 0.6634\n",
      "BATCH NUMBER 4019\n",
      "Epoch [4/5] Loss_D: 1.0430 Loss_G: 0.6616\n",
      "BATCH NUMBER 4020\n",
      "Epoch [4/5] Loss_D: 1.1051 Loss_G: 0.6259\n",
      "BATCH NUMBER 4021\n",
      "Epoch [4/5] Loss_D: 1.1857 Loss_G: 0.5767\n",
      "BATCH NUMBER 4022\n",
      "Epoch [4/5] Loss_D: 1.2452 Loss_G: 0.5356\n",
      "BATCH NUMBER 4023\n",
      "Epoch [4/5] Loss_D: 1.0960 Loss_G: 0.6245\n",
      "BATCH NUMBER 4024\n",
      "Epoch [4/5] Loss_D: 1.0538 Loss_G: 0.6518\n",
      "BATCH NUMBER 4025\n",
      "Epoch [4/5] Loss_D: 1.0332 Loss_G: 0.6689\n",
      "BATCH NUMBER 4026\n",
      "Epoch [4/5] Loss_D: 1.0223 Loss_G: 0.6783\n",
      "BATCH NUMBER 4027\n",
      "Epoch [4/5] Loss_D: 1.1035 Loss_G: 0.6192\n",
      "BATCH NUMBER 4028\n",
      "Epoch [4/5] Loss_D: 1.0366 Loss_G: 0.6672\n",
      "BATCH NUMBER 4029\n",
      "Epoch [4/5] Loss_D: 1.0308 Loss_G: 0.6724\n",
      "BATCH NUMBER 4030\n",
      "Epoch [4/5] Loss_D: 1.2046 Loss_G: 0.5616\n",
      "BATCH NUMBER 4031\n",
      "Epoch [4/5] Loss_D: 1.1320 Loss_G: 0.6046\n",
      "BATCH NUMBER 4032\n",
      "Epoch [4/5] Loss_D: 1.0440 Loss_G: 0.6608\n",
      "BATCH NUMBER 4033\n",
      "Epoch [4/5] Loss_D: 1.0596 Loss_G: 0.6464\n",
      "BATCH NUMBER 4034\n",
      "Epoch [4/5] Loss_D: 1.1545 Loss_G: 0.5929\n",
      "BATCH NUMBER 4035\n",
      "Epoch [4/5] Loss_D: 1.1396 Loss_G: 0.5969\n",
      "BATCH NUMBER 4036\n",
      "Epoch [4/5] Loss_D: 1.0316 Loss_G: 0.6714\n",
      "BATCH NUMBER 4037\n",
      "Epoch [4/5] Loss_D: 1.0757 Loss_G: 0.6431\n",
      "BATCH NUMBER 4038\n",
      "Epoch [4/5] Loss_D: 1.0778 Loss_G: 0.6410\n",
      "BATCH NUMBER 4039\n",
      "Epoch [4/5] Loss_D: 1.0428 Loss_G: 0.6600\n",
      "BATCH NUMBER 4040\n",
      "Epoch [4/5] Loss_D: 1.0321 Loss_G: 0.6714\n",
      "BATCH NUMBER 4041\n",
      "Epoch [4/5] Loss_D: 1.1996 Loss_G: 0.5574\n",
      "BATCH NUMBER 4042\n",
      "Epoch [4/5] Loss_D: 1.0842 Loss_G: 0.6362\n",
      "BATCH NUMBER 4043\n",
      "Epoch [4/5] Loss_D: 1.3148 Loss_G: 0.4947\n",
      "BATCH NUMBER 4044\n",
      "Epoch [4/5] Loss_D: 1.1041 Loss_G: 0.6185\n",
      "BATCH NUMBER 4045\n",
      "Epoch [4/5] Loss_D: 1.0842 Loss_G: 0.6279\n",
      "BATCH NUMBER 4046\n",
      "Epoch [4/5] Loss_D: 1.0546 Loss_G: 0.6531\n",
      "BATCH NUMBER 4047\n",
      "Epoch [4/5] Loss_D: 1.1175 Loss_G: 0.6159\n",
      "BATCH NUMBER 4048\n",
      "Epoch [4/5] Loss_D: 1.0146 Loss_G: 0.6854\n",
      "BATCH NUMBER 4049\n",
      "Epoch [4/5] Loss_D: 1.0833 Loss_G: 0.6354\n",
      "BATCH NUMBER 4050\n",
      "Epoch [4/5] Loss_D: 1.2864 Loss_G: 0.5099\n",
      "BATCH NUMBER 4051\n",
      "Epoch [4/5] Loss_D: 1.2153 Loss_G: 0.5505\n",
      "BATCH NUMBER 4052\n",
      "Epoch [4/5] Loss_D: 1.1031 Loss_G: 0.6184\n",
      "BATCH NUMBER 4053\n",
      "Epoch [4/5] Loss_D: 1.0451 Loss_G: 0.6583\n",
      "BATCH NUMBER 4054\n",
      "Epoch [4/5] Loss_D: 1.0262 Loss_G: 0.6760\n",
      "BATCH NUMBER 4055\n",
      "Epoch [4/5] Loss_D: 1.1301 Loss_G: 0.5974\n",
      "BATCH NUMBER 4056\n",
      "Epoch [4/5] Loss_D: 1.1755 Loss_G: 0.5771\n",
      "BATCH NUMBER 4057\n",
      "Epoch [4/5] Loss_D: 1.0396 Loss_G: 0.6636\n",
      "BATCH NUMBER 4058\n",
      "Epoch [4/5] Loss_D: 1.2187 Loss_G: 0.5466\n",
      "BATCH NUMBER 4059\n",
      "Epoch [4/5] Loss_D: 1.0751 Loss_G: 0.6431\n",
      "BATCH NUMBER 4060\n",
      "Epoch [4/5] Loss_D: 1.1929 Loss_G: 0.5612\n",
      "BATCH NUMBER 4061\n",
      "Epoch [4/5] Loss_D: 1.1419 Loss_G: 0.5887\n",
      "BATCH NUMBER 4062\n",
      "Epoch [4/5] Loss_D: 1.0490 Loss_G: 0.6551\n",
      "BATCH NUMBER 4063\n",
      "Epoch [4/5] Loss_D: 1.0353 Loss_G: 0.6673\n",
      "BATCH NUMBER 4064\n",
      "Epoch [4/5] Loss_D: 1.1981 Loss_G: 0.5649\n",
      "BATCH NUMBER 4065\n",
      "Epoch [4/5] Loss_D: 1.1141 Loss_G: 0.6101\n",
      "BATCH NUMBER 4066\n",
      "Epoch [4/5] Loss_D: 1.0542 Loss_G: 0.6516\n",
      "BATCH NUMBER 4067\n",
      "Epoch [4/5] Loss_D: 1.2624 Loss_G: 0.5315\n",
      "BATCH NUMBER 4068\n",
      "Epoch [4/5] Loss_D: 1.1185 Loss_G: 0.6154\n",
      "BATCH NUMBER 4069\n",
      "Epoch [4/5] Loss_D: 1.2492 Loss_G: 0.5310\n",
      "BATCH NUMBER 4070\n",
      "Epoch [4/5] Loss_D: 1.0422 Loss_G: 0.6619\n",
      "BATCH NUMBER 4071\n",
      "Epoch [4/5] Loss_D: 1.2497 Loss_G: 0.5318\n",
      "BATCH NUMBER 4072\n",
      "Epoch [4/5] Loss_D: 1.0441 Loss_G: 0.6598\n",
      "BATCH NUMBER 4073\n",
      "Epoch [4/5] Loss_D: 1.0882 Loss_G: 0.6319\n",
      "BATCH NUMBER 4074\n",
      "Epoch [4/5] Loss_D: 1.0385 Loss_G: 0.6647\n",
      "BATCH NUMBER 4075\n",
      "Epoch [4/5] Loss_D: 1.0923 Loss_G: 0.6280\n",
      "BATCH NUMBER 4076\n",
      "Epoch [4/5] Loss_D: 1.1598 Loss_G: 0.5898\n",
      "BATCH NUMBER 4077\n",
      "Epoch [4/5] Loss_D: 1.0976 Loss_G: 0.6240\n",
      "BATCH NUMBER 4078\n",
      "Epoch [4/5] Loss_D: 1.1224 Loss_G: 0.6039\n",
      "BATCH NUMBER 4079\n",
      "Epoch [4/5] Loss_D: 1.0818 Loss_G: 0.6377\n",
      "BATCH NUMBER 4080\n",
      "Epoch [4/5] Loss_D: 1.0814 Loss_G: 0.6372\n",
      "BATCH NUMBER 4081\n",
      "Epoch [4/5] Loss_D: 1.0589 Loss_G: 0.6569\n",
      "BATCH NUMBER 4082\n",
      "Epoch [4/5] Loss_D: 1.3275 Loss_G: 0.4833\n",
      "BATCH NUMBER 4083\n",
      "Epoch [4/5] Loss_D: 1.0788 Loss_G: 0.6397\n",
      "BATCH NUMBER 4084\n",
      "Epoch [4/5] Loss_D: 1.0284 Loss_G: 0.6734\n",
      "BATCH NUMBER 4085\n",
      "Epoch [4/5] Loss_D: 1.0360 Loss_G: 0.6658\n",
      "BATCH NUMBER 4086\n",
      "Epoch [4/5] Loss_D: 1.0545 Loss_G: 0.6541\n",
      "BATCH NUMBER 4087\n",
      "Epoch [4/5] Loss_D: 1.1655 Loss_G: 0.5834\n",
      "BATCH NUMBER 4088\n",
      "Epoch [4/5] Loss_D: 1.0483 Loss_G: 0.6576\n",
      "BATCH NUMBER 4089\n",
      "Epoch [4/5] Loss_D: 1.0836 Loss_G: 0.6273\n",
      "BATCH NUMBER 4090\n",
      "Epoch [4/5] Loss_D: 1.2698 Loss_G: 0.5231\n",
      "BATCH NUMBER 4091\n",
      "Epoch [4/5] Loss_D: 1.2034 Loss_G: 0.5603\n",
      "BATCH NUMBER 4092\n",
      "Epoch [4/5] Loss_D: 1.0258 Loss_G: 0.6760\n",
      "BATCH NUMBER 4093\n",
      "Epoch [4/5] Loss_D: 1.1113 Loss_G: 0.6133\n",
      "BATCH NUMBER 4094\n",
      "Epoch [4/5] Loss_D: 1.2769 Loss_G: 0.5174\n",
      "BATCH NUMBER 4095\n",
      "Epoch [4/5] Loss_D: 1.0921 Loss_G: 0.6309\n",
      "BATCH NUMBER 4096\n",
      "Epoch [4/5] Loss_D: 1.0380 Loss_G: 0.6649\n",
      "BATCH NUMBER 4097\n",
      "Epoch [4/5] Loss_D: 1.0644 Loss_G: 0.6425\n",
      "BATCH NUMBER 4098\n",
      "Epoch [4/5] Loss_D: 1.0764 Loss_G: 0.6318\n",
      "BATCH NUMBER 4099\n",
      "Epoch [4/5] Loss_D: 1.0385 Loss_G: 0.6638\n",
      "BATCH NUMBER 4100\n",
      "Epoch [4/5] Loss_D: 1.1883 Loss_G: 0.5737\n",
      "BATCH NUMBER 4101\n",
      "Epoch [4/5] Loss_D: 1.0740 Loss_G: 0.6434\n",
      "BATCH NUMBER 4102\n",
      "Epoch [4/5] Loss_D: 1.0754 Loss_G: 0.6417\n",
      "BATCH NUMBER 4103\n",
      "Epoch [4/5] Loss_D: 1.0543 Loss_G: 0.6519\n",
      "BATCH NUMBER 4104\n",
      "Epoch [4/5] Loss_D: 1.1285 Loss_G: 0.6064\n",
      "BATCH NUMBER 4105\n",
      "Epoch [4/5] Loss_D: 1.1677 Loss_G: 0.5740\n",
      "BATCH NUMBER 4106\n",
      "Epoch [4/5] Loss_D: 1.0610 Loss_G: 0.6550\n",
      "BATCH NUMBER 4107\n",
      "Epoch [4/5] Loss_D: 1.1297 Loss_G: 0.6059\n",
      "BATCH NUMBER 4108\n",
      "Epoch [4/5] Loss_D: 1.0374 Loss_G: 0.6645\n",
      "BATCH NUMBER 4109\n",
      "Epoch [4/5] Loss_D: 1.0507 Loss_G: 0.6533\n",
      "BATCH NUMBER 4110\n",
      "Epoch [4/5] Loss_D: 1.1186 Loss_G: 0.6063\n",
      "BATCH NUMBER 4111\n",
      "Epoch [4/5] Loss_D: 1.1187 Loss_G: 0.6205\n",
      "BATCH NUMBER 4112\n",
      "Epoch [4/5] Loss_D: 1.1401 Loss_G: 0.5969\n",
      "BATCH NUMBER 4113\n",
      "Epoch [4/5] Loss_D: 1.0790 Loss_G: 0.6392\n",
      "BATCH NUMBER 4114\n",
      "Epoch [4/5] Loss_D: 1.0620 Loss_G: 0.6467\n",
      "BATCH NUMBER 4115\n",
      "Epoch [4/5] Loss_D: 1.1092 Loss_G: 0.6230\n",
      "BATCH NUMBER 4116\n",
      "Epoch [4/5] Loss_D: 1.0412 Loss_G: 0.6620\n",
      "BATCH NUMBER 4117\n",
      "Epoch [4/5] Loss_D: 1.1441 Loss_G: 0.5836\n",
      "BATCH NUMBER 4118\n",
      "Epoch [4/5] Loss_D: 1.0838 Loss_G: 0.6354\n",
      "BATCH NUMBER 4119\n",
      "Epoch [4/5] Loss_D: 1.2402 Loss_G: 0.5409\n",
      "BATCH NUMBER 4120\n",
      "Epoch [4/5] Loss_D: 1.0776 Loss_G: 0.6402\n",
      "BATCH NUMBER 4121\n",
      "Epoch [4/5] Loss_D: 1.3638 Loss_G: 0.4715\n",
      "BATCH NUMBER 4122\n",
      "Epoch [4/5] Loss_D: 1.1044 Loss_G: 0.6139\n",
      "BATCH NUMBER 4123\n",
      "Epoch [4/5] Loss_D: 1.0539 Loss_G: 0.6507\n",
      "BATCH NUMBER 4124\n",
      "Epoch [4/5] Loss_D: 1.1760 Loss_G: 0.5758\n",
      "BATCH NUMBER 4125\n",
      "Epoch [4/5] Loss_D: 1.2006 Loss_G: 0.5646\n",
      "BATCH NUMBER 4126\n",
      "Epoch [4/5] Loss_D: 1.0786 Loss_G: 0.6397\n",
      "BATCH NUMBER 4127\n",
      "Epoch [4/5] Loss_D: 1.0998 Loss_G: 0.6120\n",
      "BATCH NUMBER 4128\n",
      "Epoch [4/5] Loss_D: 1.1097 Loss_G: 0.6127\n",
      "BATCH NUMBER 4129\n",
      "Epoch [4/5] Loss_D: 1.2370 Loss_G: 0.5331\n",
      "BATCH NUMBER 4130\n",
      "Epoch [4/5] Loss_D: 1.0362 Loss_G: 0.6668\n",
      "BATCH NUMBER 4131\n",
      "Epoch [4/5] Loss_D: 1.0472 Loss_G: 0.6570\n",
      "BATCH NUMBER 4132\n",
      "Epoch [4/5] Loss_D: 1.0721 Loss_G: 0.6355\n",
      "BATCH NUMBER 4133\n",
      "Epoch [4/5] Loss_D: 1.0975 Loss_G: 0.6222\n",
      "BATCH NUMBER 4134\n",
      "Epoch [4/5] Loss_D: 1.0399 Loss_G: 0.6638\n",
      "BATCH NUMBER 4135\n",
      "Epoch [4/5] Loss_D: 1.0411 Loss_G: 0.6620\n",
      "BATCH NUMBER 4136\n",
      "Epoch [4/5] Loss_D: 1.1222 Loss_G: 0.6116\n",
      "BATCH NUMBER 4137\n",
      "Epoch [4/5] Loss_D: 1.1304 Loss_G: 0.6051\n",
      "BATCH NUMBER 4138\n",
      "Epoch [4/5] Loss_D: 1.1353 Loss_G: 0.6014\n",
      "BATCH NUMBER 4139\n",
      "Epoch [4/5] Loss_D: 1.1670 Loss_G: 0.5830\n",
      "BATCH NUMBER 4140\n",
      "Epoch [4/5] Loss_D: 1.0620 Loss_G: 0.6454\n",
      "BATCH NUMBER 4141\n",
      "Epoch [4/5] Loss_D: 1.0803 Loss_G: 0.6368\n",
      "BATCH NUMBER 4142\n",
      "Epoch [4/5] Loss_D: 1.1767 Loss_G: 0.5749\n",
      "BATCH NUMBER 4143\n",
      "Epoch [4/5] Loss_D: 1.0516 Loss_G: 0.6527\n",
      "BATCH NUMBER 4144\n",
      "Epoch [4/5] Loss_D: 1.0561 Loss_G: 0.6488\n",
      "BATCH NUMBER 4145\n",
      "Epoch [4/5] Loss_D: 1.2391 Loss_G: 0.5402\n",
      "BATCH NUMBER 4146\n",
      "Epoch [4/5] Loss_D: 1.0852 Loss_G: 0.6354\n",
      "BATCH NUMBER 4147\n",
      "Epoch [4/5] Loss_D: 1.2544 Loss_G: 0.5277\n",
      "BATCH NUMBER 4148\n",
      "Epoch [4/5] Loss_D: 1.1881 Loss_G: 0.5658\n",
      "BATCH NUMBER 4149\n",
      "Epoch [4/5] Loss_D: 1.1105 Loss_G: 0.6214\n",
      "BATCH NUMBER 4150\n",
      "Epoch [4/5] Loss_D: 1.0705 Loss_G: 0.6471\n",
      "BATCH NUMBER 4151\n",
      "Epoch [4/5] Loss_D: 1.0601 Loss_G: 0.6474\n",
      "BATCH NUMBER 4152\n",
      "Epoch [4/5] Loss_D: 1.0485 Loss_G: 0.6555\n",
      "BATCH NUMBER 4153\n",
      "Epoch [4/5] Loss_D: 1.0495 Loss_G: 0.6555\n",
      "BATCH NUMBER 4154\n",
      "Epoch [4/5] Loss_D: 1.0804 Loss_G: 0.6375\n",
      "BATCH NUMBER 4155\n",
      "Epoch [4/5] Loss_D: 1.0537 Loss_G: 0.6533\n",
      "BATCH NUMBER 4156\n",
      "Epoch [4/5] Loss_D: 1.0560 Loss_G: 0.6592\n",
      "BATCH NUMBER 4157\n",
      "Epoch [4/5] Loss_D: 1.1188 Loss_G: 0.6134\n",
      "BATCH NUMBER 4158\n",
      "Epoch [4/5] Loss_D: 1.1782 Loss_G: 0.5737\n",
      "BATCH NUMBER 4159\n",
      "Epoch [4/5] Loss_D: 1.0523 Loss_G: 0.6524\n",
      "BATCH NUMBER 4160\n",
      "Epoch [4/5] Loss_D: 1.0857 Loss_G: 0.6337\n",
      "BATCH NUMBER 4161\n",
      "Epoch [4/5] Loss_D: 1.1849 Loss_G: 0.5766\n",
      "BATCH NUMBER 4162\n",
      "Epoch [4/5] Loss_D: 1.1009 Loss_G: 0.6224\n",
      "BATCH NUMBER 4163\n",
      "Epoch [4/5] Loss_D: 1.0410 Loss_G: 0.6625\n",
      "BATCH NUMBER 4164\n",
      "Epoch [4/5] Loss_D: 1.0579 Loss_G: 0.6482\n",
      "BATCH NUMBER 4165\n",
      "Epoch [4/5] Loss_D: 1.1116 Loss_G: 0.6209\n",
      "BATCH NUMBER 4166\n",
      "Epoch [4/5] Loss_D: 1.0391 Loss_G: 0.6644\n",
      "BATCH NUMBER 4167\n",
      "Epoch [4/5] Loss_D: 1.2352 Loss_G: 0.5432\n",
      "BATCH NUMBER 4168\n",
      "Epoch [4/5] Loss_D: 1.1730 Loss_G: 0.5709\n",
      "BATCH NUMBER 4169\n",
      "Epoch [4/5] Loss_D: 1.0555 Loss_G: 0.6487\n",
      "BATCH NUMBER 4170\n",
      "Epoch [4/5] Loss_D: 1.0272 Loss_G: 0.6748\n",
      "BATCH NUMBER 4171\n",
      "Epoch [4/5] Loss_D: 1.3200 Loss_G: 0.4890\n",
      "BATCH NUMBER 4172\n",
      "Epoch [4/5] Loss_D: 1.0560 Loss_G: 0.6507\n",
      "BATCH NUMBER 4173\n",
      "Epoch [4/5] Loss_D: 1.2521 Loss_G: 0.5317\n",
      "BATCH NUMBER 4174\n",
      "Epoch [4/5] Loss_D: 1.0254 Loss_G: 0.6762\n",
      "BATCH NUMBER 4175\n",
      "Epoch [4/5] Loss_D: 1.2324 Loss_G: 0.5375\n",
      "BATCH NUMBER 4176\n",
      "Epoch [4/5] Loss_D: 1.0884 Loss_G: 0.6316\n",
      "BATCH NUMBER 4177\n",
      "Epoch [4/5] Loss_D: 1.1648 Loss_G: 0.5850\n",
      "BATCH NUMBER 4178\n",
      "Epoch [4/5] Loss_D: 1.0811 Loss_G: 0.6365\n",
      "BATCH NUMBER 4179\n",
      "Epoch [4/5] Loss_D: 1.0528 Loss_G: 0.6526\n",
      "BATCH NUMBER 4180\n",
      "Epoch [4/5] Loss_D: 1.1303 Loss_G: 0.6047\n",
      "BATCH NUMBER 4181\n",
      "Epoch [4/5] Loss_D: 1.0663 Loss_G: 0.6496\n",
      "BATCH NUMBER 4182\n",
      "Epoch [4/5] Loss_D: 1.2454 Loss_G: 0.5350\n",
      "BATCH NUMBER 4183\n",
      "Epoch [4/5] Loss_D: 1.1287 Loss_G: 0.6056\n",
      "BATCH NUMBER 4184\n",
      "Epoch [4/5] Loss_D: 1.0349 Loss_G: 0.6676\n",
      "BATCH NUMBER 4185\n",
      "Epoch [4/5] Loss_D: 1.1116 Loss_G: 0.6122\n",
      "BATCH NUMBER 4186\n",
      "Epoch [4/5] Loss_D: 1.0390 Loss_G: 0.6648\n",
      "BATCH NUMBER 4187\n",
      "Epoch [4/5] Loss_D: 1.0706 Loss_G: 0.6382\n",
      "BATCH NUMBER 4188\n",
      "Epoch [4/5] Loss_D: 1.0855 Loss_G: 0.6341\n",
      "BATCH NUMBER 4189\n",
      "Epoch [4/5] Loss_D: 1.1719 Loss_G: 0.5758\n",
      "BATCH NUMBER 4190\n",
      "Epoch [4/5] Loss_D: 1.0196 Loss_G: 0.6809\n",
      "BATCH NUMBER 4191\n",
      "Epoch [4/5] Loss_D: 1.1851 Loss_G: 0.5669\n",
      "BATCH NUMBER 4192\n",
      "Epoch [4/5] Loss_D: 1.0877 Loss_G: 0.6326\n",
      "BATCH NUMBER 4193\n",
      "Epoch [4/5] Loss_D: 1.1842 Loss_G: 0.5695\n",
      "BATCH NUMBER 4194\n",
      "Epoch [4/5] Loss_D: 1.0488 Loss_G: 0.6553\n",
      "BATCH NUMBER 4195\n",
      "Epoch [4/5] Loss_D: 1.2381 Loss_G: 0.5421\n",
      "BATCH NUMBER 4196\n",
      "Epoch [4/5] Loss_D: 1.0966 Loss_G: 0.6267\n",
      "BATCH NUMBER 4197\n",
      "Epoch [4/5] Loss_D: 1.0589 Loss_G: 0.6466\n",
      "BATCH NUMBER 4198\n",
      "Epoch [4/5] Loss_D: 1.0504 Loss_G: 0.6553\n",
      "BATCH NUMBER 4199\n",
      "Epoch [4/5] Loss_D: 1.1948 Loss_G: 0.5691\n",
      "BATCH NUMBER 4200\n",
      "Epoch [4/5] Loss_D: 1.0975 Loss_G: 0.6339\n",
      "BATCH NUMBER 4201\n",
      "Epoch [4/5] Loss_D: 1.0528 Loss_G: 0.6524\n",
      "BATCH NUMBER 4202\n",
      "Epoch [4/5] Loss_D: 1.1116 Loss_G: 0.6117\n",
      "BATCH NUMBER 4203\n",
      "Epoch [4/5] Loss_D: 1.1242 Loss_G: 0.6113\n",
      "BATCH NUMBER 4204\n",
      "Epoch [4/5] Loss_D: 1.0184 Loss_G: 0.6823\n",
      "BATCH NUMBER 4205\n",
      "Epoch [4/5] Loss_D: 1.2605 Loss_G: 0.5228\n",
      "BATCH NUMBER 4206\n",
      "Epoch [4/5] Loss_D: 1.0481 Loss_G: 0.6567\n",
      "BATCH NUMBER 4207\n",
      "Epoch [4/5] Loss_D: 1.1408 Loss_G: 0.6064\n",
      "BATCH NUMBER 4208\n",
      "Epoch [4/5] Loss_D: 1.0485 Loss_G: 0.6558\n",
      "BATCH NUMBER 4209\n",
      "Epoch [4/5] Loss_D: 1.1972 Loss_G: 0.5648\n",
      "BATCH NUMBER 4210\n",
      "Epoch [4/5] Loss_D: 1.1108 Loss_G: 0.6214\n",
      "BATCH NUMBER 4211\n",
      "Epoch [4/5] Loss_D: 1.0248 Loss_G: 0.6757\n",
      "BATCH NUMBER 4212\n",
      "Epoch [4/5] Loss_D: 1.0167 Loss_G: 0.6839\n",
      "BATCH NUMBER 4213\n",
      "Epoch [4/5] Loss_D: 1.1568 Loss_G: 0.5914\n",
      "BATCH NUMBER 4214\n",
      "Epoch [4/5] Loss_D: 1.0461 Loss_G: 0.6574\n",
      "BATCH NUMBER 4215\n",
      "Epoch [4/5] Loss_D: 1.2883 Loss_G: 0.5069\n",
      "BATCH NUMBER 4216\n",
      "Epoch [4/5] Loss_D: 1.0305 Loss_G: 0.6713\n",
      "BATCH NUMBER 4217\n",
      "Epoch [4/5] Loss_D: 1.2454 Loss_G: 0.5363\n",
      "BATCH NUMBER 4218\n",
      "Epoch [4/5] Loss_D: 1.1757 Loss_G: 0.5757\n",
      "BATCH NUMBER 4219\n",
      "Epoch [4/5] Loss_D: 1.2037 Loss_G: 0.5603\n",
      "BATCH NUMBER 4220\n",
      "Epoch [4/5] Loss_D: 1.1352 Loss_G: 0.6015\n",
      "BATCH NUMBER 4221\n",
      "Epoch [4/5] Loss_D: 1.1208 Loss_G: 0.6129\n",
      "BATCH NUMBER 4222\n",
      "Epoch [4/5] Loss_D: 1.0800 Loss_G: 0.6383\n",
      "BATCH NUMBER 4223\n",
      "Epoch [4/5] Loss_D: 1.0749 Loss_G: 0.6330\n",
      "BATCH NUMBER 4224\n",
      "Epoch [4/5] Loss_D: 1.0561 Loss_G: 0.6493\n",
      "BATCH NUMBER 4225\n",
      "Epoch [4/5] Loss_D: 1.1022 Loss_G: 0.6286\n",
      "BATCH NUMBER 4226\n",
      "Epoch [4/5] Loss_D: 1.0805 Loss_G: 0.6376\n",
      "BATCH NUMBER 4227\n",
      "Epoch [4/5] Loss_D: 1.0573 Loss_G: 0.6585\n",
      "BATCH NUMBER 4228\n",
      "Epoch [4/5] Loss_D: 1.1180 Loss_G: 0.6061\n",
      "BATCH NUMBER 4229\n",
      "Epoch [4/5] Loss_D: 1.1589 Loss_G: 0.5884\n",
      "BATCH NUMBER 4230\n",
      "Epoch [4/5] Loss_D: 1.2021 Loss_G: 0.5618\n",
      "BATCH NUMBER 4231\n",
      "Epoch [4/5] Loss_D: 1.0880 Loss_G: 0.6229\n",
      "BATCH NUMBER 4232\n",
      "Epoch [4/5] Loss_D: 1.1510 Loss_G: 0.5969\n",
      "BATCH NUMBER 4233\n",
      "Epoch [4/5] Loss_D: 1.2205 Loss_G: 0.5655\n",
      "BATCH NUMBER 4234\n",
      "Epoch [4/5] Loss_D: 1.0946 Loss_G: 0.6290\n",
      "BATCH NUMBER 4235\n",
      "Epoch [4/5] Loss_D: 1.0871 Loss_G: 0.6237\n",
      "BATCH NUMBER 4236\n",
      "Epoch [4/5] Loss_D: 1.0714 Loss_G: 0.6361\n",
      "BATCH NUMBER 4237\n",
      "Epoch [4/5] Loss_D: 1.1247 Loss_G: 0.6104\n",
      "BATCH NUMBER 4238\n",
      "Epoch [4/5] Loss_D: 1.1291 Loss_G: 0.6053\n",
      "BATCH NUMBER 4239\n",
      "Epoch [4/5] Loss_D: 1.1393 Loss_G: 0.6073\n",
      "BATCH NUMBER 4240\n",
      "Epoch [4/5] Loss_D: 1.1057 Loss_G: 0.6140\n",
      "BATCH NUMBER 4241\n",
      "Epoch [4/5] Loss_D: 1.0372 Loss_G: 0.6659\n",
      "BATCH NUMBER 4242\n",
      "Epoch [4/5] Loss_D: 1.1195 Loss_G: 0.6042\n",
      "BATCH NUMBER 4243\n",
      "Epoch [4/5] Loss_D: 1.0338 Loss_G: 0.6677\n",
      "BATCH NUMBER 4244\n",
      "Epoch [4/5] Loss_D: 1.0912 Loss_G: 0.6288\n",
      "BATCH NUMBER 4245\n",
      "Epoch [4/5] Loss_D: 1.0923 Loss_G: 0.6366\n",
      "BATCH NUMBER 4246\n",
      "Epoch [4/5] Loss_D: 1.2194 Loss_G: 0.5599\n",
      "BATCH NUMBER 4247\n",
      "Epoch [4/5] Loss_D: 1.1203 Loss_G: 0.6153\n",
      "BATCH NUMBER 4248\n",
      "Epoch [4/5] Loss_D: 1.2107 Loss_G: 0.5564\n",
      "BATCH NUMBER 4249\n",
      "Epoch [4/5] Loss_D: 1.1042 Loss_G: 0.6269\n",
      "BATCH NUMBER 4250\n",
      "Epoch [4/5] Loss_D: 1.1055 Loss_G: 0.6265\n",
      "BATCH NUMBER 4251\n",
      "Epoch [4/5] Loss_D: 1.0288 Loss_G: 0.6720\n",
      "BATCH NUMBER 4252\n",
      "Epoch [4/5] Loss_D: 1.0271 Loss_G: 0.6745\n",
      "BATCH NUMBER 4253\n",
      "Epoch [4/5] Loss_D: 1.0263 Loss_G: 0.6745\n",
      "BATCH NUMBER 4254\n",
      "Epoch [4/5] Loss_D: 1.0600 Loss_G: 0.6462\n",
      "BATCH NUMBER 4255\n",
      "Epoch [4/5] Loss_D: 1.0292 Loss_G: 0.6725\n",
      "BATCH NUMBER 4256\n",
      "Epoch [4/5] Loss_D: 1.0560 Loss_G: 0.6485\n",
      "BATCH NUMBER 4257\n",
      "Epoch [4/5] Loss_D: 1.0377 Loss_G: 0.6648\n",
      "BATCH NUMBER 4258\n",
      "Epoch [4/5] Loss_D: 1.0567 Loss_G: 0.6495\n",
      "BATCH NUMBER 4259\n",
      "Epoch [4/5] Loss_D: 1.1030 Loss_G: 0.6288\n",
      "BATCH NUMBER 4260\n",
      "Epoch [4/5] Loss_D: 1.0500 Loss_G: 0.6552\n",
      "BATCH NUMBER 4261\n",
      "Epoch [4/5] Loss_D: 1.0890 Loss_G: 0.6319\n",
      "BATCH NUMBER 4262\n",
      "Epoch [4/5] Loss_D: 1.0267 Loss_G: 0.6738\n",
      "BATCH NUMBER 4263\n",
      "Epoch [4/5] Loss_D: 1.1682 Loss_G: 0.5829\n",
      "BATCH NUMBER 4264\n",
      "Epoch [4/5] Loss_D: 1.1377 Loss_G: 0.6084\n",
      "BATCH NUMBER 4265\n",
      "Epoch [4/5] Loss_D: 1.0867 Loss_G: 0.6324\n",
      "BATCH NUMBER 4266\n",
      "Epoch [4/5] Loss_D: 1.0615 Loss_G: 0.6461\n",
      "BATCH NUMBER 4267\n",
      "Epoch [4/5] Loss_D: 1.1013 Loss_G: 0.6298\n",
      "BATCH NUMBER 4268\n",
      "Epoch [4/5] Loss_D: 1.0803 Loss_G: 0.6383\n",
      "BATCH NUMBER 4269\n",
      "Epoch [4/5] Loss_D: 1.2694 Loss_G: 0.5241\n",
      "BATCH NUMBER 4270\n",
      "Epoch [4/5] Loss_D: 1.0946 Loss_G: 0.6270\n",
      "BATCH NUMBER 4271\n",
      "Epoch [4/5] Loss_D: 1.1417 Loss_G: 0.5969\n",
      "BATCH NUMBER 4272\n",
      "Epoch [4/5] Loss_D: 1.0784 Loss_G: 0.6296\n",
      "BATCH NUMBER 4273\n",
      "Epoch [4/5] Loss_D: 1.0348 Loss_G: 0.6673\n",
      "BATCH NUMBER 4274\n",
      "Epoch [4/5] Loss_D: 1.0675 Loss_G: 0.6498\n",
      "BATCH NUMBER 4275\n",
      "Epoch [4/5] Loss_D: 1.1878 Loss_G: 0.5744\n",
      "BATCH NUMBER 4276\n",
      "Epoch [4/5] Loss_D: 1.0153 Loss_G: 0.6849\n",
      "BATCH NUMBER 4277\n",
      "Epoch [4/5] Loss_D: 1.2729 Loss_G: 0.5210\n",
      "BATCH NUMBER 4278\n",
      "Epoch [4/5] Loss_D: 1.1768 Loss_G: 0.5752\n",
      "BATCH NUMBER 4279\n",
      "Epoch [4/5] Loss_D: 1.0767 Loss_G: 0.6413\n",
      "BATCH NUMBER 4280\n",
      "Epoch [4/5] Loss_D: 1.0469 Loss_G: 0.6593\n",
      "BATCH NUMBER 4281\n",
      "Epoch [4/5] Loss_D: 1.1380 Loss_G: 0.6080\n",
      "BATCH NUMBER 4282\n",
      "Epoch [4/5] Loss_D: 1.1206 Loss_G: 0.6126\n",
      "BATCH NUMBER 4283\n",
      "Epoch [4/5] Loss_D: 1.0614 Loss_G: 0.6463\n",
      "BATCH NUMBER 4284\n",
      "Epoch [4/5] Loss_D: 1.1775 Loss_G: 0.5738\n",
      "BATCH NUMBER 4285\n",
      "Epoch [4/5] Loss_D: 1.0444 Loss_G: 0.6591\n",
      "BATCH NUMBER 4286\n",
      "Epoch [4/5] Loss_D: 1.0740 Loss_G: 0.6365\n",
      "BATCH NUMBER 4287\n",
      "Epoch [4/5] Loss_D: 1.1237 Loss_G: 0.6109\n",
      "BATCH NUMBER 4288\n",
      "Epoch [4/5] Loss_D: 1.1173 Loss_G: 0.6174\n",
      "BATCH NUMBER 4289\n",
      "Epoch [4/5] Loss_D: 1.1056 Loss_G: 0.6270\n",
      "BATCH NUMBER 4290\n",
      "Epoch [4/5] Loss_D: 1.0171 Loss_G: 0.6831\n",
      "BATCH NUMBER 4291\n",
      "Epoch [4/5] Loss_D: 1.0837 Loss_G: 0.6349\n",
      "BATCH NUMBER 4292\n",
      "Epoch [4/5] Loss_D: 1.1885 Loss_G: 0.5654\n",
      "BATCH NUMBER 4293\n",
      "Epoch [4/5] Loss_D: 1.1219 Loss_G: 0.6129\n",
      "BATCH NUMBER 4294\n",
      "Epoch [4/5] Loss_D: 1.1567 Loss_G: 0.5915\n",
      "BATCH NUMBER 4295\n",
      "Epoch [4/5] Loss_D: 1.0838 Loss_G: 0.6357\n",
      "BATCH NUMBER 4296\n",
      "Epoch [4/5] Loss_D: 1.0263 Loss_G: 0.6748\n",
      "BATCH NUMBER 4297\n",
      "Epoch [4/5] Loss_D: 1.3100 Loss_G: 0.4984\n",
      "BATCH NUMBER 4298\n",
      "Epoch [4/5] Loss_D: 1.2083 Loss_G: 0.5589\n",
      "BATCH NUMBER 4299\n",
      "Epoch [4/5] Loss_D: 1.2209 Loss_G: 0.5558\n",
      "BATCH NUMBER 4300\n",
      "Epoch [4/5] Loss_D: 1.0483 Loss_G: 0.6665\n",
      "BATCH NUMBER 4301\n",
      "Epoch [4/5] Loss_D: 1.1481 Loss_G: 0.5895\n",
      "BATCH NUMBER 4302\n",
      "Epoch [4/5] Loss_D: 1.0955 Loss_G: 0.6259\n",
      "BATCH NUMBER 4303\n",
      "Epoch [4/5] Loss_D: 1.0344 Loss_G: 0.6681\n",
      "BATCH NUMBER 4304\n",
      "Epoch [4/5] Loss_D: 1.2216 Loss_G: 0.5557\n",
      "BATCH NUMBER 4305\n",
      "Epoch [4/5] Loss_D: 1.0884 Loss_G: 0.6312\n",
      "BATCH NUMBER 4306\n",
      "Epoch [4/5] Loss_D: 1.0499 Loss_G: 0.6556\n",
      "BATCH NUMBER 4307\n",
      "Epoch [4/5] Loss_D: 1.0453 Loss_G: 0.6586\n",
      "BATCH NUMBER 4308\n",
      "Epoch [4/5] Loss_D: 1.0577 Loss_G: 0.6581\n",
      "BATCH NUMBER 4309\n",
      "Epoch [4/5] Loss_D: 1.0712 Loss_G: 0.6455\n",
      "BATCH NUMBER 4310\n",
      "Epoch [4/5] Loss_D: 1.0380 Loss_G: 0.6651\n",
      "BATCH NUMBER 4311\n",
      "Epoch [4/5] Loss_D: 1.0259 Loss_G: 0.6762\n",
      "BATCH NUMBER 4312\n",
      "Epoch [4/5] Loss_D: 1.0429 Loss_G: 0.6620\n",
      "BATCH NUMBER 4313\n",
      "Epoch [4/5] Loss_D: 1.0749 Loss_G: 0.6336\n",
      "BATCH NUMBER 4314\n",
      "Epoch [4/5] Loss_D: 1.0982 Loss_G: 0.6234\n",
      "BATCH NUMBER 4315\n",
      "Epoch [4/5] Loss_D: 1.0849 Loss_G: 0.6337\n",
      "BATCH NUMBER 4316\n",
      "Epoch [4/5] Loss_D: 1.0564 Loss_G: 0.6597\n",
      "BATCH NUMBER 4317\n",
      "Epoch [4/5] Loss_D: 1.0398 Loss_G: 0.6629\n",
      "BATCH NUMBER 4318\n",
      "Epoch [4/5] Loss_D: 1.1346 Loss_G: 0.6036\n",
      "BATCH NUMBER 4319\n",
      "Epoch [4/5] Loss_D: 1.1770 Loss_G: 0.5755\n",
      "BATCH NUMBER 4320\n",
      "Epoch [4/5] Loss_D: 1.0194 Loss_G: 0.6811\n",
      "BATCH NUMBER 4321\n",
      "Epoch [4/5] Loss_D: 1.0787 Loss_G: 0.6316\n",
      "BATCH NUMBER 4322\n",
      "Epoch [4/5] Loss_D: 1.0900 Loss_G: 0.6286\n",
      "BATCH NUMBER 4323\n",
      "Epoch [4/5] Loss_D: 1.1058 Loss_G: 0.6146\n",
      "BATCH NUMBER 4324\n",
      "Epoch [4/5] Loss_D: 1.1455 Loss_G: 0.6007\n",
      "BATCH NUMBER 4325\n",
      "Epoch [4/5] Loss_D: 1.0364 Loss_G: 0.6660\n",
      "BATCH NUMBER 4326\n",
      "Epoch [4/5] Loss_D: 1.1138 Loss_G: 0.6195\n",
      "BATCH NUMBER 4327\n",
      "Epoch [4/5] Loss_D: 1.0723 Loss_G: 0.6353\n",
      "BATCH NUMBER 4328\n",
      "Epoch [4/5] Loss_D: 1.1425 Loss_G: 0.5940\n",
      "BATCH NUMBER 4329\n",
      "Epoch [4/5] Loss_D: 1.1693 Loss_G: 0.5823\n",
      "BATCH NUMBER 4330\n",
      "Epoch [4/5] Loss_D: 1.1389 Loss_G: 0.6080\n",
      "BATCH NUMBER 4331\n",
      "Epoch [4/5] Loss_D: 1.2390 Loss_G: 0.5407\n",
      "BATCH NUMBER 4332\n",
      "Epoch [4/5] Loss_D: 1.0371 Loss_G: 0.6653\n",
      "BATCH NUMBER 4333\n",
      "Epoch [4/5] Loss_D: 1.0725 Loss_G: 0.6444\n",
      "BATCH NUMBER 4334\n",
      "Epoch [4/5] Loss_D: 1.0337 Loss_G: 0.6695\n",
      "BATCH NUMBER 4335\n",
      "Epoch [4/5] Loss_D: 1.1834 Loss_G: 0.5795\n",
      "BATCH NUMBER 4336\n",
      "Epoch [4/5] Loss_D: 1.0310 Loss_G: 0.6712\n",
      "BATCH NUMBER 4337\n",
      "Epoch [4/5] Loss_D: 1.0536 Loss_G: 0.6524\n",
      "BATCH NUMBER 4338\n",
      "Epoch [4/5] Loss_D: 1.0873 Loss_G: 0.6324\n",
      "BATCH NUMBER 4339\n",
      "Epoch [4/5] Loss_D: 1.0787 Loss_G: 0.6388\n",
      "BATCH NUMBER 4340\n",
      "Epoch [4/5] Loss_D: 1.0603 Loss_G: 0.6551\n",
      "BATCH NUMBER 4341\n",
      "Epoch [4/5] Loss_D: 1.1356 Loss_G: 0.5998\n",
      "BATCH NUMBER 4342\n",
      "Epoch [4/5] Loss_D: 1.1604 Loss_G: 0.5897\n",
      "BATCH NUMBER 4343\n",
      "Epoch [4/5] Loss_D: 1.1446 Loss_G: 0.6019\n",
      "BATCH NUMBER 4344\n",
      "Epoch [4/5] Loss_D: 1.0517 Loss_G: 0.6538\n",
      "BATCH NUMBER 4345\n",
      "Epoch [4/5] Loss_D: 1.0272 Loss_G: 0.6747\n",
      "BATCH NUMBER 4346\n",
      "Epoch [4/5] Loss_D: 1.0727 Loss_G: 0.6450\n",
      "BATCH NUMBER 4347\n",
      "Epoch [4/5] Loss_D: 1.0558 Loss_G: 0.6592\n",
      "BATCH NUMBER 4348\n",
      "Epoch [4/5] Loss_D: 1.2344 Loss_G: 0.5443\n",
      "BATCH NUMBER 4349\n",
      "Epoch [4/5] Loss_D: 1.0827 Loss_G: 0.6284\n",
      "BATCH NUMBER 4350\n",
      "Epoch [4/5] Loss_D: 1.0543 Loss_G: 0.6503\n",
      "BATCH NUMBER 4351\n",
      "Epoch [4/5] Loss_D: 1.2057 Loss_G: 0.5597\n",
      "BATCH NUMBER 4352\n",
      "Epoch [4/5] Loss_D: 1.0716 Loss_G: 0.6454\n",
      "BATCH NUMBER 4353\n",
      "Epoch [4/5] Loss_D: 1.0819 Loss_G: 0.6295\n",
      "BATCH NUMBER 4354\n",
      "Epoch [4/5] Loss_D: 1.1753 Loss_G: 0.5775\n",
      "BATCH NUMBER 4355\n",
      "Epoch [4/5] Loss_D: 1.0524 Loss_G: 0.6525\n",
      "BATCH NUMBER 4356\n",
      "Epoch [4/5] Loss_D: 1.0707 Loss_G: 0.6460\n",
      "BATCH NUMBER 4357\n",
      "Epoch [4/5] Loss_D: 1.2241 Loss_G: 0.5539\n",
      "BATCH NUMBER 4358\n",
      "Epoch [4/5] Loss_D: 1.0970 Loss_G: 0.6237\n",
      "BATCH NUMBER 4359\n",
      "Epoch [4/5] Loss_D: 1.0946 Loss_G: 0.6282\n",
      "BATCH NUMBER 4360\n",
      "Epoch [4/5] Loss_D: 1.0695 Loss_G: 0.6476\n",
      "BATCH NUMBER 4361\n",
      "Epoch [4/5] Loss_D: 1.0672 Loss_G: 0.6418\n",
      "BATCH NUMBER 4362\n",
      "Epoch [4/5] Loss_D: 1.1079 Loss_G: 0.6135\n",
      "BATCH NUMBER 4363\n",
      "Epoch [4/5] Loss_D: 1.0319 Loss_G: 0.6699\n",
      "BATCH NUMBER 4364\n",
      "Epoch [4/5] Loss_D: 1.1202 Loss_G: 0.6069\n",
      "BATCH NUMBER 4365\n",
      "Epoch [4/5] Loss_D: 1.0647 Loss_G: 0.6418\n",
      "BATCH NUMBER 4366\n",
      "Epoch [4/5] Loss_D: 1.1830 Loss_G: 0.5796\n",
      "BATCH NUMBER 4367\n",
      "Epoch [4/5] Loss_D: 1.2104 Loss_G: 0.5588\n",
      "BATCH NUMBER 4368\n",
      "Epoch [4/5] Loss_D: 1.1908 Loss_G: 0.5743\n",
      "BATCH NUMBER 4369\n",
      "Epoch [4/5] Loss_D: 1.1603 Loss_G: 0.5812\n",
      "BATCH NUMBER 4370\n",
      "Epoch [4/5] Loss_D: 1.0367 Loss_G: 0.6648\n",
      "BATCH NUMBER 4371\n",
      "Epoch [4/5] Loss_D: 1.0770 Loss_G: 0.6404\n",
      "BATCH NUMBER 4372\n",
      "Epoch [4/5] Loss_D: 1.2707 Loss_G: 0.5222\n",
      "BATCH NUMBER 4373\n",
      "Epoch [4/5] Loss_D: 1.0603 Loss_G: 0.6555\n",
      "BATCH NUMBER 4374\n",
      "Epoch [4/5] Loss_D: 1.0598 Loss_G: 0.6488\n",
      "BATCH NUMBER 4375\n",
      "Epoch [4/5] Loss_D: 1.0525 Loss_G: 0.6540\n",
      "BATCH NUMBER 4376\n",
      "Epoch [4/5] Loss_D: 1.1412 Loss_G: 0.5959\n",
      "BATCH NUMBER 4377\n",
      "Epoch [4/5] Loss_D: 1.1102 Loss_G: 0.6222\n",
      "BATCH NUMBER 4378\n",
      "Epoch [4/5] Loss_D: 1.1207 Loss_G: 0.6126\n",
      "BATCH NUMBER 4379\n",
      "Epoch [4/5] Loss_D: 1.1421 Loss_G: 0.5962\n",
      "BATCH NUMBER 4380\n",
      "Epoch [4/5] Loss_D: 1.1064 Loss_G: 0.6257\n",
      "BATCH NUMBER 4381\n",
      "Epoch [4/5] Loss_D: 1.0846 Loss_G: 0.6342\n",
      "BATCH NUMBER 4382\n",
      "Epoch [4/5] Loss_D: 1.0448 Loss_G: 0.6591\n",
      "BATCH NUMBER 4383\n",
      "Epoch [4/5] Loss_D: 1.1690 Loss_G: 0.5820\n",
      "BATCH NUMBER 4384\n",
      "Epoch [4/5] Loss_D: 1.0429 Loss_G: 0.6596\n",
      "BATCH NUMBER 4385\n",
      "Epoch [4/5] Loss_D: 1.1257 Loss_G: 0.6079\n",
      "BATCH NUMBER 4386\n",
      "Epoch [4/5] Loss_D: 1.0778 Loss_G: 0.6299\n",
      "BATCH NUMBER 4387\n",
      "Epoch [4/5] Loss_D: 1.0759 Loss_G: 0.6416\n",
      "BATCH NUMBER 4388\n",
      "Epoch [4/5] Loss_D: 1.0560 Loss_G: 0.6596\n",
      "BATCH NUMBER 4389\n",
      "Epoch [4/5] Loss_D: 1.1160 Loss_G: 0.6191\n",
      "BATCH NUMBER 4390\n",
      "Epoch [4/5] Loss_D: 1.1011 Loss_G: 0.6306\n",
      "BATCH NUMBER 4391\n",
      "Epoch [4/5] Loss_D: 1.3494 Loss_G: 0.4741\n",
      "BATCH NUMBER 4392\n",
      "Epoch [4/5] Loss_D: 1.0712 Loss_G: 0.6389\n",
      "BATCH NUMBER 4393\n",
      "Epoch [4/5] Loss_D: 1.1546 Loss_G: 0.5924\n",
      "BATCH NUMBER 4394\n",
      "Epoch [4/5] Loss_D: 1.1053 Loss_G: 0.6260\n",
      "BATCH NUMBER 4395\n",
      "Epoch [4/5] Loss_D: 1.0163 Loss_G: 0.6840\n",
      "BATCH NUMBER 4396\n",
      "Epoch [4/5] Loss_D: 1.1832 Loss_G: 0.5669\n",
      "BATCH NUMBER 4397\n",
      "Epoch [4/5] Loss_D: 1.0415 Loss_G: 0.6611\n",
      "BATCH NUMBER 4398\n",
      "Epoch [4/5] Loss_D: 1.1008 Loss_G: 0.6301\n",
      "BATCH NUMBER 4399\n",
      "Epoch [4/5] Loss_D: 1.2247 Loss_G: 0.5520\n",
      "BATCH NUMBER 4400\n",
      "Epoch [4/5] Loss_D: 1.0368 Loss_G: 0.6658\n",
      "BATCH NUMBER 4401\n",
      "Epoch [4/5] Loss_D: 1.3232 Loss_G: 0.4871\n",
      "BATCH NUMBER 4402\n",
      "Epoch [4/5] Loss_D: 1.1404 Loss_G: 0.5970\n",
      "BATCH NUMBER 4403\n",
      "Epoch [4/5] Loss_D: 1.1137 Loss_G: 0.6180\n",
      "BATCH NUMBER 4404\n",
      "Epoch [4/5] Loss_D: 1.0774 Loss_G: 0.6402\n",
      "BATCH NUMBER 4405\n",
      "Epoch [4/5] Loss_D: 1.0370 Loss_G: 0.6657\n",
      "BATCH NUMBER 4406\n",
      "Epoch [4/5] Loss_D: 1.2256 Loss_G: 0.5510\n",
      "BATCH NUMBER 4407\n",
      "Epoch [4/5] Loss_D: 1.0943 Loss_G: 0.6260\n",
      "BATCH NUMBER 4408\n",
      "Epoch [4/5] Loss_D: 1.1081 Loss_G: 0.6233\n",
      "BATCH NUMBER 4409\n",
      "Epoch [4/5] Loss_D: 1.0273 Loss_G: 0.6749\n",
      "BATCH NUMBER 4410\n",
      "Epoch [4/5] Loss_D: 1.2542 Loss_G: 0.5288\n",
      "BATCH NUMBER 4411\n",
      "Epoch [4/5] Loss_D: 1.0677 Loss_G: 0.6494\n",
      "BATCH NUMBER 4412\n",
      "Epoch [4/5] Loss_D: 1.0737 Loss_G: 0.6369\n",
      "BATCH NUMBER 4413\n",
      "Epoch [4/5] Loss_D: 1.0568 Loss_G: 0.6484\n",
      "BATCH NUMBER 4414\n",
      "Epoch [4/5] Loss_D: 1.0738 Loss_G: 0.6456\n",
      "BATCH NUMBER 4415\n",
      "Epoch [4/5] Loss_D: 1.0670 Loss_G: 0.6505\n",
      "BATCH NUMBER 4416\n",
      "Epoch [4/5] Loss_D: 1.1545 Loss_G: 0.5938\n",
      "BATCH NUMBER 4417\n",
      "Epoch [4/5] Loss_D: 1.0907 Loss_G: 0.6287\n",
      "BATCH NUMBER 4418\n",
      "Epoch [4/5] Loss_D: 1.0322 Loss_G: 0.6703\n",
      "BATCH NUMBER 4419\n",
      "Epoch [4/5] Loss_D: 1.0703 Loss_G: 0.6462\n",
      "BATCH NUMBER 4420\n",
      "Epoch [4/5] Loss_D: 1.1173 Loss_G: 0.6146\n",
      "BATCH NUMBER 4421\n",
      "Epoch [4/5] Loss_D: 1.2048 Loss_G: 0.5600\n",
      "BATCH NUMBER 4422\n",
      "Epoch [4/5] Loss_D: 1.2095 Loss_G: 0.5572\n",
      "BATCH NUMBER 4423\n",
      "Epoch [4/5] Loss_D: 1.1083 Loss_G: 0.6147\n",
      "BATCH NUMBER 4424\n",
      "Epoch [4/5] Loss_D: 1.1263 Loss_G: 0.6087\n",
      "BATCH NUMBER 4425\n",
      "Epoch [4/5] Loss_D: 1.1538 Loss_G: 0.5853\n",
      "BATCH NUMBER 4426\n",
      "Epoch [4/5] Loss_D: 1.1006 Loss_G: 0.6224\n",
      "BATCH NUMBER 4427\n",
      "Epoch [4/5] Loss_D: 1.1473 Loss_G: 0.5996\n",
      "BATCH NUMBER 4428\n",
      "Epoch [4/5] Loss_D: 1.1535 Loss_G: 0.5949\n",
      "BATCH NUMBER 4429\n",
      "Epoch [4/5] Loss_D: 1.0991 Loss_G: 0.6227\n",
      "BATCH NUMBER 4430\n",
      "Epoch [4/5] Loss_D: 1.0739 Loss_G: 0.6440\n",
      "BATCH NUMBER 4431\n",
      "Epoch [4/5] Loss_D: 1.0614 Loss_G: 0.6545\n",
      "BATCH NUMBER 4432\n",
      "Epoch [4/5] Loss_D: 1.0426 Loss_G: 0.6615\n",
      "BATCH NUMBER 4433\n",
      "Epoch [4/5] Loss_D: 1.0744 Loss_G: 0.6437\n",
      "BATCH NUMBER 4434\n",
      "Epoch [4/5] Loss_D: 1.0993 Loss_G: 0.6250\n",
      "BATCH NUMBER 4435\n",
      "Epoch [4/5] Loss_D: 1.1531 Loss_G: 0.5946\n",
      "BATCH NUMBER 4436\n",
      "Epoch [4/5] Loss_D: 1.0594 Loss_G: 0.6562\n",
      "BATCH NUMBER 4437\n",
      "Epoch [4/5] Loss_D: 1.0978 Loss_G: 0.6330\n",
      "BATCH NUMBER 4438\n",
      "Epoch [4/5] Loss_D: 1.0965 Loss_G: 0.6238\n",
      "BATCH NUMBER 4439\n",
      "Epoch [4/5] Loss_D: 1.0871 Loss_G: 0.6319\n",
      "BATCH NUMBER 4440\n",
      "Epoch [4/5] Loss_D: 1.1452 Loss_G: 0.6020\n",
      "BATCH NUMBER 4441\n",
      "Epoch [4/5] Loss_D: 1.0381 Loss_G: 0.6645\n",
      "BATCH NUMBER 4442\n",
      "Epoch [4/5] Loss_D: 1.0674 Loss_G: 0.6412\n",
      "BATCH NUMBER 4443\n",
      "Epoch [4/5] Loss_D: 1.2890 Loss_G: 0.5089\n",
      "BATCH NUMBER 4444\n",
      "Epoch [4/5] Loss_D: 1.1608 Loss_G: 0.5876\n",
      "BATCH NUMBER 4445\n",
      "Epoch [4/5] Loss_D: 1.0526 Loss_G: 0.6517\n",
      "BATCH NUMBER 4446\n",
      "Epoch [4/5] Loss_D: 1.0716 Loss_G: 0.6463\n",
      "BATCH NUMBER 4447\n",
      "Epoch [4/5] Loss_D: 1.0816 Loss_G: 0.6368\n",
      "BATCH NUMBER 4448\n",
      "Epoch [4/5] Loss_D: 1.0463 Loss_G: 0.6648\n",
      "BATCH NUMBER 4449\n",
      "Epoch [4/5] Loss_D: 1.1343 Loss_G: 0.6024\n",
      "BATCH NUMBER 4450\n",
      "Epoch [4/5] Loss_D: 1.0269 Loss_G: 0.6779\n",
      "BATCH NUMBER 4451\n",
      "Epoch [4/5] Loss_D: 1.2357 Loss_G: 0.5473\n",
      "BATCH NUMBER 4452\n",
      "Epoch [4/5] Loss_D: 1.2482 Loss_G: 0.5317\n",
      "BATCH NUMBER 4453\n",
      "Epoch [4/5] Loss_D: 1.0616 Loss_G: 0.6451\n",
      "BATCH NUMBER 4454\n",
      "Epoch [4/5] Loss_D: 1.1177 Loss_G: 0.6158\n",
      "BATCH NUMBER 4455\n",
      "Epoch [4/5] Loss_D: 1.0992 Loss_G: 0.6318\n",
      "BATCH NUMBER 4456\n",
      "Epoch [4/5] Loss_D: 1.0479 Loss_G: 0.6569\n",
      "BATCH NUMBER 4457\n",
      "Epoch [4/5] Loss_D: 1.0787 Loss_G: 0.6401\n",
      "BATCH NUMBER 4458\n",
      "Epoch [4/5] Loss_D: 1.0636 Loss_G: 0.6531\n",
      "BATCH NUMBER 4459\n",
      "Epoch [4/5] Loss_D: 1.0399 Loss_G: 0.6631\n",
      "BATCH NUMBER 4460\n",
      "Epoch [4/5] Loss_D: 1.0467 Loss_G: 0.6565\n",
      "BATCH NUMBER 4461\n",
      "Epoch [4/5] Loss_D: 1.0770 Loss_G: 0.6437\n",
      "BATCH NUMBER 4462\n",
      "Epoch [4/5] Loss_D: 1.1201 Loss_G: 0.6131\n",
      "BATCH NUMBER 4463\n",
      "Epoch [4/5] Loss_D: 1.1283 Loss_G: 0.6261\n",
      "BATCH NUMBER 4464\n",
      "Epoch [4/5] Loss_D: 1.0714 Loss_G: 0.6360\n",
      "BATCH NUMBER 4465\n",
      "Epoch [4/5] Loss_D: 1.1013 Loss_G: 0.6193\n",
      "BATCH NUMBER 4466\n",
      "Epoch [4/5] Loss_D: 1.1801 Loss_G: 0.5814\n",
      "BATCH NUMBER 4467\n",
      "Epoch [4/5] Loss_D: 1.2743 Loss_G: 0.5194\n",
      "BATCH NUMBER 4468\n",
      "Epoch [4/5] Loss_D: 1.0499 Loss_G: 0.6549\n",
      "BATCH NUMBER 4469\n",
      "Epoch [4/5] Loss_D: 1.0433 Loss_G: 0.6595\n",
      "BATCH NUMBER 4470\n",
      "Epoch [4/5] Loss_D: 1.0817 Loss_G: 0.6255\n",
      "BATCH NUMBER 4471\n",
      "Epoch [4/5] Loss_D: 1.0471 Loss_G: 0.6569\n",
      "BATCH NUMBER 4472\n",
      "Epoch [4/5] Loss_D: 1.0193 Loss_G: 0.6818\n",
      "BATCH NUMBER 4473\n",
      "Epoch [4/5] Loss_D: 1.0362 Loss_G: 0.6665\n",
      "BATCH NUMBER 4474\n",
      "Epoch [4/5] Loss_D: 1.1233 Loss_G: 0.6102\n",
      "BATCH NUMBER 4475\n",
      "Epoch [4/5] Loss_D: 1.0336 Loss_G: 0.6685\n",
      "BATCH NUMBER 4476\n",
      "Epoch [4/5] Loss_D: 1.0572 Loss_G: 0.6470\n",
      "BATCH NUMBER 4477\n",
      "Epoch [4/5] Loss_D: 1.0799 Loss_G: 0.6408\n",
      "BATCH NUMBER 4478\n",
      "Epoch [4/5] Loss_D: 1.0588 Loss_G: 0.6470\n",
      "BATCH NUMBER 4479\n",
      "Epoch [4/5] Loss_D: 1.1593 Loss_G: 0.5897\n",
      "BATCH NUMBER 4480\n",
      "Epoch [4/5] Loss_D: 1.3122 Loss_G: 0.4962\n",
      "BATCH NUMBER 4481\n",
      "Epoch [4/5] Loss_D: 1.0557 Loss_G: 0.6503\n",
      "BATCH NUMBER 4482\n",
      "Epoch [4/5] Loss_D: 1.0287 Loss_G: 0.6728\n",
      "BATCH NUMBER 4483\n",
      "Epoch [4/5] Loss_D: 1.0660 Loss_G: 0.6416\n",
      "BATCH NUMBER 4484\n",
      "Epoch [4/5] Loss_D: 1.1047 Loss_G: 0.6088\n",
      "BATCH NUMBER 4485\n",
      "Epoch [4/5] Loss_D: 1.0485 Loss_G: 0.6557\n",
      "BATCH NUMBER 4486\n",
      "Epoch [4/5] Loss_D: 1.1367 Loss_G: 0.6001\n",
      "BATCH NUMBER 4487\n",
      "Epoch [4/5] Loss_D: 1.0743 Loss_G: 0.6429\n",
      "BATCH NUMBER 4488\n",
      "Epoch [4/5] Loss_D: 1.0525 Loss_G: 0.6512\n",
      "BATCH NUMBER 4489\n",
      "Epoch [4/5] Loss_D: 1.1798 Loss_G: 0.5745\n",
      "BATCH NUMBER 4490\n",
      "Epoch [4/5] Loss_D: 1.1247 Loss_G: 0.6105\n",
      "BATCH NUMBER 4491\n",
      "Epoch [4/5] Loss_D: 1.1557 Loss_G: 0.5920\n",
      "BATCH NUMBER 4492\n",
      "Epoch [4/5] Loss_D: 1.0306 Loss_G: 0.6720\n",
      "BATCH NUMBER 4493\n",
      "Epoch [4/5] Loss_D: 1.0496 Loss_G: 0.6652\n",
      "BATCH NUMBER 4494\n",
      "Epoch [4/5] Loss_D: 1.0560 Loss_G: 0.6490\n",
      "BATCH NUMBER 4495\n",
      "Epoch [4/5] Loss_D: 1.1353 Loss_G: 0.5997\n",
      "BATCH NUMBER 4496\n",
      "Epoch [4/5] Loss_D: 1.0548 Loss_G: 0.6606\n",
      "BATCH NUMBER 4497\n",
      "Epoch [4/5] Loss_D: 1.1981 Loss_G: 0.5647\n",
      "BATCH NUMBER 4498\n",
      "Epoch [4/5] Loss_D: 1.0381 Loss_G: 0.6651\n",
      "BATCH NUMBER 4499\n",
      "Epoch [4/5] Loss_D: 1.2181 Loss_G: 0.5578\n",
      "BATCH NUMBER 4500\n",
      "Epoch [4/5] Loss_D: 1.0262 Loss_G: 0.6764\n",
      "BATCH NUMBER 4501\n",
      "Epoch [4/5] Loss_D: 1.3161 Loss_G: 0.4954\n",
      "BATCH NUMBER 4502\n",
      "Epoch [4/5] Loss_D: 1.0324 Loss_G: 0.6694\n",
      "BATCH NUMBER 4503\n",
      "Epoch [4/5] Loss_D: 1.1538 Loss_G: 0.5939\n",
      "BATCH NUMBER 4504\n",
      "Epoch [4/5] Loss_D: 1.0805 Loss_G: 0.6381\n",
      "BATCH NUMBER 4505\n",
      "Epoch [4/5] Loss_D: 1.0943 Loss_G: 0.6278\n",
      "BATCH NUMBER 4506\n",
      "Epoch [4/5] Loss_D: 1.0517 Loss_G: 0.6525\n",
      "BATCH NUMBER 4507\n",
      "Epoch [4/5] Loss_D: 1.0559 Loss_G: 0.6494\n",
      "BATCH NUMBER 4508\n",
      "Epoch [4/5] Loss_D: 1.0951 Loss_G: 0.6275\n",
      "BATCH NUMBER 4509\n",
      "Epoch [4/5] Loss_D: 1.1269 Loss_G: 0.6076\n",
      "BATCH NUMBER 4510\n",
      "Epoch [4/5] Loss_D: 1.0879 Loss_G: 0.6314\n",
      "BATCH NUMBER 4511\n",
      "Epoch [4/5] Loss_D: 1.2401 Loss_G: 0.5394\n",
      "BATCH NUMBER 4512\n",
      "Epoch [4/5] Loss_D: 1.0591 Loss_G: 0.6463\n",
      "BATCH NUMBER 4513\n",
      "Epoch [4/5] Loss_D: 1.1068 Loss_G: 0.6257\n",
      "BATCH NUMBER 4514\n",
      "Epoch [4/5] Loss_D: 1.0323 Loss_G: 0.6695\n",
      "BATCH NUMBER 4515\n",
      "Epoch [4/5] Loss_D: 1.0378 Loss_G: 0.6685\n",
      "BATCH NUMBER 4516\n",
      "Epoch [4/5] Loss_D: 1.0504 Loss_G: 0.6544\n",
      "BATCH NUMBER 4517\n",
      "Epoch [4/5] Loss_D: 1.2046 Loss_G: 0.5618\n",
      "BATCH NUMBER 4518\n",
      "Epoch [4/5] Loss_D: 1.0533 Loss_G: 0.6515\n",
      "BATCH NUMBER 4519\n",
      "Epoch [4/5] Loss_D: 1.0290 Loss_G: 0.6721\n",
      "BATCH NUMBER 4520\n",
      "Epoch [4/5] Loss_D: 1.2312 Loss_G: 0.5478\n",
      "BATCH NUMBER 4521\n",
      "Epoch [4/5] Loss_D: 1.1086 Loss_G: 0.6234\n",
      "BATCH NUMBER 4522\n",
      "Epoch [4/5] Loss_D: 1.2470 Loss_G: 0.5433\n",
      "BATCH NUMBER 4523\n",
      "Epoch [4/5] Loss_D: 1.0549 Loss_G: 0.6509\n",
      "BATCH NUMBER 4524\n",
      "Epoch [4/5] Loss_D: 1.0845 Loss_G: 0.6336\n",
      "BATCH NUMBER 4525\n",
      "Epoch [4/5] Loss_D: 1.0345 Loss_G: 0.6680\n",
      "BATCH NUMBER 4526\n",
      "Epoch [4/5] Loss_D: 1.0907 Loss_G: 0.6294\n",
      "BATCH NUMBER 4527\n",
      "Epoch [4/5] Loss_D: 1.0254 Loss_G: 0.6758\n",
      "BATCH NUMBER 4528\n",
      "Epoch [4/5] Loss_D: 1.0873 Loss_G: 0.6321\n",
      "BATCH NUMBER 4529\n",
      "Epoch [4/5] Loss_D: 1.0092 Loss_G: 0.6906\n",
      "BATCH NUMBER 4530\n",
      "Epoch [4/5] Loss_D: 1.1736 Loss_G: 0.5870\n",
      "BATCH NUMBER 4531\n",
      "Epoch [4/5] Loss_D: 1.3416 Loss_G: 0.4807\n",
      "BATCH NUMBER 4532\n",
      "Epoch [4/5] Loss_D: 1.0381 Loss_G: 0.6710\n",
      "BATCH NUMBER 4533\n",
      "Epoch [4/5] Loss_D: 1.1291 Loss_G: 0.6061\n",
      "BATCH NUMBER 4534\n",
      "Epoch [4/5] Loss_D: 1.1178 Loss_G: 0.6142\n",
      "BATCH NUMBER 4535\n",
      "Epoch [4/5] Loss_D: 1.0902 Loss_G: 0.6312\n",
      "BATCH NUMBER 4536\n",
      "Epoch [4/5] Loss_D: 1.1032 Loss_G: 0.6282\n",
      "BATCH NUMBER 4537\n",
      "Epoch [4/5] Loss_D: 1.1362 Loss_G: 0.6093\n",
      "BATCH NUMBER 4538\n",
      "Epoch [4/5] Loss_D: 1.2792 Loss_G: 0.5146\n",
      "BATCH NUMBER 4539\n",
      "Epoch [4/5] Loss_D: 1.0620 Loss_G: 0.6543\n",
      "BATCH NUMBER 4540\n",
      "Epoch [4/5] Loss_D: 1.2213 Loss_G: 0.5554\n",
      "BATCH NUMBER 4541\n",
      "Epoch [4/5] Loss_D: 1.0738 Loss_G: 0.6431\n",
      "BATCH NUMBER 4542\n",
      "Epoch [4/5] Loss_D: 1.0745 Loss_G: 0.6431\n",
      "BATCH NUMBER 4543\n",
      "Epoch [4/5] Loss_D: 1.1059 Loss_G: 0.6181\n",
      "BATCH NUMBER 4544\n",
      "Epoch [4/5] Loss_D: 1.0700 Loss_G: 0.6465\n",
      "BATCH NUMBER 4545\n",
      "Epoch [4/5] Loss_D: 1.1476 Loss_G: 0.6005\n",
      "BATCH NUMBER 4546\n",
      "Epoch [4/5] Loss_D: 1.0917 Loss_G: 0.6289\n",
      "BATCH NUMBER 4547\n",
      "Epoch [4/5] Loss_D: 1.0327 Loss_G: 0.6705\n",
      "BATCH NUMBER 4548\n",
      "Epoch [4/5] Loss_D: 1.1053 Loss_G: 0.6171\n",
      "BATCH NUMBER 4549\n",
      "Epoch [4/5] Loss_D: 1.1359 Loss_G: 0.6079\n",
      "BATCH NUMBER 4550\n",
      "Epoch [4/5] Loss_D: 1.1639 Loss_G: 0.5994\n",
      "BATCH NUMBER 4551\n",
      "Epoch [4/5] Loss_D: 1.2454 Loss_G: 0.5344\n",
      "BATCH NUMBER 4552\n",
      "Epoch [4/5] Loss_D: 1.1052 Loss_G: 0.6270\n",
      "BATCH NUMBER 4553\n",
      "Epoch [4/5] Loss_D: 1.0419 Loss_G: 0.6627\n",
      "BATCH NUMBER 4554\n",
      "Epoch [4/5] Loss_D: 1.0167 Loss_G: 0.6837\n",
      "BATCH NUMBER 4555\n",
      "Epoch [4/5] Loss_D: 1.0400 Loss_G: 0.6642\n",
      "BATCH NUMBER 4556\n",
      "Epoch [4/5] Loss_D: 1.0368 Loss_G: 0.6649\n",
      "BATCH NUMBER 4557\n",
      "Epoch [4/5] Loss_D: 1.0433 Loss_G: 0.6604\n",
      "BATCH NUMBER 4558\n",
      "Epoch [4/5] Loss_D: 1.1069 Loss_G: 0.6251\n",
      "BATCH NUMBER 4559\n",
      "Epoch [4/5] Loss_D: 1.2615 Loss_G: 0.5313\n",
      "BATCH NUMBER 4560\n",
      "Epoch [4/5] Loss_D: 1.1103 Loss_G: 0.6117\n",
      "BATCH NUMBER 4561\n",
      "Epoch [4/5] Loss_D: 1.1476 Loss_G: 0.6002\n",
      "BATCH NUMBER 4562\n",
      "Epoch [4/5] Loss_D: 1.2291 Loss_G: 0.5487\n",
      "BATCH NUMBER 4563\n",
      "Epoch [4/5] Loss_D: 1.2086 Loss_G: 0.5591\n",
      "BATCH NUMBER 4564\n",
      "Epoch [4/5] Loss_D: 1.1160 Loss_G: 0.6168\n",
      "BATCH NUMBER 4565\n",
      "Epoch [4/5] Loss_D: 1.1402 Loss_G: 0.6060\n",
      "BATCH NUMBER 4566\n",
      "Epoch [4/5] Loss_D: 1.1361 Loss_G: 0.5997\n",
      "BATCH NUMBER 4567\n",
      "Epoch [4/5] Loss_D: 1.1322 Loss_G: 0.6130\n",
      "BATCH NUMBER 4568\n",
      "Epoch [4/5] Loss_D: 1.0845 Loss_G: 0.6340\n",
      "BATCH NUMBER 4569\n",
      "Epoch [4/5] Loss_D: 1.0618 Loss_G: 0.6544\n",
      "BATCH NUMBER 4570\n",
      "Epoch [4/5] Loss_D: 1.0418 Loss_G: 0.6608\n",
      "BATCH NUMBER 4571\n",
      "Epoch [4/5] Loss_D: 1.0236 Loss_G: 0.6773\n",
      "BATCH NUMBER 4572\n",
      "Epoch [4/5] Loss_D: 1.0920 Loss_G: 0.6274\n",
      "BATCH NUMBER 4573\n",
      "Epoch [4/5] Loss_D: 1.2263 Loss_G: 0.5437\n",
      "BATCH NUMBER 4574\n",
      "Epoch [4/5] Loss_D: 1.0473 Loss_G: 0.6569\n",
      "BATCH NUMBER 4575\n",
      "Epoch [4/5] Loss_D: 1.0288 Loss_G: 0.6726\n",
      "BATCH NUMBER 4576\n",
      "Epoch [4/5] Loss_D: 1.0934 Loss_G: 0.6264\n",
      "BATCH NUMBER 4577\n",
      "Epoch [4/5] Loss_D: 1.0324 Loss_G: 0.6696\n",
      "BATCH NUMBER 4578\n",
      "Epoch [4/5] Loss_D: 1.0223 Loss_G: 0.6791\n",
      "BATCH NUMBER 4579\n",
      "Epoch [4/5] Loss_D: 1.0969 Loss_G: 0.6247\n",
      "BATCH NUMBER 4580\n",
      "Epoch [4/5] Loss_D: 1.0856 Loss_G: 0.6340\n",
      "BATCH NUMBER 4581\n",
      "Epoch [4/5] Loss_D: 1.0742 Loss_G: 0.6437\n",
      "BATCH NUMBER 4582\n",
      "Epoch [4/5] Loss_D: 1.0435 Loss_G: 0.6610\n",
      "BATCH NUMBER 4583\n",
      "Epoch [4/5] Loss_D: 1.1124 Loss_G: 0.6195\n",
      "BATCH NUMBER 4584\n",
      "Epoch [4/5] Loss_D: 1.1539 Loss_G: 0.5845\n",
      "BATCH NUMBER 4585\n",
      "Epoch [4/5] Loss_D: 1.0225 Loss_G: 0.6784\n",
      "BATCH NUMBER 4586\n",
      "Epoch [4/5] Loss_D: 1.1453 Loss_G: 0.6011\n",
      "BATCH NUMBER 4587\n",
      "Epoch [4/5] Loss_D: 1.1088 Loss_G: 0.6234\n",
      "BATCH NUMBER 4588\n",
      "Epoch [4/5] Loss_D: 1.2050 Loss_G: 0.5697\n",
      "BATCH NUMBER 4589\n",
      "Epoch [4/5] Loss_D: 1.1159 Loss_G: 0.6169\n",
      "BATCH NUMBER 4590\n",
      "Epoch [4/5] Loss_D: 1.0955 Loss_G: 0.6240\n",
      "BATCH NUMBER 4591\n",
      "Epoch [4/5] Loss_D: 1.1104 Loss_G: 0.6216\n",
      "BATCH NUMBER 4592\n",
      "Epoch [4/5] Loss_D: 1.0569 Loss_G: 0.6474\n",
      "BATCH NUMBER 4593\n",
      "Epoch [4/5] Loss_D: 1.0765 Loss_G: 0.6416\n",
      "BATCH NUMBER 4594\n",
      "Epoch [4/5] Loss_D: 1.1446 Loss_G: 0.6018\n",
      "BATCH NUMBER 4595\n",
      "Epoch [4/5] Loss_D: 1.1141 Loss_G: 0.6190\n",
      "BATCH NUMBER 4596\n",
      "Epoch [4/5] Loss_D: 1.0322 Loss_G: 0.6695\n",
      "BATCH NUMBER 4597\n",
      "Epoch [4/5] Loss_D: 1.0584 Loss_G: 0.6478\n",
      "BATCH NUMBER 4598\n",
      "Epoch [4/5] Loss_D: 1.0334 Loss_G: 0.6686\n",
      "BATCH NUMBER 4599\n",
      "Epoch [4/5] Loss_D: 1.0675 Loss_G: 0.6423\n",
      "BATCH NUMBER 4600\n",
      "Epoch [4/5] Loss_D: 1.2265 Loss_G: 0.5428\n",
      "BATCH NUMBER 4601\n",
      "Epoch [4/5] Loss_D: 1.2537 Loss_G: 0.5298\n",
      "BATCH NUMBER 4602\n",
      "Epoch [4/5] Loss_D: 1.1050 Loss_G: 0.6262\n",
      "BATCH NUMBER 4603\n",
      "Epoch [4/5] Loss_D: 1.1783 Loss_G: 0.5738\n",
      "BATCH NUMBER 4604\n",
      "Epoch [4/5] Loss_D: 1.0293 Loss_G: 0.6721\n",
      "BATCH NUMBER 4605\n",
      "Epoch [4/5] Loss_D: 1.1904 Loss_G: 0.5725\n",
      "BATCH NUMBER 4606\n",
      "Epoch [4/5] Loss_D: 1.0597 Loss_G: 0.6557\n",
      "BATCH NUMBER 4607\n",
      "Epoch [4/5] Loss_D: 1.0627 Loss_G: 0.6534\n",
      "BATCH NUMBER 4608\n",
      "Epoch [4/5] Loss_D: 1.1999 Loss_G: 0.5638\n",
      "BATCH NUMBER 4609\n",
      "Epoch [4/5] Loss_D: 1.1668 Loss_G: 0.5823\n",
      "BATCH NUMBER 4610\n",
      "Epoch [4/5] Loss_D: 1.0568 Loss_G: 0.6484\n",
      "BATCH NUMBER 4611\n",
      "Epoch [4/5] Loss_D: 1.0561 Loss_G: 0.6498\n",
      "BATCH NUMBER 4612\n",
      "Epoch [4/5] Loss_D: 1.1360 Loss_G: 0.6001\n",
      "BATCH NUMBER 4613\n",
      "Epoch [4/5] Loss_D: 1.0984 Loss_G: 0.6247\n",
      "BATCH NUMBER 4614\n",
      "Epoch [4/5] Loss_D: 1.0266 Loss_G: 0.6796\n",
      "BATCH NUMBER 4615\n",
      "Epoch [4/5] Loss_D: 1.0631 Loss_G: 0.6529\n",
      "BATCH NUMBER 4616\n",
      "Epoch [4/5] Loss_D: 1.0269 Loss_G: 0.6745\n",
      "BATCH NUMBER 4617\n",
      "Epoch [4/5] Loss_D: 1.1928 Loss_G: 0.5702\n",
      "BATCH NUMBER 4618\n",
      "Epoch [4/5] Loss_D: 1.0188 Loss_G: 0.6816\n",
      "BATCH NUMBER 4619\n",
      "Epoch [4/5] Loss_D: 1.1843 Loss_G: 0.5787\n",
      "BATCH NUMBER 4620\n",
      "Epoch [4/5] Loss_D: 1.0785 Loss_G: 0.6399\n",
      "BATCH NUMBER 4621\n",
      "Epoch [4/5] Loss_D: 1.0588 Loss_G: 0.6565\n",
      "BATCH NUMBER 4622\n",
      "Epoch [4/5] Loss_D: 1.0865 Loss_G: 0.6336\n",
      "BATCH NUMBER 4623\n",
      "Epoch [4/5] Loss_D: 1.0598 Loss_G: 0.6466\n",
      "BATCH NUMBER 4624\n",
      "Epoch [4/5] Loss_D: 1.0260 Loss_G: 0.6749\n",
      "BATCH NUMBER 4625\n",
      "Epoch [4/5] Loss_D: 1.1001 Loss_G: 0.6304\n",
      "BATCH NUMBER 4626\n",
      "Epoch [4/5] Loss_D: 1.1128 Loss_G: 0.6190\n",
      "BATCH NUMBER 4627\n",
      "Epoch [4/5] Loss_D: 1.1314 Loss_G: 0.6049\n",
      "BATCH NUMBER 4628\n",
      "Epoch [4/5] Loss_D: 1.1811 Loss_G: 0.5804\n",
      "BATCH NUMBER 4629\n",
      "Epoch [4/5] Loss_D: 1.0575 Loss_G: 0.6467\n",
      "BATCH NUMBER 4630\n",
      "Epoch [4/5] Loss_D: 1.0614 Loss_G: 0.6542\n",
      "BATCH NUMBER 4631\n",
      "Epoch [4/5] Loss_D: 1.0709 Loss_G: 0.6465\n",
      "BATCH NUMBER 4632\n",
      "Epoch [4/5] Loss_D: 1.0391 Loss_G: 0.6647\n",
      "BATCH NUMBER 4633\n",
      "Epoch [4/5] Loss_D: 1.0295 Loss_G: 0.6724\n",
      "BATCH NUMBER 4634\n",
      "Epoch [4/5] Loss_D: 1.0865 Loss_G: 0.6325\n",
      "BATCH NUMBER 4635\n",
      "Epoch [4/5] Loss_D: 1.2350 Loss_G: 0.5374\n",
      "BATCH NUMBER 4636\n",
      "Epoch [4/5] Loss_D: 1.0663 Loss_G: 0.6504\n",
      "BATCH NUMBER 4637\n",
      "Epoch [4/5] Loss_D: 1.2357 Loss_G: 0.5430\n",
      "BATCH NUMBER 4638\n",
      "Epoch [4/5] Loss_D: 1.1154 Loss_G: 0.6177\n",
      "BATCH NUMBER 4639\n",
      "Epoch [4/5] Loss_D: 1.1540 Loss_G: 0.5838\n",
      "BATCH NUMBER 4640\n",
      "Epoch [4/5] Loss_D: 1.1074 Loss_G: 0.6243\n",
      "BATCH NUMBER 4641\n",
      "Epoch [4/5] Loss_D: 1.1036 Loss_G: 0.6279\n",
      "BATCH NUMBER 4642\n",
      "Epoch [4/5] Loss_D: 1.0656 Loss_G: 0.6506\n",
      "BATCH NUMBER 4643\n",
      "Epoch [4/5] Loss_D: 1.1608 Loss_G: 0.5874\n",
      "BATCH NUMBER 4644\n",
      "Epoch [4/5] Loss_D: 1.0640 Loss_G: 0.6418\n",
      "BATCH NUMBER 4645\n",
      "Epoch [4/5] Loss_D: 1.0386 Loss_G: 0.6648\n",
      "BATCH NUMBER 4646\n",
      "Epoch [4/5] Loss_D: 1.0523 Loss_G: 0.6519\n",
      "BATCH NUMBER 4647\n",
      "Epoch [4/5] Loss_D: 1.0293 Loss_G: 0.6719\n",
      "BATCH NUMBER 4648\n",
      "Epoch [4/5] Loss_D: 1.0535 Loss_G: 0.6509\n",
      "BATCH NUMBER 4649\n",
      "Epoch [4/5] Loss_D: 1.0556 Loss_G: 0.6601\n",
      "BATCH NUMBER 4650\n",
      "Epoch [4/5] Loss_D: 1.0392 Loss_G: 0.6627\n",
      "BATCH NUMBER 4651\n",
      "Epoch [4/5] Loss_D: 1.0279 Loss_G: 0.6734\n",
      "BATCH NUMBER 4652\n",
      "Epoch [4/5] Loss_D: 1.0622 Loss_G: 0.6472\n",
      "BATCH NUMBER 4653\n",
      "Epoch [4/5] Loss_D: 1.0214 Loss_G: 0.6794\n",
      "BATCH NUMBER 4654\n",
      "Epoch [4/5] Loss_D: 1.1436 Loss_G: 0.6029\n",
      "BATCH NUMBER 4655\n",
      "Epoch [4/5] Loss_D: 1.0251 Loss_G: 0.6766\n",
      "BATCH NUMBER 4656\n",
      "Epoch [4/5] Loss_D: 1.1125 Loss_G: 0.6203\n",
      "BATCH NUMBER 4657\n",
      "Epoch [4/5] Loss_D: 1.0778 Loss_G: 0.6397\n",
      "BATCH NUMBER 4658\n",
      "Epoch [4/5] Loss_D: 1.0217 Loss_G: 0.6790\n",
      "BATCH NUMBER 4659\n",
      "Epoch [4/5] Loss_D: 1.1570 Loss_G: 0.5927\n",
      "BATCH NUMBER 4660\n",
      "Epoch [4/5] Loss_D: 1.0415 Loss_G: 0.6626\n",
      "BATCH NUMBER 4661\n",
      "Epoch [4/5] Loss_D: 1.0752 Loss_G: 0.6425\n",
      "BATCH NUMBER 4662\n",
      "Epoch [4/5] Loss_D: 1.0252 Loss_G: 0.6757\n",
      "BATCH NUMBER 4663\n",
      "Epoch [4/5] Loss_D: 1.0448 Loss_G: 0.6590\n",
      "BATCH NUMBER 4664\n",
      "Epoch [4/5] Loss_D: 1.0813 Loss_G: 0.6368\n",
      "BATCH NUMBER 4665\n",
      "Epoch [4/5] Loss_D: 1.0777 Loss_G: 0.6397\n",
      "BATCH NUMBER 4666\n",
      "Epoch [4/5] Loss_D: 1.1428 Loss_G: 0.6036\n",
      "BATCH NUMBER 4667\n",
      "Epoch [4/5] Loss_D: 1.0589 Loss_G: 0.6570\n",
      "BATCH NUMBER 4668\n",
      "Epoch [4/5] Loss_D: 1.0674 Loss_G: 0.6486\n",
      "BATCH NUMBER 4669\n",
      "Epoch [4/5] Loss_D: 1.0261 Loss_G: 0.6756\n",
      "BATCH NUMBER 4670\n",
      "Epoch [4/5] Loss_D: 1.0721 Loss_G: 0.6348\n",
      "BATCH NUMBER 4671\n",
      "Epoch [4/5] Loss_D: 1.0538 Loss_G: 0.6521\n",
      "BATCH NUMBER 4672\n",
      "Epoch [4/5] Loss_D: 1.2350 Loss_G: 0.5431\n",
      "BATCH NUMBER 4673\n",
      "Epoch [4/5] Loss_D: 1.0508 Loss_G: 0.6640\n",
      "BATCH NUMBER 4674\n",
      "Epoch [4/5] Loss_D: 1.0637 Loss_G: 0.6548\n",
      "BATCH NUMBER 4675\n",
      "Epoch [4/5] Loss_D: 1.0428 Loss_G: 0.6606\n",
      "BATCH NUMBER 4676\n",
      "Epoch [4/5] Loss_D: 1.0101 Loss_G: 0.6895\n",
      "BATCH NUMBER 4677\n",
      "Epoch [4/5] Loss_D: 1.1814 Loss_G: 0.5800\n",
      "BATCH NUMBER 4678\n",
      "Epoch [4/5] Loss_D: 1.1233 Loss_G: 0.6115\n",
      "BATCH NUMBER 4679\n",
      "Epoch [4/5] Loss_D: 1.0319 Loss_G: 0.6716\n",
      "BATCH NUMBER 4680\n",
      "Epoch [4/5] Loss_D: 1.2232 Loss_G: 0.5536\n",
      "BATCH NUMBER 4681\n",
      "Epoch [4/5] Loss_D: 1.1181 Loss_G: 0.6161\n",
      "BATCH NUMBER 4682\n",
      "Epoch [4/5] Loss_D: 1.0313 Loss_G: 0.6706\n",
      "BATCH NUMBER 4683\n",
      "Epoch [4/5] Loss_D: 1.0725 Loss_G: 0.6439\n",
      "BATCH NUMBER 4684\n",
      "Epoch [4/5] Loss_D: 1.0332 Loss_G: 0.6691\n",
      "BATCH NUMBER 4685\n",
      "Epoch [4/5] Loss_D: 1.0310 Loss_G: 0.6720\n",
      "BATCH NUMBER 4686\n",
      "Epoch [4/5] Loss_D: 1.1127 Loss_G: 0.6206\n",
      "BATCH NUMBER 4687\n",
      "Epoch [4/5] Loss_D: 1.0943 Loss_G: 0.6247\n",
      "BATCH NUMBER 4688\n",
      "Epoch [4/5] Loss_D: 1.0959 Loss_G: 0.6265\n",
      "BATCH NUMBER 4689\n",
      "Epoch [4/5] Loss_D: 1.0627 Loss_G: 0.6441\n",
      "BATCH NUMBER 4690\n",
      "Epoch [4/5] Loss_D: 1.0927 Loss_G: 0.6273\n",
      "BATCH NUMBER 4691\n",
      "Epoch [4/5] Loss_D: 1.2293 Loss_G: 0.5488\n",
      "BATCH NUMBER 4692\n",
      "Epoch [4/5] Loss_D: 1.0458 Loss_G: 0.6577\n",
      "BATCH NUMBER 4693\n",
      "Epoch [4/5] Loss_D: 1.0796 Loss_G: 0.6400\n",
      "BATCH NUMBER 4694\n",
      "Epoch [4/5] Loss_D: 1.1366 Loss_G: 0.6094\n",
      "BATCH NUMBER 4695\n",
      "Epoch [4/5] Loss_D: 1.0285 Loss_G: 0.6735\n",
      "BATCH NUMBER 4696\n",
      "Epoch [4/5] Loss_D: 1.1888 Loss_G: 0.5732\n",
      "BATCH NUMBER 4697\n",
      "Epoch [4/5] Loss_D: 1.0715 Loss_G: 0.6451\n",
      "BATCH NUMBER 4698\n",
      "Epoch [4/5] Loss_D: 1.1094 Loss_G: 0.6243\n",
      "BATCH NUMBER 4699\n",
      "Epoch [4/5] Loss_D: 1.1439 Loss_G: 0.6027\n",
      "BATCH NUMBER 4700\n",
      "Epoch [4/5] Loss_D: 1.0973 Loss_G: 0.6356\n",
      "BATCH NUMBER 4701\n",
      "Epoch [4/5] Loss_D: 1.0478 Loss_G: 0.6577\n",
      "BATCH NUMBER 4702\n",
      "Epoch [4/5] Loss_D: 1.0222 Loss_G: 0.6784\n",
      "BATCH NUMBER 4703\n",
      "Epoch [4/5] Loss_D: 1.1622 Loss_G: 0.5870\n",
      "BATCH NUMBER 4704\n",
      "Epoch [4/5] Loss_D: 1.0421 Loss_G: 0.6617\n",
      "BATCH NUMBER 4705\n",
      "Epoch [4/5] Loss_D: 1.0737 Loss_G: 0.6434\n",
      "BATCH NUMBER 4706\n",
      "Epoch [4/5] Loss_D: 1.1755 Loss_G: 0.5749\n",
      "BATCH NUMBER 4707\n",
      "Epoch [4/5] Loss_D: 1.3125 Loss_G: 0.4976\n",
      "BATCH NUMBER 4708\n",
      "Epoch [4/5] Loss_D: 1.0423 Loss_G: 0.6631\n",
      "BATCH NUMBER 4709\n",
      "Epoch [4/5] Loss_D: 1.0531 Loss_G: 0.6524\n",
      "BATCH NUMBER 4710\n",
      "Epoch [4/5] Loss_D: 1.0811 Loss_G: 0.6368\n",
      "BATCH NUMBER 4711\n",
      "Epoch [4/5] Loss_D: 1.1422 Loss_G: 0.5940\n",
      "BATCH NUMBER 4712\n",
      "Epoch [4/5] Loss_D: 1.2250 Loss_G: 0.5522\n",
      "BATCH NUMBER 4713\n",
      "Epoch [4/5] Loss_D: 1.0966 Loss_G: 0.6341\n",
      "BATCH NUMBER 4714\n",
      "Epoch [4/5] Loss_D: 1.2034 Loss_G: 0.5619\n",
      "BATCH NUMBER 4715\n",
      "Epoch [4/5] Loss_D: 1.1542 Loss_G: 0.5931\n",
      "BATCH NUMBER 4716\n",
      "Epoch [4/5] Loss_D: 1.1260 Loss_G: 0.6186\n",
      "BATCH NUMBER 4717\n",
      "Epoch [4/5] Loss_D: 1.0324 Loss_G: 0.6697\n",
      "BATCH NUMBER 4718\n",
      "Epoch [4/5] Loss_D: 1.0908 Loss_G: 0.6290\n",
      "BATCH NUMBER 4719\n",
      "Epoch [4/5] Loss_D: 1.0275 Loss_G: 0.6739\n",
      "BATCH NUMBER 4720\n",
      "Epoch [4/5] Loss_D: 1.0859 Loss_G: 0.6330\n",
      "BATCH NUMBER 4721\n",
      "Epoch [4/5] Loss_D: 1.2262 Loss_G: 0.5504\n",
      "BATCH NUMBER 4722\n",
      "Epoch [4/5] Loss_D: 1.0843 Loss_G: 0.6343\n",
      "BATCH NUMBER 4723\n",
      "Epoch [4/5] Loss_D: 1.1085 Loss_G: 0.6235\n",
      "BATCH NUMBER 4724\n",
      "Epoch [4/5] Loss_D: 1.0287 Loss_G: 0.6732\n",
      "BATCH NUMBER 4725\n",
      "Epoch [4/5] Loss_D: 1.0505 Loss_G: 0.6540\n",
      "BATCH NUMBER 4726\n",
      "Epoch [4/5] Loss_D: 1.0527 Loss_G: 0.6520\n",
      "BATCH NUMBER 4727\n",
      "Epoch [4/5] Loss_D: 1.1417 Loss_G: 0.5952\n",
      "BATCH NUMBER 4728\n",
      "Epoch [4/5] Loss_D: 1.0965 Loss_G: 0.6240\n",
      "BATCH NUMBER 4729\n",
      "Epoch [4/5] Loss_D: 1.2178 Loss_G: 0.5513\n",
      "BATCH NUMBER 4730\n",
      "Epoch [4/5] Loss_D: 1.0389 Loss_G: 0.6646\n",
      "BATCH NUMBER 4731\n",
      "Epoch [4/5] Loss_D: 1.0395 Loss_G: 0.6621\n",
      "BATCH NUMBER 4732\n",
      "Epoch [4/5] Loss_D: 1.0980 Loss_G: 0.6325\n",
      "BATCH NUMBER 4733\n",
      "Epoch [4/5] Loss_D: 1.0218 Loss_G: 0.6792\n",
      "BATCH NUMBER 4734\n",
      "Epoch [4/5] Loss_D: 1.0843 Loss_G: 0.6340\n",
      "BATCH NUMBER 4735\n",
      "Epoch [4/5] Loss_D: 1.1084 Loss_G: 0.6146\n",
      "BATCH NUMBER 4736\n",
      "Epoch [4/5] Loss_D: 1.0307 Loss_G: 0.6706\n",
      "BATCH NUMBER 4737\n",
      "Epoch [4/5] Loss_D: 1.0396 Loss_G: 0.6631\n",
      "BATCH NUMBER 4738\n",
      "Epoch [4/5] Loss_D: 1.1053 Loss_G: 0.6260\n",
      "BATCH NUMBER 4739\n",
      "Epoch [4/5] Loss_D: 1.2117 Loss_G: 0.5540\n",
      "BATCH NUMBER 4740\n",
      "Epoch [4/5] Loss_D: 1.0641 Loss_G: 0.6520\n",
      "BATCH NUMBER 4741\n",
      "Epoch [4/5] Loss_D: 1.0427 Loss_G: 0.6609\n",
      "BATCH NUMBER 4742\n",
      "Epoch [4/5] Loss_D: 1.0392 Loss_G: 0.6637\n",
      "BATCH NUMBER 4743\n",
      "Epoch [4/5] Loss_D: 1.1617 Loss_G: 0.5979\n",
      "BATCH NUMBER 4744\n",
      "Epoch [4/5] Loss_D: 1.0566 Loss_G: 0.6481\n",
      "BATCH NUMBER 4745\n",
      "Epoch [4/5] Loss_D: 1.0309 Loss_G: 0.6711\n",
      "BATCH NUMBER 4746\n",
      "Epoch [4/5] Loss_D: 1.0303 Loss_G: 0.6711\n",
      "BATCH NUMBER 4747\n",
      "Epoch [4/5] Loss_D: 1.0314 Loss_G: 0.6702\n",
      "BATCH NUMBER 4748\n",
      "Epoch [4/5] Loss_D: 1.0785 Loss_G: 0.6388\n",
      "BATCH NUMBER 4749\n",
      "Epoch [4/5] Loss_D: 1.0400 Loss_G: 0.6639\n",
      "BATCH NUMBER 4750\n",
      "Epoch [4/5] Loss_D: 1.1703 Loss_G: 0.5808\n",
      "BATCH NUMBER 4751\n",
      "Epoch [4/5] Loss_D: 1.0203 Loss_G: 0.6800\n",
      "BATCH NUMBER 4752\n",
      "Epoch [4/5] Loss_D: 1.2569 Loss_G: 0.5348\n",
      "BATCH NUMBER 4753\n",
      "Epoch [4/5] Loss_D: 1.0913 Loss_G: 0.6300\n",
      "BATCH NUMBER 4754\n",
      "Epoch [4/5] Loss_D: 1.0336 Loss_G: 0.6685\n",
      "BATCH NUMBER 4755\n",
      "Epoch [4/5] Loss_D: 1.0434 Loss_G: 0.6590\n",
      "BATCH NUMBER 4756\n",
      "Epoch [4/5] Loss_D: 1.0864 Loss_G: 0.6335\n",
      "BATCH NUMBER 4757\n",
      "Epoch [4/5] Loss_D: 1.1328 Loss_G: 0.6033\n",
      "BATCH NUMBER 4758\n",
      "Epoch [4/5] Loss_D: 1.2055 Loss_G: 0.5582\n",
      "BATCH NUMBER 4759\n",
      "Epoch [4/5] Loss_D: 1.0837 Loss_G: 0.6252\n",
      "BATCH NUMBER 4760\n",
      "Epoch [4/5] Loss_D: 1.1465 Loss_G: 0.6001\n",
      "BATCH NUMBER 4761\n",
      "Epoch [4/5] Loss_D: 1.1649 Loss_G: 0.5840\n",
      "BATCH NUMBER 4762\n",
      "Epoch [4/5] Loss_D: 1.0947 Loss_G: 0.6252\n",
      "BATCH NUMBER 4763\n",
      "Epoch [4/5] Loss_D: 1.0243 Loss_G: 0.6773\n",
      "BATCH NUMBER 4764\n",
      "Epoch [4/5] Loss_D: 1.0669 Loss_G: 0.6492\n",
      "BATCH NUMBER 4765\n",
      "Epoch [4/5] Loss_D: 1.0872 Loss_G: 0.6343\n",
      "BATCH NUMBER 4766\n",
      "Epoch [4/5] Loss_D: 1.0693 Loss_G: 0.6494\n",
      "BATCH NUMBER 4767\n",
      "Epoch [4/5] Loss_D: 1.2658 Loss_G: 0.5266\n",
      "BATCH NUMBER 4768\n",
      "Epoch [4/5] Loss_D: 1.0643 Loss_G: 0.6519\n",
      "BATCH NUMBER 4769\n",
      "Epoch [4/5] Loss_D: 1.1557 Loss_G: 0.5941\n",
      "BATCH NUMBER 4770\n",
      "Epoch [4/5] Loss_D: 1.0999 Loss_G: 0.6312\n",
      "BATCH NUMBER 4771\n",
      "Epoch [4/5] Loss_D: 1.0600 Loss_G: 0.6455\n",
      "BATCH NUMBER 4772\n",
      "Epoch [4/5] Loss_D: 1.1135 Loss_G: 0.6198\n",
      "BATCH NUMBER 4773\n",
      "Epoch [4/5] Loss_D: 1.0706 Loss_G: 0.6463\n",
      "BATCH NUMBER 4774\n",
      "Epoch [4/5] Loss_D: 1.0322 Loss_G: 0.6695\n",
      "BATCH NUMBER 4775\n",
      "Epoch [4/5] Loss_D: 1.1419 Loss_G: 0.6045\n",
      "BATCH NUMBER 4776\n",
      "Epoch [4/5] Loss_D: 1.0525 Loss_G: 0.6520\n",
      "BATCH NUMBER 4777\n",
      "Epoch [4/5] Loss_D: 1.1021 Loss_G: 0.6292\n",
      "BATCH NUMBER 4778\n",
      "Epoch [4/5] Loss_D: 1.1307 Loss_G: 0.6056\n",
      "BATCH NUMBER 4779\n",
      "Epoch [4/5] Loss_D: 1.1514 Loss_G: 0.5955\n",
      "BATCH NUMBER 4780\n",
      "Epoch [4/5] Loss_D: 1.0430 Loss_G: 0.6612\n",
      "BATCH NUMBER 4781\n",
      "Epoch [4/5] Loss_D: 1.0722 Loss_G: 0.6444\n",
      "BATCH NUMBER 4782\n",
      "Epoch [4/5] Loss_D: 1.1354 Loss_G: 0.6100\n",
      "BATCH NUMBER 4783\n",
      "Epoch [4/5] Loss_D: 1.0918 Loss_G: 0.6286\n",
      "BATCH NUMBER 4784\n",
      "Epoch [4/5] Loss_D: 1.0579 Loss_G: 0.6579\n",
      "BATCH NUMBER 4785\n",
      "Epoch [4/5] Loss_D: 1.0570 Loss_G: 0.6581\n",
      "BATCH NUMBER 4786\n",
      "Epoch [4/5] Loss_D: 1.2029 Loss_G: 0.5618\n",
      "BATCH NUMBER 4787\n",
      "Epoch [4/5] Loss_D: 1.0313 Loss_G: 0.6709\n",
      "BATCH NUMBER 4788\n",
      "Epoch [4/5] Loss_D: 1.1485 Loss_G: 0.5980\n",
      "BATCH NUMBER 4789\n",
      "Epoch [4/5] Loss_D: 1.1624 Loss_G: 0.5856\n",
      "BATCH NUMBER 4790\n",
      "Epoch [4/5] Loss_D: 1.0246 Loss_G: 0.6770\n",
      "BATCH NUMBER 4791\n",
      "Epoch [4/5] Loss_D: 1.1647 Loss_G: 0.5865\n",
      "BATCH NUMBER 4792\n",
      "Epoch [4/5] Loss_D: 1.0176 Loss_G: 0.6829\n",
      "BATCH NUMBER 4793\n",
      "Epoch [4/5] Loss_D: 1.0728 Loss_G: 0.6452\n",
      "BATCH NUMBER 4794\n",
      "Epoch [4/5] Loss_D: 1.0427 Loss_G: 0.6605\n",
      "BATCH NUMBER 4795\n",
      "Epoch [4/5] Loss_D: 1.0274 Loss_G: 0.6747\n",
      "BATCH NUMBER 4796\n",
      "Epoch [4/5] Loss_D: 1.0356 Loss_G: 0.6665\n",
      "BATCH NUMBER 4797\n",
      "Epoch [4/5] Loss_D: 1.0195 Loss_G: 0.6809\n",
      "BATCH NUMBER 4798\n",
      "Epoch [4/5] Loss_D: 1.0819 Loss_G: 0.6382\n",
      "BATCH NUMBER 4799\n",
      "Epoch [4/5] Loss_D: 1.1059 Loss_G: 0.6253\n",
      "BATCH NUMBER 4800\n",
      "Epoch [4/5] Loss_D: 1.3683 Loss_G: 0.4672\n",
      "BATCH NUMBER 4801\n",
      "Epoch [4/5] Loss_D: 1.0505 Loss_G: 0.6541\n",
      "BATCH NUMBER 4802\n",
      "Epoch [4/5] Loss_D: 1.0797 Loss_G: 0.6383\n",
      "BATCH NUMBER 4803\n",
      "Epoch [4/5] Loss_D: 1.0518 Loss_G: 0.6631\n",
      "BATCH NUMBER 4804\n",
      "Epoch [4/5] Loss_D: 1.0352 Loss_G: 0.6689\n",
      "BATCH NUMBER 4805\n",
      "Epoch [4/5] Loss_D: 1.0378 Loss_G: 0.6647\n",
      "BATCH NUMBER 4806\n",
      "Epoch [4/5] Loss_D: 1.0631 Loss_G: 0.6526\n",
      "BATCH NUMBER 4807\n",
      "Epoch [4/5] Loss_D: 1.0857 Loss_G: 0.6337\n",
      "BATCH NUMBER 4808\n",
      "Epoch [4/5] Loss_D: 1.1103 Loss_G: 0.6217\n",
      "BATCH NUMBER 4809\n",
      "Epoch [4/5] Loss_D: 1.0305 Loss_G: 0.6718\n",
      "BATCH NUMBER 4810\n",
      "Epoch [4/5] Loss_D: 1.0360 Loss_G: 0.6658\n",
      "BATCH NUMBER 4811\n",
      "Epoch [4/5] Loss_D: 1.0401 Loss_G: 0.6627\n",
      "BATCH NUMBER 4812\n",
      "Epoch [4/5] Loss_D: 1.2714 Loss_G: 0.5222\n",
      "BATCH NUMBER 4813\n",
      "Epoch [4/5] Loss_D: 1.0602 Loss_G: 0.6559\n",
      "BATCH NUMBER 4814\n",
      "Epoch [4/5] Loss_D: 1.0935 Loss_G: 0.6364\n",
      "BATCH NUMBER 4815\n",
      "Epoch [4/5] Loss_D: 1.0202 Loss_G: 0.6802\n",
      "BATCH NUMBER 4816\n",
      "Epoch [4/5] Loss_D: 1.0636 Loss_G: 0.6424\n",
      "BATCH NUMBER 4817\n",
      "Epoch [4/5] Loss_D: 1.0371 Loss_G: 0.6660\n",
      "BATCH NUMBER 4818\n",
      "Epoch [4/5] Loss_D: 1.0522 Loss_G: 0.6522\n",
      "BATCH NUMBER 4819\n",
      "Epoch [4/5] Loss_D: 1.0694 Loss_G: 0.6485\n",
      "BATCH NUMBER 4820\n",
      "Epoch [4/5] Loss_D: 1.0962 Loss_G: 0.6342\n",
      "BATCH NUMBER 4821\n",
      "Epoch [4/5] Loss_D: 1.1642 Loss_G: 0.5859\n",
      "BATCH NUMBER 4822\n",
      "Epoch [4/5] Loss_D: 1.0186 Loss_G: 0.6819\n",
      "BATCH NUMBER 4823\n",
      "Epoch [4/5] Loss_D: 1.1697 Loss_G: 0.5903\n",
      "BATCH NUMBER 4824\n",
      "Epoch [4/5] Loss_D: 1.1239 Loss_G: 0.6101\n",
      "BATCH NUMBER 4825\n",
      "Epoch [4/5] Loss_D: 1.0657 Loss_G: 0.6404\n",
      "BATCH NUMBER 4826\n",
      "Epoch [4/5] Loss_D: 1.0249 Loss_G: 0.6766\n",
      "BATCH NUMBER 4827\n",
      "Epoch [4/5] Loss_D: 1.1881 Loss_G: 0.5748\n",
      "BATCH NUMBER 4828\n",
      "Epoch [4/5] Loss_D: 1.2760 Loss_G: 0.5197\n",
      "BATCH NUMBER 4829\n",
      "Epoch [4/5] Loss_D: 1.0341 Loss_G: 0.6699\n",
      "BATCH NUMBER 4830\n",
      "Epoch [4/5] Loss_D: 1.2760 Loss_G: 0.5172\n",
      "BATCH NUMBER 4831\n",
      "Epoch [4/5] Loss_D: 1.1552 Loss_G: 0.5925\n",
      "BATCH NUMBER 4832\n",
      "Epoch [4/5] Loss_D: 1.0655 Loss_G: 0.6505\n",
      "BATCH NUMBER 4833\n",
      "Epoch [4/5] Loss_D: 1.0628 Loss_G: 0.6532\n",
      "BATCH NUMBER 4834\n",
      "Epoch [4/5] Loss_D: 1.0995 Loss_G: 0.6211\n",
      "BATCH NUMBER 4835\n",
      "Epoch [4/5] Loss_D: 1.0432 Loss_G: 0.6595\n",
      "BATCH NUMBER 4836\n",
      "Epoch [4/5] Loss_D: 1.0453 Loss_G: 0.6592\n",
      "BATCH NUMBER 4837\n",
      "Epoch [4/5] Loss_D: 1.1901 Loss_G: 0.5634\n",
      "BATCH NUMBER 4838\n",
      "Epoch [4/5] Loss_D: 1.1647 Loss_G: 0.5984\n",
      "BATCH NUMBER 4839\n",
      "Epoch [4/5] Loss_D: 1.0290 Loss_G: 0.6729\n",
      "BATCH NUMBER 4840\n",
      "Epoch [4/5] Loss_D: 1.0727 Loss_G: 0.6440\n",
      "BATCH NUMBER 4841\n",
      "Epoch [4/5] Loss_D: 1.1804 Loss_G: 0.5804\n",
      "BATCH NUMBER 4842\n",
      "Epoch [4/5] Loss_D: 1.0293 Loss_G: 0.6725\n",
      "BATCH NUMBER 4843\n",
      "Epoch [4/5] Loss_D: 1.0276 Loss_G: 0.6740\n",
      "BATCH NUMBER 4844\n",
      "Epoch [4/5] Loss_D: 1.0359 Loss_G: 0.6666\n",
      "BATCH NUMBER 4845\n",
      "Epoch [4/5] Loss_D: 1.2133 Loss_G: 0.5620\n",
      "BATCH NUMBER 4846\n",
      "Epoch [4/5] Loss_D: 1.0161 Loss_G: 0.6840\n",
      "BATCH NUMBER 4847\n",
      "Epoch [4/5] Loss_D: 1.2280 Loss_G: 0.5494\n",
      "BATCH NUMBER 4848\n",
      "Epoch [4/5] Loss_D: 1.1574 Loss_G: 0.5911\n",
      "BATCH NUMBER 4849\n",
      "Epoch [4/5] Loss_D: 1.0688 Loss_G: 0.6484\n",
      "BATCH NUMBER 4850\n",
      "Epoch [4/5] Loss_D: 1.0224 Loss_G: 0.6782\n",
      "BATCH NUMBER 4851\n",
      "Epoch [4/5] Loss_D: 1.0808 Loss_G: 0.6379\n",
      "BATCH NUMBER 4852\n",
      "Epoch [4/5] Loss_D: 1.1207 Loss_G: 0.6118\n",
      "BATCH NUMBER 4853\n",
      "Epoch [4/5] Loss_D: 1.0640 Loss_G: 0.6427\n",
      "BATCH NUMBER 4854\n",
      "Epoch [4/5] Loss_D: 1.0289 Loss_G: 0.6729\n",
      "BATCH NUMBER 4855\n",
      "Epoch [4/5] Loss_D: 1.0933 Loss_G: 0.6270\n",
      "BATCH NUMBER 4856\n",
      "Epoch [4/5] Loss_D: 1.1947 Loss_G: 0.5693\n",
      "BATCH NUMBER 4857\n",
      "Epoch [4/5] Loss_D: 1.1304 Loss_G: 0.6147\n",
      "BATCH NUMBER 4858\n",
      "Epoch [4/5] Loss_D: 1.0654 Loss_G: 0.6510\n",
      "BATCH NUMBER 4859\n",
      "Epoch [4/5] Loss_D: 1.0670 Loss_G: 0.6494\n",
      "BATCH NUMBER 4860\n",
      "Epoch [4/5] Loss_D: 1.1203 Loss_G: 0.6139\n",
      "BATCH NUMBER 4861\n",
      "Epoch [4/5] Loss_D: 1.0273 Loss_G: 0.6750\n",
      "BATCH NUMBER 4862\n",
      "Epoch [4/5] Loss_D: 1.1062 Loss_G: 0.6256\n",
      "BATCH NUMBER 4863\n",
      "Epoch [4/5] Loss_D: 1.2539 Loss_G: 0.5370\n",
      "BATCH NUMBER 4864\n",
      "Epoch [4/5] Loss_D: 1.0299 Loss_G: 0.6717\n",
      "BATCH NUMBER 4865\n",
      "Epoch [4/5] Loss_D: 1.3256 Loss_G: 0.4945\n",
      "BATCH NUMBER 4866\n",
      "Epoch [4/5] Loss_D: 1.0572 Loss_G: 0.6491\n",
      "BATCH NUMBER 4867\n",
      "Epoch [4/5] Loss_D: 1.1059 Loss_G: 0.6256\n",
      "BATCH NUMBER 4868\n",
      "Epoch [4/5] Loss_D: 1.1513 Loss_G: 0.5957\n",
      "BATCH NUMBER 4869\n",
      "Epoch [4/5] Loss_D: 1.0973 Loss_G: 0.6331\n",
      "BATCH NUMBER 4870\n",
      "Epoch [4/5] Loss_D: 1.2739 Loss_G: 0.5206\n",
      "BATCH NUMBER 4871\n",
      "Epoch [4/5] Loss_D: 1.0232 Loss_G: 0.6773\n",
      "BATCH NUMBER 4872\n",
      "Epoch [4/5] Loss_D: 1.0944 Loss_G: 0.6267\n",
      "BATCH NUMBER 4873\n",
      "Epoch [4/5] Loss_D: 1.0426 Loss_G: 0.6604\n",
      "BATCH NUMBER 4874\n",
      "Epoch [4/5] Loss_D: 1.1199 Loss_G: 0.6146\n",
      "BATCH NUMBER 4875\n",
      "Epoch [4/5] Loss_D: 1.1732 Loss_G: 0.5870\n",
      "BATCH NUMBER 4876\n",
      "Epoch [4/5] Loss_D: 1.1269 Loss_G: 0.6066\n",
      "BATCH NUMBER 4877\n",
      "Epoch [4/5] Loss_D: 1.1349 Loss_G: 0.6025\n",
      "BATCH NUMBER 4878\n",
      "Epoch [4/5] Loss_D: 1.1123 Loss_G: 0.6198\n",
      "BATCH NUMBER 4879\n",
      "Epoch [4/5] Loss_D: 1.0595 Loss_G: 0.6559\n",
      "BATCH NUMBER 4880\n",
      "Epoch [4/5] Loss_D: 1.0435 Loss_G: 0.6612\n",
      "BATCH NUMBER 4881\n",
      "Epoch [4/5] Loss_D: 1.0675 Loss_G: 0.6495\n",
      "BATCH NUMBER 4882\n",
      "Epoch [4/5] Loss_D: 1.0655 Loss_G: 0.6507\n",
      "BATCH NUMBER 4883\n",
      "Epoch [4/5] Loss_D: 1.0230 Loss_G: 0.6800\n",
      "BATCH NUMBER 4884\n",
      "Epoch [4/5] Loss_D: 1.0688 Loss_G: 0.6486\n",
      "BATCH NUMBER 4885\n",
      "Epoch [4/5] Loss_D: 1.0368 Loss_G: 0.6675\n",
      "BATCH NUMBER 4886\n",
      "Epoch [4/5] Loss_D: 1.1400 Loss_G: 0.6062\n",
      "BATCH NUMBER 4887\n",
      "Epoch [4/5] Loss_D: 1.1408 Loss_G: 0.6055\n",
      "BATCH NUMBER 4888\n",
      "Epoch [4/5] Loss_D: 1.1231 Loss_G: 0.6110\n",
      "BATCH NUMBER 4889\n",
      "Epoch [4/5] Loss_D: 1.0835 Loss_G: 0.6343\n",
      "BATCH NUMBER 4890\n",
      "Epoch [4/5] Loss_D: 1.1043 Loss_G: 0.6177\n",
      "BATCH NUMBER 4891\n",
      "Epoch [4/5] Loss_D: 1.0641 Loss_G: 0.6523\n",
      "BATCH NUMBER 4892\n",
      "Epoch [4/5] Loss_D: 1.1525 Loss_G: 0.5952\n",
      "BATCH NUMBER 4893\n",
      "Epoch [4/5] Loss_D: 1.0278 Loss_G: 0.6745\n",
      "BATCH NUMBER 4894\n",
      "Epoch [4/5] Loss_D: 1.0689 Loss_G: 0.6478\n",
      "BATCH NUMBER 4895\n",
      "Epoch [4/5] Loss_D: 1.1140 Loss_G: 0.6089\n",
      "BATCH NUMBER 4896\n",
      "Epoch [4/5] Loss_D: 1.2218 Loss_G: 0.5553\n",
      "BATCH NUMBER 4897\n",
      "Epoch [4/5] Loss_D: 1.1200 Loss_G: 0.6125\n",
      "BATCH NUMBER 4898\n",
      "Epoch [4/5] Loss_D: 1.1343 Loss_G: 0.6111\n",
      "BATCH NUMBER 4899\n",
      "Epoch [4/5] Loss_D: 1.0768 Loss_G: 0.6414\n",
      "BATCH NUMBER 4900\n",
      "Epoch [4/5] Loss_D: 1.0507 Loss_G: 0.6568\n",
      "BATCH NUMBER 4901\n",
      "Epoch [4/5] Loss_D: 1.0203 Loss_G: 0.6805\n",
      "BATCH NUMBER 4902\n",
      "Epoch [4/5] Loss_D: 1.1197 Loss_G: 0.6176\n",
      "BATCH NUMBER 4903\n",
      "Epoch [4/5] Loss_D: 1.0483 Loss_G: 0.6566\n",
      "BATCH NUMBER 4904\n",
      "Epoch [4/5] Loss_D: 1.1025 Loss_G: 0.6187\n",
      "BATCH NUMBER 4905\n",
      "Epoch [4/5] Loss_D: 1.0473 Loss_G: 0.6563\n",
      "BATCH NUMBER 4906\n",
      "Epoch [4/5] Loss_D: 1.0213 Loss_G: 0.6797\n",
      "BATCH NUMBER 4907\n",
      "Epoch [4/5] Loss_D: 1.0746 Loss_G: 0.6427\n",
      "BATCH NUMBER 4908\n",
      "Epoch [4/5] Loss_D: 1.1139 Loss_G: 0.6186\n",
      "BATCH NUMBER 4909\n",
      "Epoch [4/5] Loss_D: 1.0350 Loss_G: 0.6672\n",
      "BATCH NUMBER 4910\n",
      "Epoch [4/5] Loss_D: 1.0565 Loss_G: 0.6587\n",
      "BATCH NUMBER 4911\n",
      "Epoch [4/5] Loss_D: 1.3389 Loss_G: 0.4840\n",
      "BATCH NUMBER 4912\n",
      "Epoch [4/5] Loss_D: 1.0437 Loss_G: 0.6634\n",
      "BATCH NUMBER 4913\n",
      "Epoch [4/5] Loss_D: 1.0916 Loss_G: 0.6386\n",
      "BATCH NUMBER 4914\n",
      "Epoch [4/5] Loss_D: 1.2074 Loss_G: 0.5562\n",
      "BATCH NUMBER 4915\n",
      "Epoch [4/5] Loss_D: 1.0325 Loss_G: 0.6691\n",
      "BATCH NUMBER 4916\n",
      "Epoch [4/5] Loss_D: 1.0337 Loss_G: 0.6690\n",
      "BATCH NUMBER 4917\n",
      "Epoch [4/5] Loss_D: 1.0756 Loss_G: 0.6419\n",
      "BATCH NUMBER 4918\n",
      "Epoch [4/5] Loss_D: 1.1448 Loss_G: 0.6019\n",
      "BATCH NUMBER 4919\n",
      "Epoch [4/5] Loss_D: 1.0767 Loss_G: 0.6410\n",
      "BATCH NUMBER 4920\n",
      "Epoch [4/5] Loss_D: 1.1095 Loss_G: 0.6222\n",
      "BATCH NUMBER 4921\n",
      "Epoch [4/5] Loss_D: 1.1899 Loss_G: 0.5740\n",
      "BATCH NUMBER 4922\n",
      "Epoch [4/5] Loss_D: 1.2044 Loss_G: 0.5704\n",
      "BATCH NUMBER 4923\n",
      "Epoch [4/5] Loss_D: 1.0327 Loss_G: 0.6695\n",
      "BATCH NUMBER 4924\n",
      "Epoch [4/5] Loss_D: 1.1443 Loss_G: 0.5932\n",
      "BATCH NUMBER 4925\n",
      "Epoch [4/5] Loss_D: 1.2139 Loss_G: 0.5617\n",
      "BATCH NUMBER 4926\n",
      "Epoch [4/5] Loss_D: 1.0839 Loss_G: 0.6345\n",
      "BATCH NUMBER 4927\n",
      "Epoch [4/5] Loss_D: 1.0256 Loss_G: 0.6764\n",
      "BATCH NUMBER 4928\n",
      "Epoch [4/5] Loss_D: 1.0608 Loss_G: 0.6550\n",
      "BATCH NUMBER 4929\n",
      "Epoch [4/5] Loss_D: 1.2260 Loss_G: 0.5517\n",
      "BATCH NUMBER 4930\n",
      "Epoch [4/5] Loss_D: 1.0266 Loss_G: 0.6750\n",
      "BATCH NUMBER 4931\n",
      "Epoch [4/5] Loss_D: 1.2485 Loss_G: 0.5417\n",
      "BATCH NUMBER 4932\n",
      "Epoch [4/5] Loss_D: 1.1072 Loss_G: 0.6138\n",
      "BATCH NUMBER 4933\n",
      "Epoch [4/5] Loss_D: 1.0288 Loss_G: 0.6727\n",
      "BATCH NUMBER 4934\n",
      "Epoch [4/5] Loss_D: 1.1870 Loss_G: 0.5743\n",
      "BATCH NUMBER 4935\n",
      "Epoch [4/5] Loss_D: 1.1486 Loss_G: 0.5988\n",
      "BATCH NUMBER 4936\n",
      "Epoch [4/5] Loss_D: 1.0407 Loss_G: 0.6622\n",
      "BATCH NUMBER 4937\n",
      "Epoch [4/5] Loss_D: 1.0733 Loss_G: 0.6453\n",
      "BATCH NUMBER 4938\n",
      "Epoch [4/5] Loss_D: 1.2994 Loss_G: 0.5073\n",
      "BATCH NUMBER 4939\n",
      "Epoch [4/5] Loss_D: 1.0327 Loss_G: 0.6694\n",
      "BATCH NUMBER 4940\n",
      "Epoch [4/5] Loss_D: 1.0293 Loss_G: 0.6719\n",
      "BATCH NUMBER 4941\n",
      "Epoch [4/5] Loss_D: 1.0810 Loss_G: 0.6373\n",
      "BATCH NUMBER 4942\n",
      "Epoch [4/5] Loss_D: 1.0438 Loss_G: 0.6599\n",
      "BATCH NUMBER 4943\n",
      "Epoch [4/5] Loss_D: 1.1047 Loss_G: 0.6273\n",
      "BATCH NUMBER 4944\n",
      "Epoch [4/5] Loss_D: 1.1397 Loss_G: 0.6066\n",
      "BATCH NUMBER 4945\n",
      "Epoch [4/5] Loss_D: 1.0511 Loss_G: 0.6540\n",
      "BATCH NUMBER 4946\n",
      "Epoch [4/5] Loss_D: 1.1179 Loss_G: 0.6155\n",
      "BATCH NUMBER 4947\n",
      "Epoch [4/5] Loss_D: 1.1027 Loss_G: 0.6282\n",
      "BATCH NUMBER 4948\n",
      "Epoch [4/5] Loss_D: 1.3418 Loss_G: 0.4804\n",
      "BATCH NUMBER 4949\n",
      "Epoch [4/5] Loss_D: 1.1813 Loss_G: 0.5798\n",
      "BATCH NUMBER 4950\n",
      "Epoch [4/5] Loss_D: 1.0511 Loss_G: 0.6531\n",
      "BATCH NUMBER 4951\n",
      "Epoch [4/5] Loss_D: 1.0885 Loss_G: 0.6412\n",
      "BATCH NUMBER 4952\n",
      "Epoch [4/5] Loss_D: 1.0317 Loss_G: 0.6694\n",
      "BATCH NUMBER 4953\n",
      "Epoch [4/5] Loss_D: 1.0392 Loss_G: 0.6634\n",
      "BATCH NUMBER 4954\n",
      "Epoch [4/5] Loss_D: 1.0623 Loss_G: 0.6531\n",
      "BATCH NUMBER 4955\n",
      "Epoch [4/5] Loss_D: 1.1069 Loss_G: 0.6243\n",
      "BATCH NUMBER 4956\n",
      "Epoch [4/5] Loss_D: 1.1499 Loss_G: 0.5977\n",
      "BATCH NUMBER 4957\n",
      "Epoch [4/5] Loss_D: 1.0611 Loss_G: 0.6551\n",
      "BATCH NUMBER 4958\n",
      "Epoch [4/5] Loss_D: 1.0754 Loss_G: 0.6417\n",
      "BATCH NUMBER 4959\n",
      "Epoch [4/5] Loss_D: 1.0266 Loss_G: 0.6746\n",
      "BATCH NUMBER 4960\n",
      "Epoch [4/5] Loss_D: 1.0341 Loss_G: 0.6691\n",
      "BATCH NUMBER 4961\n",
      "Epoch [4/5] Loss_D: 1.0672 Loss_G: 0.6487\n",
      "BATCH NUMBER 4962\n",
      "Epoch [4/5] Loss_D: 1.2015 Loss_G: 0.5628\n",
      "BATCH NUMBER 4963\n",
      "Epoch [4/5] Loss_D: 1.0983 Loss_G: 0.6320\n",
      "BATCH NUMBER 4964\n",
      "Epoch [4/5] Loss_D: 1.0680 Loss_G: 0.6488\n",
      "BATCH NUMBER 4965\n",
      "Epoch [4/5] Loss_D: 1.0990 Loss_G: 0.6315\n",
      "BATCH NUMBER 4966\n",
      "Epoch [4/5] Loss_D: 1.0183 Loss_G: 0.6821\n",
      "BATCH NUMBER 4967\n",
      "Epoch [4/5] Loss_D: 1.1888 Loss_G: 0.5738\n",
      "BATCH NUMBER 4968\n",
      "Epoch [4/5] Loss_D: 1.1038 Loss_G: 0.6274\n",
      "BATCH NUMBER 4969\n",
      "Epoch [4/5] Loss_D: 1.0493 Loss_G: 0.6654\n",
      "BATCH NUMBER 4970\n",
      "Epoch [4/5] Loss_D: 1.1870 Loss_G: 0.5674\n",
      "BATCH NUMBER 4971\n",
      "Epoch [4/5] Loss_D: 1.0240 Loss_G: 0.6774\n",
      "BATCH NUMBER 4972\n",
      "Epoch [4/5] Loss_D: 1.2710 Loss_G: 0.5242\n",
      "BATCH NUMBER 4973\n",
      "Epoch [4/5] Loss_D: 1.0179 Loss_G: 0.6830\n",
      "BATCH NUMBER 4974\n",
      "Epoch [4/5] Loss_D: 1.0099 Loss_G: 0.6898\n",
      "BATCH NUMBER 4975\n",
      "Epoch [4/5] Loss_D: 1.0735 Loss_G: 0.6444\n",
      "BATCH NUMBER 4976\n",
      "Epoch [4/5] Loss_D: 1.1582 Loss_G: 0.5907\n",
      "BATCH NUMBER 4977\n",
      "Epoch [4/5] Loss_D: 1.1365 Loss_G: 0.6095\n",
      "BATCH NUMBER 4978\n",
      "Epoch [4/5] Loss_D: 1.1014 Loss_G: 0.6293\n",
      "BATCH NUMBER 4979\n",
      "Epoch [4/5] Loss_D: 1.1049 Loss_G: 0.6263\n",
      "BATCH NUMBER 4980\n",
      "Epoch [4/5] Loss_D: 1.1033 Loss_G: 0.6176\n",
      "BATCH NUMBER 4981\n",
      "Epoch [4/5] Loss_D: 1.0223 Loss_G: 0.6787\n",
      "BATCH NUMBER 4982\n",
      "Epoch [4/5] Loss_D: 1.1287 Loss_G: 0.6072\n",
      "BATCH NUMBER 4983\n",
      "Epoch [4/5] Loss_D: 1.0700 Loss_G: 0.6472\n",
      "BATCH NUMBER 4984\n",
      "Epoch [4/5] Loss_D: 1.0540 Loss_G: 0.6501\n",
      "BATCH NUMBER 4985\n",
      "Epoch [4/5] Loss_D: 1.1504 Loss_G: 0.5967\n",
      "BATCH NUMBER 4986\n",
      "Epoch [4/5] Loss_D: 1.0359 Loss_G: 0.6675\n",
      "BATCH NUMBER 4987\n",
      "Epoch [4/5] Loss_D: 1.0482 Loss_G: 0.6561\n",
      "BATCH NUMBER 4988\n",
      "Epoch [4/5] Loss_D: 1.0397 Loss_G: 0.6640\n",
      "BATCH NUMBER 4989\n",
      "Epoch [4/5] Loss_D: 1.0169 Loss_G: 0.6835\n",
      "BATCH NUMBER 4990\n",
      "Epoch [4/5] Loss_D: 1.0851 Loss_G: 0.6342\n",
      "BATCH NUMBER 4991\n",
      "Epoch [4/5] Loss_D: 1.0821 Loss_G: 0.6375\n",
      "BATCH NUMBER 4992\n",
      "Epoch [4/5] Loss_D: 1.1874 Loss_G: 0.5754\n",
      "BATCH NUMBER 4993\n",
      "Epoch [4/5] Loss_D: 1.0558 Loss_G: 0.6556\n",
      "BATCH NUMBER 4994\n",
      "Epoch [4/5] Loss_D: 1.0478 Loss_G: 0.6574\n",
      "BATCH NUMBER 4995\n",
      "Epoch [4/5] Loss_D: 1.1992 Loss_G: 0.5644\n",
      "BATCH NUMBER 4996\n",
      "Epoch [4/5] Loss_D: 1.0425 Loss_G: 0.6618\n",
      "BATCH NUMBER 4997\n",
      "Epoch [4/5] Loss_D: 1.1163 Loss_G: 0.6165\n",
      "BATCH NUMBER 4998\n",
      "Epoch [4/5] Loss_D: 1.0252 Loss_G: 0.6765\n",
      "BATCH NUMBER 4999\n",
      "Epoch [4/5] Loss_D: 1.2244 Loss_G: 0.5528\n",
      "BATCH NUMBER 5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# For more accurate error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "prompt = 'Generate one positive tweet and one negative tweet on a very specific topic. Ensure that each tweet is enclosed in straight quotation marks (\"\"). The positive tweet should express enthusiasm or praise, and the negative tweet should convey criticism or disappointment. Your comments should include specific features, aspects, or things that are praised/critisized. Make sure to include a space after the colon. \\nPositive: \"'\n",
    "batches = 1\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch in data_loader:\n",
    "        \n",
    "        # Convert the input batch into a PyTorch tensor and move to GPU\n",
    "        padded_inputs = tokenizer(batch['text'], padding=True, return_tensors=\"pt\", truncation=True)\n",
    "        real_text = padded_inputs['input_ids'].cuda()\n",
    "\n",
    "        # Generate fake text logits using noise input\n",
    "        noise = torch.randint(0, len(tokenizer), (1, 16)).cuda()\n",
    "        outputs = generator(noise)\n",
    "        fake_logits = outputs.logits.argmax(dim=-1).detach().cuda()  # Detach to avoid backpropagation\n",
    "        fake_logits_raw = outputs.logits.cuda()  # Raw logits before argmax for shape consistency\n",
    "        \n",
    "        # Reset gradients for discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Create real labels tensor and move to GPU\n",
    "        real_labels = torch.ones((real_text.size(0), 1), dtype=torch.float).cuda()\n",
    "\n",
    "        # Get discriminator's prediction on real text and calculate loss\n",
    "        real_output = discriminator(real_text)\n",
    "        lossD_real = criterion(real_output.view(-1, 1), real_labels)\n",
    "        \n",
    "        # Reshape fake logits to ensure correct shape\n",
    "        fake_logits = fake_logits.view(-1, 1).cuda()\n",
    "\n",
    "        # Get discriminator's prediction on fake text and calculate loss\n",
    "        fake_output = discriminator(fake_logits)\n",
    "        \n",
    "        # Create fake labels tensor and move to GPU\n",
    "        fake_labels = torch.zeros((fake_output.size(0), 1), dtype=torch.float).cuda()\n",
    "        \n",
    "        lossD_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Combine real and fake losses for the discriminator\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        lossD.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizerD.step()\n",
    "\n",
    "        # Reset gradients for generator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Generate new fake text logits\n",
    "        with torch.no_grad():\n",
    "            outputs = generator(noise)\n",
    "        fake_logits = outputs.logits.argmax(dim=-1).cuda()\n",
    "\n",
    "        # Get discriminator's assessment of the newly generated fake data\n",
    "        fake_output = discriminator(fake_logits.view(-1, 1).cuda())\n",
    "\n",
    "        real_labels = torch.ones((fake_output.size(0), 1), dtype=torch.float).cuda()\n",
    "\n",
    "        # Calculate the generator's loss\n",
    "\n",
    "        lossG = criterion(fake_output.view(-1, 1), real_labels)\n",
    "        lossG.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizerG.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch}/{epochs}] Loss_D: {lossD.item():.4f} Loss_G: {lossG.item():.4f}')\n",
    "        print(\"BATCH NUMBER \" + str(batches))\n",
    "        batches+=1\n",
    "        del padded_inputs, real_text, noise, outputs, fake_logits, fake_logits_raw, real_labels, real_output, fake_labels, fake_output\n",
    "        # gc.collect(generation=2)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fb08d-299c-4802-9c96-d525ad7c06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d140bb82-c643-4faa-b7b9-e88b139fcad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epoch [4/5] Loss_D: 1.2244 Loss_G: 0.5528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5abb1f-325f-44ed-b487-2d74a3d2a290",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1005\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1003\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1004\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m   1007\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1008\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1009\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:457\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a09bec7a-6886-4c99-a460-1516942a541f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c98b91-3f32-4243-aa81-1b5c4c2217f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: !\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "noise_dim = 32  # replace with the appropriate noise dimension used during training\n",
    "noise = torch.randint(0, len(tokenizer), (batch_size, noise_dim)).to('cuda')\n",
    "\n",
    "# Run the generator to generate new data\n",
    "with torch.no_grad():  # no gradient calculation needed during inference\n",
    "    outputs = generator(noise)\n",
    "\n",
    "# Post-process the output if necessary\n",
    "generated_text_logits = outputs.logits\n",
    "generated_text_ids = generated_text_logits.argmax(dim=-1)\n",
    "\n",
    "# Convert generated token IDs to text using the tokenizer\n",
    "generated_text = tokenizer.batch_decode(generated_text_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print or use the generated text\n",
    "for i, text in enumerate(generated_text):\n",
    "    print(f\"Generated Text {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a3f96-49c7-435c-8096-aa362526682d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
