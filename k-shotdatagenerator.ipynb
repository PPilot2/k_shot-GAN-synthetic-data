{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9685a5f-1689-455b-80c6-c2147070f961",
   "metadata": {},
   "source": [
    "# Data generator\n",
    "\n",
    "#### This notebook generates and stores a synthetic dataset using k-shot prompting\n",
    "\n",
    "- We reccomend the use of a GPU for larger datasets and models. \n",
    "- Throughout this notebook CAPITAL_LETTERS indicate that you must enter information (such as paths, sizes, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f745d0d-2c67-4c35-8c5c-bfbe2a2f982c",
   "metadata": {},
   "source": [
    "### Setup\n",
    "- Install and import libraries\n",
    "- Get prompts with K examples\n",
    "- Load in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba749d-71aa-436a-a348-d25c58c425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install --upgrade transformers\n",
    "!pip install einops\n",
    "!pip install torch\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25e9e825-3a40-4d7a-ae26-eacea69891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset, DatasetDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a747609d-c9f9-440f-8bac-c8298e52ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns empty positive and negative prompts   \n",
    "def resetPrompts():\n",
    "    positivePrompt = '''Generate an enthusiastic and positive social media tweet. Your tweet should express praise or excitement. Use the following examples as a guide for formatting and tone:\\n'''\n",
    "    negativePrompt = '''Generate a disapproving and negative social media tweet. The tweet should convey criticism or disappointment. Use the following examples as a guide for formatting and tone:\\n'''\n",
    "\n",
    "    return positivePrompt, negativePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1417f418-5659-4264-9bff-e68e466a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns prompts with k examples \n",
    "def addKExamples(k):\n",
    "\n",
    "    #Get the empty prompts without examples\n",
    "    fullPositivePrompt, fullNegativePrompt = resetPrompts()\n",
    "    train = dataset['train']\n",
    "\n",
    "    #Filter through the dataset and select the first k positive and negative datapoints\n",
    "    positiveExamples = train.filter(lambda x: x['label'] == 0).select(range(k))[\"text\"]\n",
    "    negativeExamples = train.filter(lambda x: x['label'] == 1).select(range(k))[\"text\"]\n",
    "    \n",
    "    for example in positiveExamples:\n",
    "        fullPositivePrompt += f'Positive: \"{example}\"\\n'\n",
    "        \n",
    "    for example in negativeExamples:\n",
    "        fullNegativePrompt += f'Negative: \"{example}\"\\n'\n",
    "\n",
    "    # We store the prompt without the actual model prompting area to make filtering easier later. \n",
    "    examplesPositivePrompt = fullPositivePrompt\n",
    "    examplesNegativePrompt = fullNegativePrompt\n",
    "    \n",
    "    fullPositivePrompt += 'Positive: \"'\n",
    "    fullNegativePrompt += 'Negative: \"'\n",
    "    \n",
    "    return fullPositivePrompt, fullNegativePrompt, examplesPositivePrompt, examplesNegativePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e40653fa-56ff-4757-aae7-cea7f554fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "fullPositivePrompt, fullNegativePrompt, examplesPositivePrompt, examplesNegativePrompt = addKExamples(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccde460f-50bf-47c2-91ed-d18a902b715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34db04b3c31846428eeaadb41ce64069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e46929108844d5183b74f99611ff971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f13a33ff094ebe8ba699eb1f414cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba82b84167449d895b83d7077844f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae868e778934543a126d081490d3c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fbde8b50a24ab4b2316a5de8daa20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cd9cf9db8f43fa95fd117f815b6f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761becc1713c4c828afd69cd63b6f788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f245052b5bd4201af93af1f9df7fa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88530c0bf8d24141a55ebf3709127840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660f576672cf4b88a54459c6bfd3e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640a6c29ced84ef5afa27b3041ca392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login is neccessary for gated models like Mistral-7B\n",
    "login(YOUR_HUGGINGFACE_KEY)\n",
    "\n",
    "# Enter the name of the Huggingface model you want to use. \n",
    "# We used \"mistralai/Mistral-7B-Instruct-v0.3\" and \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = MODEL_NAME\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "  model = model.cuda()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a221200-7859-4664-9846-b086d8d94b6f",
   "metadata": {},
   "source": [
    "### Model Config\n",
    "- Set up the neccessary methods to generate and filter through tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32f5a4ed-c54d-4fc3-a112-67680e5f21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a generated tweet and its logits. \n",
    "def promptModel(prompt, length):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        do_sample=True,\n",
    "        attention_mask=attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        max_length=length,\n",
    "        temperature = 0.9, \n",
    "        top_p = 0.9, \n",
    "        repetition_penalty=1.2,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get logits\n",
    "        logits = model(outputs.sequences).logits\n",
    "\n",
    "\n",
    "    return text, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ef6f959-c434-4cc7-81fb-4ba936addbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses regex to filter and return only the tweet (extract the unwanted additional generation)\n",
    "def findTweets(generated_text, isPositive, plainPrompt):\n",
    "    output = generated_text.replace(plainPrompt, \"\").replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "    if(isPositive):\n",
    "        positive_match = re.search(r'Positive:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        positive_tweet = positive_match.group(1).strip() if positive_match else \"-1\"\n",
    "        return positive_tweet\n",
    "\n",
    "    else:\n",
    "        negative_match = re.search(r'Negative:\\s*\"\\s*([^\"]*)\\s*\"', output, re.DOTALL)\n",
    "        negative_tweet = negative_match.group(1).strip() if negative_match else \"-1\"\n",
    "        return negative_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "195bb976-9646-4a11-b090-2abd358f879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeatedly prompts the model until it generates with correct formatting. \n",
    "def generateAndExtractTweets(prompt, label, length, plainPrompt):     \n",
    "    while True:\n",
    "        text, logits = promptModel(prompt, length)\n",
    "        tweet = findTweets(text, label == 1, plainPrompt)\n",
    "        \n",
    "        if tweet != \"-1\" and len(tweet) > 0:\n",
    "            length = len(tokenizer.encode(tweet, add_special_tokens=True))\n",
    "            text, logits = promptModel(tweet, length + 1)  \n",
    "            return text, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df5d83-4be9-4b10-8486-f4a2b02d3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "generateAndExtractTweets(fullNegativePrompt, 0, 1650, examplesNegativePrompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935c5a0-403c-42a8-a013-bd452a5bd630",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "\n",
    "All of our tests have datasets with 15,000 training datapoints and 2,000 testing datapoints. \n",
    "\n",
    "Everything is formatted identical to sentiment140, meaning that we have features of \"text\" and \"labels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b252068f-bda9-404e-8bea-731b1d1d745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a synthetic dataset given its size and tokens per tweet. It prompts the model, the neccessary amount of times and arranges everything into a Dataset object. \n",
    "\n",
    "def generateDataset (size, tokensPerTweet):\n",
    "\n",
    "    syntheticDataset = {\n",
    "        'text': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    #Generating half as positive datapoints with a label of 1.\n",
    "    for i in range (size//2):\n",
    "        text, logits = generateAndExtractTweets(fullPositivePrompt, 1, tokensPerTweet, examplesPositivePrompt)\n",
    "        syntheticDataset[\"text\"].append(text)\n",
    "        syntheticDataset[\"label\"].append(1)\n",
    "        if(i % 20 == 0):\n",
    "            print(i)\n",
    "\n",
    "    print(\"FINISHED POSITIVE\")\n",
    "\n",
    "    #Generating half as negative datapoints with a label of 1.\n",
    "    for i in range (size//2):\n",
    "        text, logits = generateAndExtractTweets(fullNegativePrompt, 0, tokensPerTweet, examplesNegativePrompt)\n",
    "        syntheticDataset[\"text\"].append(text)\n",
    "        syntheticDataset[\"label\"].append(0)\n",
    "        if(i % 20 == 0):\n",
    "            print(i)\n",
    "            \n",
    "    print(\"FINISHED NEGATIVE\")\n",
    "\n",
    "    syntheticDataset = Dataset.from_dict(syntheticDataset).shuffle()\n",
    "\n",
    "    return syntheticDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8cfdc-addd-410d-9756-3e651b634324",
   "metadata": {},
   "source": [
    "As the prompt gets larger with more examples, we must also increase the alloted tweet size. \n",
    "- k = 5 uses a size value of 350 tokens\n",
    "- k = 10 uses a size value of 500 tokens\n",
    "- k = 15 uses a size value of 650 tokens\n",
    "- k = 30 uses a size value of 1150 tokens\n",
    "- k = 50 uses a size value of 1650 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c0f31-e836-4518-809b-c7fff5457b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a full synthetic dataset. \n",
    "\n",
    "# Our dataset used 15k for trainSize, 2k for testSize\n",
    "\n",
    "trainSize = TRAIN_SIZE\n",
    "testSize =  TEST_SIZE\n",
    "tweetSize = TWEET_SIZE\n",
    "saveDir = YOUR_SAVE_DIR\n",
    "\n",
    "train = generateDataset(trainSize, tweetSize)\n",
    "print(\"\\n\\nFINISHED TRAIN\\n\\n\")\n",
    "test = generateDataset(testSize, tweetSize)\n",
    "print(\"\\n\\nFINISHED TEST\\n\\n\")\n",
    "\n",
    "syntheticDataset = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': test\n",
    "})\n",
    "\n",
    "syntheticDataset.save_to_disk(saveDir)\n",
    "syntheticDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df2cd0-176a-48a4-9b49-699aa8902309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
