{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPXrMPY56os2",
    "outputId": "4f83f4a2-9dcc-4be0-8696-732fa930859c"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install einops\n",
    "!pip install openai\n",
    "!pip install evaluate\n",
    "!pip install torch\n",
    "!pip install joblib\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn\n",
    "!pip install tf-keras\n",
    "!pip install accelerate -U\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client --ignore-installed\n",
    "!pip install --upgrade google-cloud --ignore-installed\n",
    "!pip install --upgrade google-cloud-vision --ignore-installed\n",
    "!pip install google-cloud-aiplatform --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZYgITSjZ6wcK"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "# from google.colab import auth\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wx6SOMBC6yog"
   },
   "outputs": [],
   "source": [
    "def load_and_extract_dataset(zip_path):\n",
    "    # Create a temporary directory\n",
    "    temp_dir = '/tmp/dataset_extracted'\n",
    "    \n",
    "    # Ensure the directory is clean\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    os.makedirs(temp_dir)\n",
    "    \n",
    "    # Unzip the file to the temporary directory\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "\n",
    "    # Load the dataset from the extracted directory\n",
    "    dataset = load_from_disk(temp_dir)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3v6WlfcH6zCi",
    "outputId": "3e99cfc9-421c-404a-db87-7a528a85ede6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1600000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 359\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_name = 'dataset.zip'\n",
    "dataset = load_and_extract_dataset(zip_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ndfUKdBeTeED"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py-EPWEx7pzS"
   },
   "outputs": [],
   "source": [
    "# auth.authenticate_user()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ysgPdBlD9ci3"
   },
   "outputs": [],
   "source": [
    "# Function to load the dataset and pick K random entries\n",
    "def pick_random_entries(K):\n",
    "    # pos = []\n",
    "    # neg = []\n",
    "    # for i in range(len(train_dataset)):\n",
    "    #   if train_dataset[i]['label'] == 0 and len(neg) <= 5:\n",
    "    #     neg.append(train_dataset[i])\n",
    "    #   if train_dataset[i]['label'] == 1 and len(neg) <= 5:\n",
    "    #     pos.append(train_dataset[i])\n",
    "\n",
    "    random_entries = random.sample(list(train_dataset), K)\n",
    "    # Delete the useless stuff\n",
    "    # for entry in random_entries:\n",
    "    #   del entry[\"query\"]\n",
    "    #   del entry[\"user\"]\n",
    "\n",
    "    return random_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRnCNBsE74wU",
    "outputId": "aa1b7e0b-0596-4a79-fd21-72d81bd8e42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"2 glasses of wine and I'm not even sleepy. If that doesn't make me tired then I don't know what will \", 'label': 0}\n",
      "{'text': \"The longest security line I have ever seen! And I'm at the end if the line  - http://bkite.com/07AzK\", 'label': 0}\n",
      "{'text': '@ckillius no problem, i thought it would make it more fun and interactive ', 'label': 1}\n",
      "{'text': 'says goodmorning people..  lakas ng ulan! http://plurk.com/p/yq9tp', 'label': 1}\n",
      "{'text': '@midnightamatory Me too. :&quot;&gt; ', 'label': 1}\n",
      "{'text': 'I could Tweet about him all night lon ', 'label': 1}\n",
      "{'text': 'homework. because i have nothing better to do ', 'label': 0}\n",
      "{'text': 'Tamagotchi Plus Color! I want one! Now those bring back memories ', 'label': 1}\n",
      "{'text': 'Slight headache today-was it the cheap champagne, the homemade redcurrant wine or the copious amounts of other wines? Or all of the above ', 'label': 0}\n",
      "{'text': 'http://twitpic.com/8d1tn - Bye bye long hair ', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "random_entries = pick_random_entries(k)\n",
    "\n",
    "for entry in random_entries:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIReiwyt78eb",
    "outputId": "19a886b0-9307-4cb9-90a5-b6de1504e2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE: \"2 glasses of wine and I'm not even sleepy. If that doesn't make me tired then I don't know what will \"\n",
      "NEGATIVE: \"The longest security line I have ever seen! And I'm at the end if the line  - http://bkite.com/07AzK\"\n",
      "POSITIVE: \"@ckillius no problem, i thought it would make it more fun and interactive \"\n",
      "POSITIVE: \"says goodmorning people..  lakas ng ulan! http://plurk.com/p/yq9tp\"\n",
      "POSITIVE: \"@midnightamatory Me too. :&quot;&gt; \"\n",
      "POSITIVE: \"I could Tweet about him all night lon \"\n",
      "NEGATIVE: \"homework. because i have nothing better to do \"\n",
      "POSITIVE: \"Tamagotchi Plus Color! I want one! Now those bring back memories \"\n",
      "NEGATIVE: \"Slight headache today-was it the cheap champagne, the homemade redcurrant wine or the copious amounts of other wines? Or all of the above \"\n",
      "NEGATIVE: \"http://twitpic.com/8d1tn - Bye bye long hair \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "\n",
    "for entry in random_entries:\n",
    "  prompt += 'POSITIVE: \"' if entry['label'] == 1 else 'NEGATIVE: \"'\n",
    "  prompt += entry[\"text\"] + '\"\\n'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "053UcC-ey3Hr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443c19ac45e94381a228ce0df702f585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praha\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\praha\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c192bc67b629457a8377baec8138faf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c9011bbfa64fba948a31950c4c2525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45ac83f0dc74d47b4ddb99b0c46d667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praha\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Initialize BERT components\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dmqvXYuD6WA1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\Y'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\Y'\n",
      "C:\\Users\\praha\\AppData\\Local\\Temp\\ipykernel_25692\\2697959696.py:3: SyntaxWarning: invalid escape sequence '\\Y'\n",
      "  format = content3 = \"Format the output as follows:\\n POSITIVE: \\Your positive comment here.\\\\n NEGATIVE: \\Your negative comment here.\\\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE: \"OMG I just got a new puppy!!! üê∂üíñ Can\\'t wait to spoil him rotten!! #PuppyLove\"  \\nNEGATIVE: \"Ughhh, why is my internet so slow today? üò© Can\\'t even stream my shows!! #Frustrating\"  \\n\\nPOSITIVE: \"Just finished an amazing book! üìö‚ú® Highly recommend it to everyone!! #MustRead\"  \\nNEGATIVE: \"Why do I always forget my umbrella when it rains?? üò§‚òîÔ∏è I\\'m soaked!\"  \\n\\nPOSITIVE: \"Just found the cutest little cafe in town! ‚òïÔ∏è‚ù§Ô∏è Can\\'t wait to go back!! #CoffeeLover\"  \\nNEGATIVE: \"Why is it so hard to find parking in this city?!! üò° It\\'s a nightmare!!\"  \\n\\nPOSITIVE: \"Feeling super productive today! Got so much done! üí™üéâ #Winning\"  \\nNEGATIVE: \"Why does every single delivery take FOREVER?? üò© I\\'m hungry now!\"  \\n\\nPOSITIVE: \"Just got back from the most amazing trip! üåç‚úàÔ∏è Can\\'t wait to share pics! #TravelGoals\"  \\nNEGATIVE: \"Seriously, can we talk about how bad this movie was? üòí I was sooo bored!\"  ']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "format = content3 = \"Format the output as follows:\\n POSITIVE: \\Your positive comment here.\\\\n NEGATIVE: \\Your negative comment here.\\\\n\"\n",
    "content1 = \"You will be provided with several examples of social media comments along with whether the comment is of positive (POSITIVE) or negative (NEGATIVE) sentiment. Generate a positive and negative data entry that are each as indistinguishable as possible from the examples. Ensure the generated comments are realistic, contextually appropriate, and free of escape characters. Also ensure that the generated comments contain a few minor spelling or grammar mistakes. Make sure that the generated comments sound as human as possible and haven't been generated before. \" + format\n",
    "content2 = \"You will be provided with several examples of social media comments along with whether the comment is of positive (POSITIVE) or negative (NEGATIVE) sentiment. Generate a positive and negative data entry that are each as indistinguishable as possible from the examples. \" + format\n",
    "content3 = \"Make data similar to the social media examples that have been provided. Add punctuation, capitalization and spelling errors. \" + format\n",
    "content4 = \"Make data similar to the social media examples that have been provided. Add frequent punctuation, frequent capitalization, and frequent spelling errors to make the text sound more human. but make sure that the errors are not complete gibberish. Feel free to add emojis and infrequent hashtags. Make sure that each response is distinctly different from the previous one. \" + format\n",
    "content5 = \"Make data similar to the social media examples that have been provided. Add frequent punctuation, frequent capitalization, and frequent spelling errors to make the text sound more human. but make sure that the errors are not complete gibberish. Try to add information about current real world events. Feel free to add emojis and infrequent hashtags. Make sure that each response is distinctly different from the previous one. \" + format\n",
    "def generate_tweet_like_text(num_responses):\n",
    "\n",
    "    responses = []\n",
    "    for _ in range(num_responses):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": content4},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,  # Adjust for creativity\n",
    "            top_p=0.9,        # Adjust for diversity\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "\n",
    "    return responses\n",
    "\n",
    "num_responses = 1\n",
    "\n",
    "tweets = generate_tweet_like_text(num_responses)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "ea09d49c09204e0ca91abe17bcf74649",
      "3659725d35c74be8999fe64b2562b776",
      "d257f88a0e604381b81ef184f17a3cd5",
      "9c489d17625840b5aea90ec563df6734",
      "16e0d81d84814e149ada333b8a3cc0bf",
      "e57d00243ec0494f984fe785a999e2f6",
      "3b4b68637e5147e0a9fce676665d66cf",
      "9689ed8e89a24f4fa88de1eb28dc559b",
      "a62c12425fb2427486f8a84be34217a9",
      "ed04aff9469046d98847de7dd2f79f03",
      "5470febca80044a98831880572f2581a",
      "38945dd8ae594b3e932490161dbf535f",
      "3773947e63b54b2ea18b39e423861a00",
      "497936dfb42e4da082d00c76b483a170",
      "d34febc2c68a42c3936093a7f54452f1",
      "e1f17ffd077f404393297ee131a9193f",
      "ff68f516734449b8bec9b79357c3db02",
      "ca7562212ba14a13a6140190ca1597b3",
      "2726e0223b3142bdb6042f2bd9d2cf3c",
      "5e76468b755e454fb4f41eb009b6f2b9",
      "ff3d5a523529444c9d0c123433463264",
      "9b77619256a940b2add76609a38f0752"
     ]
    },
    "id": "zjdC4-SA1Pwq",
    "outputId": "daca676c-34e2-4f9b-ce58-8e7a21270794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\praha\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37771761e6634abc93804ba84bc1e482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d88fab7d6c64ea3b2a612672762e33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example training data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "human_texts = \"\"\n",
    "for i in random_entries:\n",
    "  human_texts += i['text']\n",
    "machine_texts = \"\"\n",
    "for tweet in tweets:\n",
    "    if tweet.strip() != \"\":\n",
    "      machine_texts += tweet\n",
    "# Labels\n",
    "# Extract embeddings\n",
    "text = human_texts + machine_texts\n",
    "# X = get_bert_embeddings(human_texts + machine_texts)\n",
    "# X = X.transpose()\n",
    "\n",
    "labels = np.array([1] * len(human_texts) + [0] * len(machine_texts))\n",
    "labels = np.random.permutation(labels)\n",
    "labels = list(labels[0:len(text)])\n",
    "\n",
    "df = pd.DataFrame({'text': text, 'label': labels})\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SfzKIGqcaW3A"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34b06b2e964463e85e63a3b1910a203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:193\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    192\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m     out \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:368\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:229\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    228\u001b[0m         trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mArrowInvalid \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:368\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3538\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3537\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3538\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite(example)\n\u001b[0;32m   3539\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:500\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[1;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_examples_on_file()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:458\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    455\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[0;32m    456\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[0;32m    457\u001b[0m         ]\n\u001b[1;32m--> 458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_batch(batch_examples\u001b[38;5;241m=\u001b[39mbatch_examples)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:568\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    567\u001b[0m typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 568\u001b[0m arrays\u001b[38;5;241m.\u001b[39mappend(pa\u001b[38;5;241m.\u001b[39marray(typed_sequence))\n\u001b[0;32m    569\u001b[0m inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:248\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:112\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:242\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trying_cast_to_python_objects \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m--> 242\u001b[0m     out \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    243\u001b[0m         cast_to_python_objects(data, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optimize_list_casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:368\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:193\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    192\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m     out \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:368\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:229\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    228\u001b[0m         trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mArrowInvalid \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:368\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n\u001b[1;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(format_labels)\n\u001b[0;32m      6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(format_labels)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3587\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3587\u001b[0m         writer\u001b[38;5;241m.\u001b[39mfinalize()\n\u001b[0;32m   3588\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3589\u001b[0m         tmp_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:599\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[1;34m(self, close_stream)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_examples_on_file()\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:458\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    455\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[0;32m    456\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[0;32m    457\u001b[0m         ]\n\u001b[1;32m--> 458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_batch(batch_examples\u001b[38;5;241m=\u001b[39mbatch_examples)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:568\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    566\u001b[0m         col_try_type \u001b[38;5;241m=\u001b[39m try_features[col] \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    567\u001b[0m         typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 568\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(pa\u001b[38;5;241m.\u001b[39marray(typed_sequence))\n\u001b[0;32m    569\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    570\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:248\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:112\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\datasets\\arrow_writer.py:242\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trying_cast_to_python_objects \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m--> 242\u001b[0m     out \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    243\u001b[0m         cast_to_python_objects(data, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optimize_list_casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m         out \u001b[38;5;241m=\u001b[39m cast_array_to_feature(\n\u001b[0;32m    247\u001b[0m             out, \u001b[38;5;28mtype\u001b[39m, allow_primitive_to_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_decimal_to_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    248\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:368\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\algoverse-env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not convert 0 with type numpy.int64: did not recognize Python value type when inferring an Arrow data type"
     ]
    }
   ],
   "source": [
    "def format_labels(example):\n",
    "    example['label'] = torch.tensor(example['label'], dtype=torch.long)\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(format_labels)\n",
    "test_dataset = test_dataset.map(format_labels)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "9YkTXmS0Zn6E",
    "outputId": "ad820519-cf88-425c-8398-c4d024f8abc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.03,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/756 00:13 < 00:22, 20.62 it/s, Epoch 1.13/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "# Save the tokenizer and model\n",
    "tokenizer.save_pretrained('bert_tokenizer')\n",
    "model.save_pretrained('bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "8u4_exdNWOko"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/10) Evaluation: Machine\n",
      "(2/10) Evaluation: Machine\n",
      "(3/10) Evaluation: Machine\n",
      "(4/10) Evaluation: Machine\n",
      "(5/10) Evaluation: Machine\n",
      "(6/10) Evaluation: Machine\n",
      "(7/10) Evaluation: Machine\n",
      "(8/10) Evaluation: Machine\n",
      "(9/10) Evaluation: Machine\n",
      "(10/10) Evaluation: Machine\n",
      "Training accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "training_epochs = 10\n",
    "score = training_epochs\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_tokenizer')\n",
    "model = BertForSequenceClassification.from_pretrained('bert_model')\n",
    "\n",
    "def evaluate_comment(comment):\n",
    "    global score\n",
    "    inputs = tokenizer(comment, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = softmax(outputs.logits, dim=1)\n",
    "        prediction = torch.argmax(probs, dim=1).item()\n",
    "    if prediction == 1:\n",
    "        return \"Human\"\n",
    "    else:\n",
    "        score -= 1\n",
    "        return \"Machine\"\n",
    "\n",
    "# Example evaluation\n",
    "for i in range(training_epochs):\n",
    "    new_comment = generate_tweet_like_text(1)\n",
    "    print(\"(\" + str(i+1) + \"/\" + str(training_epochs) + \")\", \"Evaluation:\", evaluate_comment(new_comment))\n",
    "print(\"Training accuracy:\", score/training_epochs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16e0d81d84814e149ada333b8a3cc0bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2726e0223b3142bdb6042f2bd9d2cf3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3659725d35c74be8999fe64b2562b776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e57d00243ec0494f984fe785a999e2f6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3b4b68637e5147e0a9fce676665d66cf",
      "value": "Map:‚Äá100%"
     }
    },
    "3773947e63b54b2ea18b39e423861a00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff68f516734449b8bec9b79357c3db02",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ca7562212ba14a13a6140190ca1597b3",
      "value": "Map:‚Äá100%"
     }
    },
    "38945dd8ae594b3e932490161dbf535f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3773947e63b54b2ea18b39e423861a00",
       "IPY_MODEL_497936dfb42e4da082d00c76b483a170",
       "IPY_MODEL_d34febc2c68a42c3936093a7f54452f1"
      ],
      "layout": "IPY_MODEL_e1f17ffd077f404393297ee131a9193f"
     }
    },
    "3b4b68637e5147e0a9fce676665d66cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "497936dfb42e4da082d00c76b483a170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2726e0223b3142bdb6042f2bd9d2cf3c",
      "max": 473,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e76468b755e454fb4f41eb009b6f2b9",
      "value": 473
     }
    },
    "5470febca80044a98831880572f2581a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e76468b755e454fb4f41eb009b6f2b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9689ed8e89a24f4fa88de1eb28dc559b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b77619256a940b2add76609a38f0752": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c489d17625840b5aea90ec563df6734": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed04aff9469046d98847de7dd2f79f03",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5470febca80044a98831880572f2581a",
      "value": "‚Äá1889/1889‚Äá[00:21&lt;00:00,‚Äá89.91‚Äáexamples/s]"
     }
    },
    "a62c12425fb2427486f8a84be34217a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca7562212ba14a13a6140190ca1597b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d257f88a0e604381b81ef184f17a3cd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9689ed8e89a24f4fa88de1eb28dc559b",
      "max": 1889,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a62c12425fb2427486f8a84be34217a9",
      "value": 1889
     }
    },
    "d34febc2c68a42c3936093a7f54452f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff3d5a523529444c9d0c123433463264",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9b77619256a940b2add76609a38f0752",
      "value": "‚Äá473/473‚Äá[00:07&lt;00:00,‚Äá66.69‚Äáexamples/s]"
     }
    },
    "e1f17ffd077f404393297ee131a9193f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e57d00243ec0494f984fe785a999e2f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea09d49c09204e0ca91abe17bcf74649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3659725d35c74be8999fe64b2562b776",
       "IPY_MODEL_d257f88a0e604381b81ef184f17a3cd5",
       "IPY_MODEL_9c489d17625840b5aea90ec563df6734"
      ],
      "layout": "IPY_MODEL_16e0d81d84814e149ada333b8a3cc0bf"
     }
    },
    "ed04aff9469046d98847de7dd2f79f03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3d5a523529444c9d0c123433463264": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff68f516734449b8bec9b79357c3db02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
